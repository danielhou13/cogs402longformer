{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2839,
     "status": "ok",
     "timestamp": 1654646658378,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "b_HI-3J_1Gfg",
    "outputId": "7c363935-66b2-480f-ab80-e098af1212da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1654646658378,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "Jck92aCS1c3S"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/content/drive/My Drive/{}'.format(\"cogs402longformer/\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4304,
     "status": "ok",
     "timestamp": 1654646662678,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "zMjzQIFJ2P_T"
   },
   "outputs": [],
   "source": [
    "pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 2205,
     "status": "ok",
     "timestamp": 1654646664879,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "Uno0qwr12UTd"
   },
   "outputs": [],
   "source": [
    "pip install captum --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 3862,
     "status": "ok",
     "timestamp": 1654646668737,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "OaOSPMJE3ONc"
   },
   "outputs": [],
   "source": [
    "pip install datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1654646668738,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "hRSNYTrRIPkr"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "import os\n",
    "print(os.getcwd())\n",
    "path_parent = os.path.dirname(os.getcwd())\n",
    "os.chdir(path_parent)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 3328,
     "status": "ok",
     "timestamp": 1654646672060,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "wrRRZJ6-0_Il"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification, BertConfig\n",
    "\n",
    "from captum.attr import visualization as viz\n",
    "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1654646672060,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "b5V31lsc0_Il"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 12485,
     "status": "ok",
     "timestamp": 1654646684540,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "9nzBfB-v0_Im"
   },
   "outputs": [],
   "source": [
    "from transformers import LongformerForSequenceClassification, LongformerTokenizer, LongformerConfig\n",
    "# replace <PATH-TO-SAVED-MODEL> with the real path of the saved model\n",
    "model_path = '/content/drive/MyDrive/cogs402longformer/models/longformer-finetuned_papers/checkpoint-2356'\n",
    "\n",
    "# load model\n",
    "model = LongformerForSequenceClassification.from_pretrained(model_path, num_labels = 2)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "ok",
     "timestamp": 1654646684541,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "vAAjmDRl0_In"
   },
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1654646684541,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "UgBSBpz-0_In"
   },
   "outputs": [],
   "source": [
    "def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id):\n",
    "\n",
    "    text_ids = tokenizer.encode(text, truncation = True, add_special_tokens=False, max_length = 2048)\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + text_ids + [sep_token_id]\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(text_ids)\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "    position_ids = torch.arange(seq_length, dtype=torch.long, device=device)\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "\n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85,
     "referenced_widgets": [
      "96a7a90aa4674e4196808ce4a58a078f",
      "bbbeb6d370d343ceba0abccc4102b8b1",
      "514692f5f7f84154aaecd3e4b9a070f0",
      "cb1fc6fd97f842d3993e953e1e4af976",
      "4953f1840f5649de9d31eb95223a45ce",
      "bce2d2221ed943518b7b1414e22da7fe",
      "1af916f5e2e34049b92a638978f417c7",
      "0b022ab327f84c64ac9c810a76e1b287",
      "4e2c23bd048a434e9c9e561c6eb03cff",
      "e49d2d39dd8a4bf0876519f43a498a97",
      "c1465032d27e4b5ba8bb94c69d72b65c"
     ]
    },
    "executionInfo": {
     "elapsed": 3816,
     "status": "ok",
     "timestamp": 1654646688350,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "w6aQM9nM0_Ip",
    "outputId": "27d0018e-ffb9-4fbc-8a6f-038d5d0e5f92"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration danielhou13--cogs402dataset-cc784554b797f843\n",
      "Reusing dataset parquet (/root/.cache/huggingface/datasets/danielhou13___parquet/danielhou13--cogs402dataset-cc784554b797f843/0.0.0/0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96a7a90aa4674e4196808ce4a58a078f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "cogs402_ds = load_dataset(\"danielhou13/cogs402dataset\")[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 634,
     "status": "ok",
     "timestamp": 1654646688971,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "GCbvY3Ln0_Iq",
    "outputId": "7b36fc45-3aaa-42d7-d963-75fdf91a87e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "testval = 923\n",
    "text = cogs402_ds['text'][testval]\n",
    "label = cogs402_ds['labels'][testval]\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1654646688972,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "c-t2gomr0_Iq"
   },
   "outputs": [],
   "source": [
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n",
    "position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "indices = input_ids[0].detach().tolist()\n",
    "all_tokens = tokenizer.convert_ids_to_tokens(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1654646688972,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "kE5fQlC_TIFE"
   },
   "outputs": [],
   "source": [
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    attributions = attributions / torch.norm(attributions)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1654646688972,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "9aEws7aELnwe",
    "outputId": "c00b9fa3-e77d-46ca-d55c-856b0d996e6b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'lp', 'opt', ':', 'ĠA', 'ĠRule', 'ĠOptim', 'ization', 'ĠTool', 'Ġfor', 'ĠAnswer', 'ĠSet', 'ĠProgramming', 'Ġ', 'Ġar', 'X', 'iv', ':', '16', '08', '.', '05', '675', 'v', '2', 'Ġ[', 'cs', '.', 'LO', ']', 'Ġ23', 'ĠAug', 'Ġ2016', 'Ġ', 'ĠManuel', 'ĠB', 'ich', 'ler', ',', 'ĠMichael', 'ĠMor', 'ak', ',', 'Ġand', 'ĠStefan', 'ĠWol', 'tr', 'an', 'ĠT', 'U', 'ĠW', 'ien', ',', 'ĠVienna', ',', 'ĠAustria', 'Ġ{', 's', 'urn', 'ame', '}', '@', 'db', 'ai', '.', 'tu', 'w', 'ien', '.', 'ac', '.', 'at', 'Ġ', 'ĠAbstract', '.', 'ĠState', '-', 'of', '-', 'the', '-', 'art', 'Ġanswer', 'Ġset', 'Ġprogramming', 'Ġ(', 'AS', 'P', ')', 'Ġsol', 'vers', 'Ġrely', 'Ġon', 'Ġa', 'Ġprogram', 'Ġcalled', 'Ġa', 'Ġgrou', 'nder', 'Ġto', 'Ġconvert', 'Ġnon', '-', 'ground', 'Ġprograms', 'Ġcontaining', 'Ġvariables', 'Ġinto', 'Ġvariable', '-', 'free', ',', 'Ġpropos', 'itional', 'Ġprograms', '.', 'ĠThe', 'Ġsize', 'Ġof', 'Ġthis', 'Ġgrounding', 'Ġdepends', 'Ġheavily', 'Ġon', 'Ġthe', 'Ġsize', 'Ġof', 'Ġthe', 'Ġnon', '-', 'ground', 'Ġrules', ',', 'Ġand', 'Ġthus', ',', 'Ġreducing', 'Ġthe', 'Ġsize', 'Ġof', 'Ġsuch', 'Ġrules', 'Ġis', 'Ġa', 'Ġpromising', 'Ġapproach', 'Ġto', 'Ġimprove', 'Ġsolving', 'Ġperformance', '.', 'ĠTo', 'Ġthis', 'Ġend', ',', 'Ġin', 'Ġthis', 'Ġpaper', 'Ġwe', 'Ġannounce', 'Ġl', 'p', 'opt', ',', 'Ġa', 'Ġtool', 'Ġthat', 'Ġdecom', 'poses', 'Ġlarge', 'Ġlogic', 'Ġprogramming', 'Ġrules', 'Ġinto', 'Ġsmaller', 'Ġrules', 'Ġthat', 'Ġare', 'Ġeasier', 'Ġto', 'Ġhandle', 'Ġfor', 'Ġcurrent', 'Ġsol', 'vers', '.', 'ĠThe', 'Ġtool', 'Ġis', 'Ġspecifically', 'Ġtailored', 'Ġto', 'Ġhandle', 'Ġthe', 'Ġstandard', 'Ġsyntax', 'Ġof', 'Ġthe', 'ĠASP', 'Ġlanguage', 'Ġ(', 'AS', 'P', '-', 'Core', ')', 'Ġand', 'Ġmakes', 'Ġit', 'Ġeasier', 'Ġfor', 'Ġusers', 'Ġto', 'Ġwrite', 'Ġefficient', 'Ġand', 'Ġintuitive', 'ĠASP', 'Ġprograms', ',', 'Ġwhich', 'Ġwould', 'Ġotherwise', 'Ġoften', 'Ġrequire', 'Ġsignificant', 'Ġhand', '-', 'tun', 'ing', 'Ġby', 'Ġexpert', 'ĠASP', 'Ġengineers', '.', 'ĠIt', 'Ġis', 'Ġbased', 'Ġon', 'Ġan', 'Ġidea', 'Ġproposed', 'Ġby', 'ĠMor', 'ak', 'Ġand', 'ĠWol', 'tr', 'an', 'Ġ(', '2012', ')', 'Ġthat', 'Ġwe', 'Ġextend', 'Ġsignificantly', 'Ġin', 'Ġorder', 'Ġto', 'Ġhandle', 'Ġthe', 'Ġfull', 'ĠASP', 'Ġsyntax', ',', 'Ġincluding', 'Ġcomplex', 'Ġconstructs', 'Ġlike', 'Ġaggreg', 'ates', ',', 'Ġweak', 'Ġconstraints', ',', 'Ġand', 'Ġarithmetic', 'Ġexpressions', '.', 'ĠWe', 'Ġpresent', 'Ġthe', 'Ġalgorithm', ',', 'Ġthe', 'Ġtheoretical', 'Ġfoundations', 'Ġon', 'Ġhow', 'Ġto', 'Ġtreat', 'Ġthese', 'Ġconstructs', ',', 'Ġas', 'Ġwell', 'Ġas', 'Ġan', 'Ġexperimental', 'Ġevaluation', 'Ġshowing', 'Ġthe', 'Ġviability', 'Ġof', 'Ġour', 'Ġapproach', '.', 'Ġ', 'Ġ1', 'Ġ', 'ĠIntroduction', 'Ġ', 'ĠAnswer', 'Ġset', 'Ġprogramming', 'Ġ(', 'AS', 'P', ')', 'Ġ[', '15', ',', '17', ',', '8', ',', '13', ']', 'Ġis', 'Ġa', 'Ġwell', '-', 'established', 'Ġlogic', 'Ġprogramming', 'Ġparadigm', 'Ġbased', 'Ġon', 'Ġthe', 'Ġstable', 'Ġmodel', 'Ġsemantics', 'Ġof', 'Ġlogic', 'Ġprograms', '.', 'ĠIts', 'Ġmain', 'Ġadvantage', 'Ġis', 'Ġan', 'Ġintuitive', ',', 'Ġdecl', 'ar', 'ative', 'Ġlanguage', ',', 'Ġand', 'Ġthe', 'Ġfact', 'Ġthat', ',', 'Ġgenerally', ',', 'Ġeach', 'Ġanswer', 'Ġset', 'Ġof', 'Ġa', 'Ġgiven', 'Ġlogic', 'Ġprogram', 'Ġdescribes', 'Ġa', 'Ġvalid', 'Ġanswer', 'Ġto', 'Ġthe', 'Ġoriginal', 'Ġquestion', '.', 'ĠMoreover', ',', 'ĠASP', 'Ġsol', 'vers', 'âĢĶ', 'see', 'Ġe', '.', 'g', '.', 'Ġ[', '14', ',', '1', ',', '12', ',', '2', ']', 'âĢĶ', 'have', 'Ġmade', 'Ġhuge', 'Ġstrides', 'Ġin', 'Ġefficiency', '.', 'ĠA', 'Ġlogic', 'Ġprogram', 'Ġusually', 'Ġconsists', 'Ġof', 'Ġa', 'Ġset', 'Ġof', 'Ġlogical', 'Ġimplications', 'Ġby', 'Ġwhich', 'Ġnew', 'Ġfacts', 'Ġcan', 'Ġbe', 'Ġinferred', 'Ġfrom', 'Ġexisting', 'Ġones', ',', 'Ġand', 'Ġa', 'Ġset', 'Ġof', 'Ġfacts', 'Ġthat', 'Ġrepresent', 'Ġthe', 'Ġconcrete', 'Ġinput', 'Ġinstance', '.', 'ĠLogic', 'Ġprogramming', 'Ġin', 'Ġgeneral', ',', 'Ġand', 'ĠASP', 'Ġin', 'Ġparticular', ',', 'Ġhave', 'Ġalso', 'Ġgained', 'Ġpopularity', 'Ġbecause', 'Ġof', 'Ġtheir', 'Ġintuitive', ',', 'Ġdecl', 'ar', 'ative', 'Ġsyntax', '.', 'ĠThe', 'Ġfollowing', 'Ġexample', 'Ġillustrates', 'Ġthis', ':', 'ĠExample', 'Ġ1', '.', 'ĠThe', 'Ġfollowing', 'Ġrule', 'Ġnaturally', 'Ġexpresses', 'Ġthe', 'Ġfact', 'Ġthat', 'Ġtwo', 'Ġpeople', 'Ġare', 'Ġrelatives', 'Ġof', 'Ġthe', 'Ġsame', 'Ġgeneration', 'Ġup', 'Ġto', 'Ġsecond', 'Ġcousin', 'Ġif', 'Ġthey', 'Ġshare', 'Ġa', 'Ġgreat', '-', 'grand', 'parent', '.', 'Ġupt', 'ose', 'cond', 'c', 'ous', 'in', '(', 'X', ',', 'ĠY', ')', 'Ġ:', 'parent', '(', 'X', ',', 'ĠP', 'X', '),', 'Ġparent', '(', 'P', 'X', ',', 'ĠGP', 'X', '),', 'Ġparent', '(', 'GP', 'X', ',', 'ĠG', 'GP', '),', 'Ġparent', '(', 'GP', 'Y', ',', 'ĠG', 'GP', '),', 'Ġparent', '(', 'P', 'Y', ',', 'ĠGP', 'Y', '),', 'Ġparent', '(', 'Y', ',', 'ĠP', 'Y', '),', 'ĠX', 'Ġ!=', 'ĠY', '.', 'Ġ', 'Ġ', 'Č', 'Rules', 'Ġwritten', 'Ġin', 'Ġan', 'Ġintuitive', 'Ġfashion', ',', 'Ġlike', 'Ġthe', 'Ġone', 'Ġin', 'Ġthe', 'Ġabove', 'Ġexample', ',', 'Ġare', 'Ġusually', 'Ġlarger', 'Ġthan', 'Ġstrictly', 'Ġnecessary', '.', 'ĠUnfortunately', ',', 'Ġthe', 'Ġuse', 'Ġof', 'Ġlarge', 'Ġrules', 'Ġcauses', 'Ġproblems', 'Ġfor', 'Ġcurrent', 'ĠASP', 'Ġsol', 'vers', 'Ġsince', 'Ġthe', 'Ġinput', 'Ġprogram', 'Ġis', 'Ġgrounded', 'Ġfirst', 'Ġ(', 'i', '.', 'e', '.', 'Ġall', 'Ġthe', 'Ġvariables', 'Ġin', 'Ġeach', 'Ġrule', 'Ġare', 'Ġreplaced', 'Ġby', 'Ġall', 'Ġpossible', ',', 'Ġvalid', 'Ġcombinations', 'Ġof', 'Ġconstants', ').', 'ĠThis', 'Ġgrounding', 'Ġstep', 'Ġgenerally', 'Ġrequires', 'Ġexponential', 'Ġtime', 'Ġfor', 'Ġrules', 'Ġof', 'Ġarbitrary', 'Ġsize', '.', 'ĠIn', 'Ġpractice', ',', 'Ġthe', 'Ġgrounding', 'Ġtime', 'Ġcan', 'Ġthus', 'Ġbecome', 'Ġprohib', 'itive', 'ly', 'Ġlarge', '.', 'ĠAlso', ',', 'Ġthe', 'ĠASP', 'Ġsol', 'ver', 'Ġis', 'Ġusually', 'Ġquicker', 'Ġin', 'Ġevaluating', 'Ġthe', 'Ġprogram', 'Ġif', 'Ġthe', 'Ġgrounding', 'Ġsize', 'Ġremains', 'Ġsmall', '.', 'ĠIn', 'Ġorder', 'Ġto', 'Ġincrease', 'Ġsolving', 'Ġperformance', ',', 'Ġwe', 'Ġcould', 'Ġtherefore', 'Ġsplit', 'Ġthe', 'Ġrule', 'Ġin', 'ĠExample', 'Ġ1', 'Ġup', 'Ġinto', 'Ġseveral', 'Ġsmaller', 'Ġones', 'Ġby', 'Ġhand', ',', 'Ġkeeping', 'Ġtrack', 'Ġof', 'Ġgrandparents', 'Ġand', 'Ġgreat', 'grand', 'parents', 'Ġin', 'Ġseparate', 'Ġpred', 'icates', ',', 'Ġand', 'Ġthen', 'Ġwriting', 'Ġa', 'Ġsmaller', 'Ġversion', 'Ġof', 'Ġthe', 'Ġsecond', 'Ġcousin', 'Ġrule', '.', 'ĠWhile', 'Ġthis', 'Ġis', 'Ġcomparatively', 'Ġeasy', 'Ġto', 'Ġdo', 'Ġfor', 'Ġthis', 'Ġexample', ',', 'Ġthis', 'Ġcan', 'Ġbecome', 'Ġvery', 'Ġtedious', 'Ġif', 'Ġthe', 'Ġrules', 'Ġbecome', 'Ġeven', 'Ġmore', 'Ġcomplex', 'Ġand', 'Ġlarger', ',', 'Ġmaybe', 'Ġalso', 'Ġinvolving', 'Ġneg', 'ation', 'Ġor', 'Ġarithmetic', 'Ġexpressions', '.', 'ĠHowever', ',', 'Ġsince', 'Ġcurrent', 'ĠASP', 'Ġground', 'ers', 'Ġand', 'Ġsol', 'vers', 'Ġbecome', 'Ġincreasingly', 'Ġslower', 'Ġwith', 'Ġlarger', 'Ġrules', ',', 'Ġand', 'Ġnoting', 'Ġthe', 'Ġfact', 'Ġthat', 'ĠASP', 'Ġprograms', 'Ġoften', 'Ġneed', 'Ġexpert', 'Ġhand', '-', 'tun', 'ing', 'Ġto', 'Ġperform', 'Ġwell', 'Ġin', 'Ġpractice', ',', 'Ġthis', 'Ġrepresents', 'Ġa', 'Ġsignificant', 'Ġentry', 'Ġbarrier', 'Ġand', 'Ġcontradicts', 'Ġthe', 'Ġfact', 'Ġthat', 'Ġlogic', 'Ġprograms', 'Ġshould', 'Ġbe', 'Ġfully', 'Ġdecl', 'ar', 'ative', ':', 'Ġin', 'Ġa', 'Ġperfect', 'Ġworld', ',', 'Ġthe', 'Ġconcrete', 'Ġformulation', 'Ġshould', 'Ġnot', 'Ġhave', 'Ġan', 'Ġimpact', 'Ġon', 'Ġthe', 'Ġruntime', '.', 'ĠIn', 'Ġaddition', ',', 'Ġto', 'Ġminimize', 'Ġsol', 'ver', 'Ġruntime', 'Ġin', 'Ġgeneral', ',', 'Ġit', 'Ġis', 'Ġtherefore', 'Ġone', 'Ġof', 'Ġour', 'Ġgoals', 'Ġto', 'Ġenable', 'Ġlogic', 'Ġprograms', 'Ġto', 'Ġbe', 'Ġwritten', 'Ġin', 'Ġan', 'Ġintuitive', ',', 'Ġfully', 'Ġdecl', 'ar', 'ative', 'Ġway', 'Ġwithout', 'Ġhaving', 'Ġto', 'Ġthink', 'Ġabout', 'Ġvarious', 'Ġtechnical', 'Ġencoding', 'Ġoptimizations', '.', 'ĠTo', 'Ġthis', 'Ġend', ',', 'Ġin', 'Ġthis', 'Ġpaper', 'Ġwe', 'Ġpropose', 'Ġthe', 'Ġl', 'p', 'opt', 'Ġtool', 'Ġthat', 'Ġautomatically', 'Ġoptim', 'izes', 'Ġand', 'Ġre', 'writ', 'es', 'Ġlarge', 'Ġlogic', 'Ġprogramming', 'Ġrules', 'Ġinto', 'Ġmultiple', 'Ġsmaller', 'Ġones', 'Ġin', 'Ġorder', 'Ġto', 'Ġimprove', 'Ġsolving', 'Ġperformance', '.', 'ĠThis', 'Ġtool', ',', 'Ġbased', 'Ġon', 'Ġan', 'Ġidea', 'Ġproposed', 'Ġfor', 'Ġvery', 'Ġsimple', 'ĠASP', 'Ġprograms', 'Ġin', 'Ġ[', '18', '],', 'Ġuses', 'Ġthe', 'Ġconcept', 'Ġof', 'Ġtree', 'Ġdecom', 'pos', 'itions', 'Ġof', 'Ġrules', 'Ġto', 'Ġsplit', 'Ġthem', 'Ġinto', 'Ġsmaller', 'Ġchunks', '.', 'ĠInt', 'uitive', 'ly', ',', 'Ġvia', 'Ġa', 'Ġtree', 'Ġdecom', 'position', 'Ġjoins', 'Ġin', 'Ġthe', 'Ġbody', 'Ġof', 'Ġa', 'Ġrule', 'Ġare', 'Ġarranged', 'Ġinto', 'Ġa', 'Ġtree', '-', 'like', 'Ġform', '.', 'ĠJo', 'ins', 'Ġthat', 'Ġbelong', 'Ġtogether', 'Ġare', 'Ġthen', 'Ġsplit', 'Ġoff', 'Ġinto', 'Ġa', 'Ġseparate', 'Ġrule', ',', 'Ġonly', 'Ġkeeping', 'Ġthe', 'Ġjoin', 'Ġresult', 'Ġin', 'Ġa', 'Ġtemporary', 'Ġatom', '.', 'ĠWe', 'Ġthen', 'Ġextend', 'Ġthe', 'Ġalgorithm', 'Ġto', 'Ġhandle', 'Ġthe', 'Ġentire', 'Ġstandardized', 'ĠASP', 'Ġlanguage', 'Ġ[', '10', '],', 'Ġand', 'Ġalso', 'Ġintroduce', 'Ġnew', 'Ġoptimizations', 'Ġfor', 'Ġcomplex', 'Ġlanguage', 'Ġconstructs', 'Ġsuch', 'Ġas', 'Ġweak', 'Ġconstraints', ',', 'Ġarithmetic', 'Ġexpressions', ',', 'Ġand', 'Ġaggreg', 'ates', '.', 'ĠThe', 'Ġmain', 'Ġcontributions', 'Ġof', 'Ġthis', 'Ġpaper', 'Ġare', 'Ġtherefore', 'Ġas', 'Ġfollows', ':', 'ĠâĢĵ', 'Ġwe', 'Ġextend', ',', 'Ġon', 'Ġa', 'Ġtheoretical', 'Ġbasis', ',', 'Ġthe', 'Ġl', 'p', 'opt', 'Ġalgorithm', 'Ġproposed', 'Ġin', 'Ġ[', '18', ']', 'Ġto', 'Ġthe', 'Ġfull', 'Ġsyntax', 'Ġof', 'Ġthe', 'ĠASP', 'Ġlanguage', 'Ġaccording', 'Ġto', 'Ġthe', 'ĠASP', '-', 'Core', '-', '2', 'Ġlanguage', 'Ġspecification', 'Ġ[', '10', '];', 'ĠâĢĵ', 'Ġwe', 'Ġestablish', 'Ġhow', 'Ġto', 'Ġtreat', 'Ġcomplex', 'Ġconstructs', 'Ġlike', 'Ġaggreg', 'ates', ',', 'Ġand', 'Ġpropose', 'Ġan', 'Ġadaptation', 'Ġof', 'Ġthe', 'Ġdecom', 'position', 'Ġapproach', 'Ġso', 'Ġthat', 'Ġit', 'Ġcan', 'Ġsplit', 'Ġup', 'Ġlarge', 'Ġaggregate', 'Ġexpressions', 'Ġinto', 'Ġmultiple', 'Ġsmaller', 'Ġrules', 'Ġand', 'Ġexpressions', ',', 'Ġfurther', 'Ġreducing', 'Ġthe', 'Ġgrounding', 'Ġsize', ';', 'ĠâĢĵ', 'Ġwe', 'Ġimplement', 'Ġthe', 'Ġl', 'p', 'opt', 'Ġalgorithm', 'Ġin', 'ĠC', '++', ',', 'Ġyielding', 'Ġthe', 'Ġl', 'p', 'opt', 'Ġtool', 'Ġfor', 'Ġautomated', 'Ġlogic', 'Ġprogram', 'Ġoptimization', ',', 'Ġand', 'Ġgive', 'Ġan', 'Ġoverview', 'Ġof', 'Ġhow', 'Ġthis', 'Ġtool', 'Ġis', 'Ġused', 'Ġin', 'Ġpractice', ';', 'Ġand', 'ĠâĢĵ', 'Ġwe', 'Ġperform', 'Ġan', 'Ġexperimental', 'Ġevaluation', 'Ġof', 'Ġthe', 'Ġtool', 'Ġon', 'Ġthe', 'Ġenc', 'od', 'ings', 'Ġand', 'Ġinstances', 'Ġused', 'Ġin', 'Ġthe', 'Ġfifth', 'ĠAnswer', 'ĠSet', 'ĠProgramming', 'ĠCompetition', 'Ġwhich', 'Ġshow', 'Ġthe', 'Ġbenefit', 'Ġof', 'Ġour', 'Ġapproach', ',', 'Ġeven', 'Ġfor', 'Ġenc', 'od', 'ings', 'Ġalready', 'Ġheavily', 'Ġhand', '-', 'optim', 'ized', 'Ġby', 'ĠASP', 'Ġexperts', '.', 'Ġ', 'Ġ', 'Č', '2', 'Ġ', 'ĠPrel', 'im', 'in', 'aries', 'Ġ', 'ĠGeneral', 'ĠDefinitions', '.', 'ĠWe', 'Ġdefine', 'Ġtwo', 'Ġpair', 'wise', 'Ġdis', 'j', 'oint', 'Ġcount', 'ably', 'Ġinfinite', 'Ġsets', 'Ġof', 'Ġsymbols', ':', 'Ġa', 'Ġset', 'ĠC', 'Ġof', 'Ġconstants', 'Ġand', 'Ġa', 'Ġset', 'ĠV', 'Ġof', 'Ġvariables', '.', 'ĠDifferent', 'Ġconstants', 'Ġrepresent', 'Ġdifferent', 'Ġvalues', 'Ġ(', 'unique', 'Ġname', 'Ġassumption', ').', 'ĠBy', 'ĠX', 'Ġwe', 'Ġdenote', 'Ġsequences', 'Ġ(', 'or', ',', 'Ġwith', 'Ġslight', 'Ġnot', 'ational', 'Ġabuse', ',', 'Ġsets', ')', 'Ġof', 'Ġvariables', 'ĠX', '1', 'Ġ,', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ,', 'ĠX', 'k', 'Ġwith', 'Ġk', 'Ġ>', 'Ġ0', '.', 'ĠFor', 'Ġbre', 'vity', ',', 'Ġlet', 'Ġ[', 'n', ']', 'Ġ=', 'Ġ{', '1', ',', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ,', 'Ġn', '},', 'Ġfor', 'Ġany', 'Ġinteger', 'Ġn', 'Ġ>', 'Ġ1', '.', 'ĠA', 'Ġ(', 'rel', 'ational', ')', 'Ġschema', 'ĠS', 'Ġis', 'Ġa', 'Ġ(', 'f', 'inite', ')', 'Ġset', 'Ġof', 'Ġrelational', 'Ġsymbols', 'Ġ(', 'or', 'Ġpred', 'icates', ').', 'ĠWe', 'Ġwrite', 'Ġp', '/', 'n', 'Ġfor', 'Ġthe', 'Ġfact', 'Ġthat', 'Ġp', 'Ġis', 'Ġan', 'Ġn', '-', 'ary', 'Ġpredicate', '.', 'ĠA', 'Ġterm', 'Ġis', 'Ġa', 'Ġconstant', 'Ġor', 'Ġvariable', '.', 'ĠAn', 'Ġatomic', 'Ġformula', 'Ġa', 'Ġover', 'ĠS', 'Ġ(', 'called', 'ĠS', '-', 'atom', ')', 'Ġhas', 'Ġthe', 'Ġform', 'Ġp', '(', 't', '),', 'Ġwhere', 'Ġp', 'ĠâĪ', 'Ī', 'ĠS', 'Ġand', 'Ġt', 'Ġis', 'Ġa', 'Ġsequence', 'Ġof', 'Ġterms', '.', 'ĠAn', 'ĠS', '-', 'lit', 'eral', 'Ġis', 'Ġeither', 'Ġan', 'ĠS', '-', 'atom', 'Ġ(', 'i', '.', 'e', '.', 'Ġa', 'Ġpositive', 'Ġliteral', '),', 'Ġor', 'Ġan', 'ĠS', '-', 'atom', 'Ġpreceded', 'Ġby', 'Ġthe', 'Ġneg', 'ation', 'Ġsymbol', 'ĠâĢ', 'ľ', 'Â', '¬', 'âĢ', 'Ŀ', 'Ġ(', 'i', '.', 'e', '.', 'Ġa', 'Ġnegative', 'Ġliteral', ').', 'ĠFor', 'Ġa', 'Ġliteral', 'Ġ`', ',', 'Ġwe', 'Ġwrite', 'Ġdom', '(', '`', ')', 'Ġfor', 'Ġthe', 'Ġset', 'Ġof', 'Ġits', 'Ġterms', ',', 'Ġand', 'Ġvar', 'Ġ(', '`', ')', 'Ġfor', 'Ġits', 'Ġvariables', '.', 'ĠThis', 'Ġnotation', 'Ġnaturally', 'Ġextends', 'Ġto', 'Ġsets', 'Ġof', 'Ġliter', 'als', '.', 'ĠFor', 'Ġbre', 'vity', ',', 'Ġwe', 'Ġwill', 'Ġtreat', 'Ġconj', 'unctions', 'Ġof', 'Ġliter', 'als', 'Ġas', 'Ġsets', '.', 'ĠFor', 'Ġa', 'Ġdomain', 'ĠC', 'Ġâ', 'Ĭ', 'Ĩ', 'ĠC', ',', 'Ġa', 'Ġ(', 'total', 'Ġor', 'Ġtwo', '-', 'valued', ')', 'ĠS', '-', 'interpret', 'ation', 'ĠI', 'Ġis', 'Ġa', 'Ġset', 'Ġof', 'ĠS', '-', 'at', 'oms', 'Ġcontaining', 'Ġonly', 'Ġconstants', 'Ġfrom', 'ĠC', 'Ġsuch', 'Ġthat', ',', 'Ġfor', 'Ġevery', 'ĠS', '-', 'atom', 'Ġp', '(', 'a', ')', 'ĠâĪ', 'Ī', 'ĠI', ',', 'Ġp', '(', 'a', ')', 'Ġis', 'Ġtrue', ',', 'Ġand', 'Ġotherwise', 'Ġfalse', '.', 'ĠWhen', 'Ġobvious', 'Ġfrom', 'Ġthe', 'Ġcontext', ',', 'Ġwe', 'Ġwill', 'Ġomit', 'Ġthe', 'Ġschema', '-', 'prefix', '.', 'ĠA', 'Ġsubstitution', 'Ġfrom', 'Ġa', 'Ġset', 'Ġof', 'Ġliter', 'als', 'ĠL', 'Ġto', 'Ġa', 'Ġset', 'Ġof', 'Ġliter', 'als', 'ĠL', '0', 'Ġis', 'Ġa', 'Ġmapping', 'Ġs', 'Ġ:', 'ĠC', 'âĪ', 'ª', 'V', 'ĠâĨĴ', 'ĠC', 'ĠâĪ', 'ª', 'ĠV', 'Ġthat', 'Ġis', 'Ġdefined', 'Ġon', 'Ġdom', '(', 'L', '),', 'Ġis', 'Ġthe', 'Ġidentity', 'Ġon', 'ĠC', ',', 'Ġand', 'Ġp', '(', 't', '1', 'Ġ,', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ,', 'Ġt', 'n', 'Ġ)', 'ĠâĪ', 'Ī', 'ĠL', 'Ġ(', 'resp', '.', 'ĠÂ', '¬', 'p', '(', 't', '1', 'Ġ,', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ,', 'Ġt', 'n', 'Ġ)', 'ĠâĪ', 'Ī', 'ĠL', ')', 'Ġimplies', 'Ġp', '(', 's', '(', 't', '1', 'Ġ),', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ,', 'Ġs', '(', 'tn', 'Ġ))', 'ĠâĪ', 'Ī', 'ĠL', '0', 'Ġ(', 'resp', '.,', 'ĠÂ', '¬', 'p', '(', 's', '(', 't', '1', 'Ġ),', 'Ġ.', 'Ġ.', 'Ġ.', 'Ġ,', 'Ġs', '(', 'tn', 'Ġ))', 'ĠâĪ', 'Ī', 'ĠL', '0', 'Ġ).', 'ĠAnswer', 'ĠSet', 'ĠProgramming', 'Ġ(', 'AS', 'P', ').', 'ĠA', 'Ġlogic', 'Ġprogramming', 'Ġrule', 'Ġis', 'Ġa', 'Ġuniversally', 'Ġquant', 'ified', 'Ġreverse', 'Ġfirst', '-', 'order', 'Ġimplication', 'Ġof', 'Ġthe', 'Ġform', 'ĠH', '(', 'X', ',', 'ĠY', ')', 'ĠâĨ', 'Ĳ', 'ĠB', 'Ġ+', 'Ġ(', 'X', ',', 'ĠY', ',', 'ĠZ', ',', 'ĠW', ')', 'ĠâĪ', '§', 'ĠB', 'ĠâĪĴ', 'Ġ(', 'X', ',', 'ĠZ', '),', 'Ġwhere', 'ĠH', 'Ġ(', 'the', 'Ġhead', '),', 'Ġresp', '.', 'ĠB', 'Ġ+', 'Ġ(', 'the', 'Ġpositive', 'Ġbody', '),', 'Ġis', 'Ġa', 'Ġdis', 'j', 'unction', ',', 'Ġresp', '.', 'Ġconjunction', ',', 'Ġof', 'Ġatoms', ',', 'Ġand', 'ĠB', 'ĠâĪĴ', 'Ġ(', 'the', 'Ġnegative', 'Ġbody', ')', 'Ġis', 'Ġa', 'Ġconjunction', 'Ġof', 'Ġnegative', 'Ġliter', 'als', ',', 'Ġeach', 'Ġover', 'Ġterms', 'Ġfrom', 'ĠC', 'ĠâĪ', 'ª', 'ĠV', '.', 'ĠFor', 'Ġa', 'Ġrule', 'ĠÏ', 'Ģ', ',', 'Ġlet', 'ĠH', 'Ġ(', 'ÏĢ', '),', 'ĠB', 'Ġ+', 'Ġ(', 'ÏĢ', '),', 'Ġand', 'ĠB', 'ĠâĪĴ', 'Ġ(', 'ÏĢ', ')', 'Ġdenote', 'Ġthe', 'Ġset', 'Ġof', 'Ġatoms', 'Ġoccurring', 'Ġin', 'Ġthe', 'Ġhead', ',', 'Ġthe', 'Ġpositive', ',', 'Ġand', 'Ġthe', 'Ġnegative', 'Ġbody', ',', 'Ġrespectively', '.', 'ĠLet', 'ĠB', 'Ġ(', 'ÏĢ', ')', 'Ġ=', 'ĠB', 'Ġ+', 'Ġ(', 'ÏĢ', ')', 'ĠâĪ', 'ª', 'ĠB', 'ĠâĪĴ', 'Ġ(', 'ÏĢ', ').', 'ĠA', 'Ġrule', 'ĠÏ', 'Ģ', 'Ġwhere', 'ĠH', 'Ġ(', 'ÏĢ', ')', 'Ġ=', 'ĠâĪ', 'ħ', 'Ġis', 'Ġcalled', 'Ġa', 'Ġconstraint', '.', 'ĠSubst', 'it', 'utions', 'Ġnaturally', 'Ġextend', 'Ġto', 'Ġrules', '.', 'ĠWe', 'Ġfocus', 'Ġon', 'Ġsafe', 'Ġrules', 'Ġwhere', 'Ġevery', 'Ġvariable', 'Ġin', 'Ġthe', 'Ġrule', 'Ġoccurs', 'Ġin', 'Ġthe', 'Ġpositive', 'Ġbody', '.', 'ĠA', 'Ġrule', 'Ġis', 'Ġcalled', 'Ġground', 'Ġif', 'Ġall', 'Ġits', 'Ġterms', 'Ġare', 'Ġconstants', '.', 'ĠThe', 'Ġgrounding', 'Ġof', 'Ġa', 'Ġrule', 'ĠÏ', 'Ģ', 'Ġw', '.', 'r', '.', 't', '.', 'Ġa', 'Ġdomain', 'ĠC', 'Ġâ', 'Ĭ', 'Ĩ', 'ĠC', 'Ġis', 'Ġthe', 'Ġset', 'Ġof', 'Ġrules', 'Ġground', 'ĠC', 'Ġ(', 'ÏĢ', ')', 'Ġ=', 'Ġ{', 's', '(', 'ÏĢ', ')', 'Ġ|', 'Ġs', 'Ġis', 'Ġa', 'Ġsubstitution', ',', 'Ġmapping', 'Ġvar', 'Ġ(', 'ÏĢ', ')', 'Ġto', 'Ġelements', 'Ġfrom', 'ĠC', '}.', 'ĠA', 'Ġlogic', 'Ġprogram', 'ĠÎ', 'ł', 'Ġis', 'Ġa', 'Ġfinite', 'Ġset', 'Ġof', 'Ġlogic', 'Ġprogramming', 'Ġrules', '.', 'ĠThe', 'Ġschema', 'Ġof', 'Ġa', 'Ġprogram', 'ĠÎ', 'ł', ',', 'Ġden', 'oted', 'Ġsch', '(', 'Î', 'ł', '),', 'Ġis', 'Ġthe', 'Ġset', 'Ġof', 'Ġpred', 'icates', 'Ġappearing', 'Ġin', 'ĠÎ', 'ł', '.', 'ĠThe', 'Ġactive', 'Ġdomain', 'Ġof', 'ĠÎ', 'ł', ',', 'Ġden', 'oted', 'Ġad', 'om', '(', 'Î', 'ł', '),', 'Ġwith', 'Ġad', 'om', '(', 'Î', 'ł', ')', 'Ġâ', 'Ĭ', 'Ĥ', 'ĠC', ',', 'Ġis', 'Ġthe', 'Ġset', 'Ġof', '</s>']\n"
     ]
    }
   ],
   "source": [
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1654646688973,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "-g6r5FPMbcgG"
   },
   "outputs": [],
   "source": [
    "#set 1 if we are dealing with a positive class, and 0 if dealing with negative class\n",
    "def custom_forward2(inputs_emb, global_attention_mask) :\n",
    "    preds = model(inputs_embeds=inputs_emb, global_attention_mask=global_attention_mask)\n",
    "    return torch.softmax(preds.logits, dim = 1)[:, 0] # for negative attribution, \n",
    "    #return torch.softmax(preds, dim = 1)[:, 1] #<- for positive attribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1654646688973,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "tcaYnQVKbeKg"
   },
   "outputs": [],
   "source": [
    "def construct_whole_longformer_embeddings(input_ids, ref_input_ids, \\\n",
    "                                          token_type_ids=None, ref_token_type_ids=None, \\\n",
    "                                          position_ids=None, ref_position_ids=None):\n",
    "    input_embeddings = model.longformer.embeddings(input_ids, token_type_ids=token_type_ids, position_ids=position_ids)\n",
    "    ref_input_embeddings = model.longformer.embeddings(ref_input_ids, token_type_ids=ref_token_type_ids, position_ids=ref_position_ids)\n",
    "    \n",
    "    print(input_embeddings)\n",
    "    return input_embeddings, ref_input_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1654646688973,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "J1nmeVVUcrny",
    "outputId": "a38ac939-02aa-46bc-e035-637242eb0580"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.2746, -0.0095,  0.3482,  ..., -0.0345, -0.0225, -0.1789],\n",
      "         [-0.4242, -0.0695,  0.5289,  ..., -0.4890, -0.5733, -0.4481],\n",
      "         [-0.0438,  0.2288,  0.6584,  ...,  0.3309, -0.0427,  0.2786],\n",
      "         ...,\n",
      "         [-0.1743, -0.3578,  0.0495,  ..., -0.1654,  0.2040, -0.7195],\n",
      "         [-0.1707, -0.1062,  0.1560,  ...,  0.2212,  0.1290,  0.1111],\n",
      "         [ 0.0133,  0.4446, -0.2146,  ...,  0.1282,  0.2887, -0.0445]]],\n",
      "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>)\n",
      "torch.Size([1, 2050, 768])\n",
      "tensor([[1, 0, 0,  ..., 0, 0, 0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "layer_attrs = []\n",
    "\n",
    "# The token that we would like to examine separately.\n",
    "token_to_explain = 334 # the index of the token that we would like to examine more thoroughly\n",
    "layer_attrs_dist = []\n",
    "\n",
    "input_embeddings, ref_input_embeddings = construct_whole_longformer_embeddings(input_ids, ref_input_ids, \\\n",
    "                                         position_ids=position_ids, ref_position_ids=ref_position_ids)\n",
    "\n",
    "print(input_embeddings.shape)\n",
    "globalattention= torch.zeros_like(input_ids)\n",
    "globalattention[:, 0] = 1\n",
    "print(globalattention)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 415
    },
    "executionInfo": {
     "elapsed": 17282,
     "status": "error",
     "timestamp": 1654646706247,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "i7BK1FQseqdq",
    "outputId": "50297a5b-8f32-45ea-cf9a-3967bea7344a"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-9cf2a209cc3d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m                                       \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobalattention\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                       \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m                                       internal_batch_size=2)\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mlayer_attrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msummarize_attributions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer_attributions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/log/__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     33\u001b[0m             \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/attr/_core/layer/layer_conductance.py\u001b[0m in \u001b[0;36mattribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, internal_batch_size, return_convergence_delta, attribute_to_layer_input)\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madditional_forward_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m                 \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 287\u001b[0;31m                 \u001b[0mattribute_to_layer_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattribute_to_layer_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    288\u001b[0m             )\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/attr/_utils/batching.py\u001b[0m in \u001b[0;36m_batch_attribution\u001b[0;34m(attr_method, num_examples, internal_batch_size, n_steps, include_endpoint, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0malphas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfull_alphas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart_step\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mend_step\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         current_attr = attr_method._attribute(\n\u001b[0;32m---> 79\u001b[0;31m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_steps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep_sizes_and_alphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_sizes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m         )\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/attr/_core/layer/layer_conductance.py\u001b[0m in \u001b[0;36m_attribute\u001b[0;34m(self, inputs, baselines, target, additional_forward_args, n_steps, method, attribute_to_layer_input, step_sizes_and_alphas)\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0mtarget_ind\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpanded_target\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m             \u001b[0mdevice_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m             \u001b[0mattribute_to_layer_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattribute_to_layer_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m         )\n\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/_utils/gradient.py\u001b[0m in \u001b[0;36mcompute_layer_gradients_and_eval\u001b[0;34m(forward_fn, layer, inputs, target_ind, additional_forward_args, gradient_neuron_selector, device_ids, attribute_to_layer_input, output_fn)\u001b[0m\n\u001b[1;32m    598\u001b[0m             \u001b[0mattribute_to_layer_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattribute_to_layer_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m             \u001b[0mforward_hook_with_return\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m             \u001b[0mrequire_layer_grads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m         )\n\u001b[1;32m    602\u001b[0m         assert output[0].numel() == 1, (\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/_utils/gradient.py\u001b[0m in \u001b[0;36m_forward_layer_distributed_eval\u001b[0;34m(forward_fn, inputs, layer, target_ind, additional_forward_args, attribute_to_layer_input, forward_hook_with_return, require_layer_grads)\u001b[0m\n\u001b[1;32m    296\u001b[0m             \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtarget_ind\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 298\u001b[0;31m             \u001b[0madditional_forward_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madditional_forward_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    299\u001b[0m         )\n\u001b[1;32m    300\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/captum/_utils/common.py\u001b[0m in \u001b[0;36m_run_forward\u001b[0;34m(forward_func, inputs, target, additional_forward_args)\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0madditional_forward_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madditional_forward_args\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m         \u001b[0;32melse\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m     )\n\u001b[1;32m    461\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_select_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-ef3d2b1e7305>\u001b[0m in \u001b[0;36mcustom_forward2\u001b[0;34m(inputs_emb, global_attention_mask)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#set 1 if we are dealing with a positive class, and 0 if dealing with negative class\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcustom_forward2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs_embeds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs_emb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_attention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mglobal_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;31m# for negative attribution,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#return torch.softmax(preds, dim = 1)[:, 1] #<- for positive attribution\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1891\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1892\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1893\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1894\u001b[0m         )\n\u001b[1;32m   1895\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1708\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1709\u001b[0m             \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1710\u001b[0;31m             \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1711\u001b[0m         )\n\u001b[1;32m   1712\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, padding_len, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1290\u001b[0m                     \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m                     \u001b[0mis_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m                     \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m                 )\n\u001b[1;32m   1294\u001b[0m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1214\u001b[0m             \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m             \u001b[0mis_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m         )\n\u001b[1;32m   1218\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself_attn_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m   1150\u001b[0m             \u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0mis_global_attn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_global_attn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1152\u001b[0;31m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1153\u001b[0m         )\n\u001b[1;32m   1154\u001b[0m         \u001b[0mattn_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1108\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1111\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    659\u001b[0m                 \u001b[0mmax_num_global_attn_indices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_num_global_attn_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    660\u001b[0m                 \u001b[0mis_index_global_attn_nonzero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_index_global_attn_nonzero\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 661\u001b[0;31m                 \u001b[0mis_local_index_global_attn_nonzero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mis_local_index_global_attn_nonzero\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    662\u001b[0m             )\n\u001b[1;32m    663\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36m_compute_attn_output_with_global_indices\u001b[0;34m(self, value_vectors, attn_probs, max_num_global_attn_indices, is_index_global_attn_nonzero, is_local_index_global_attn_nonzero)\u001b[0m\n\u001b[1;32m    991\u001b[0m         \u001b[0;31m# compute attn output with global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         attn_output_without_global = self._sliding_chunks_matmul_attn_probs_value(\n\u001b[0;32m--> 993\u001b[0;31m             \u001b[0mattn_probs_without_global\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_sided_attn_window_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    994\u001b[0m         )\n\u001b[1;32m    995\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mattn_output_only_global\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn_output_without_global\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36m_sliding_chunks_matmul_attn_probs_value\u001b[0;34m(self, attn_probs, value, window_overlap)\u001b[0m\n\u001b[1;32m    898\u001b[0m         \u001b[0mchunked_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadded_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_strided\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked_value_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunked_value_stride\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    899\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 900\u001b[0;31m         \u001b[0mchunked_attn_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pad_and_diagonalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunked_attn_probs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    901\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    902\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bcwd,bcdh->bcwh\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunked_attn_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunked_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/models/longformer/modeling_longformer.py\u001b[0m in \u001b[0;36m_pad_and_diagonalize\u001b[0;34m(chunked_hidden_states)\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0mtotal_num_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_overlap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mchunked_hidden_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    751\u001b[0m         chunked_hidden_states = nn.functional.pad(\n\u001b[0;32m--> 752\u001b[0;31m             \u001b[0mchunked_hidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_overlap\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    753\u001b[0m         )  # total_num_heads x num_chunks x window_overlap x (hidden_dim+window_overlap+1). Padding value is not important because it'll be overwritten\n\u001b[1;32m    754\u001b[0m         chunked_hidden_states = chunked_hidden_states.view(\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36m_pad\u001b[0;34m(input, pad, mode, value)\u001b[0m\n\u001b[1;32m   4362\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Padding length too large\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4363\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"constant\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4364\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_pad_nd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4365\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4366\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Padding mode \"{}\"\" doesn\\'t take in value argument'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 182.00 MiB (GPU 0; 14.76 GiB total capacity; 12.45 GiB already allocated; 163.75 MiB free; 13.21 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "for i in range(model.config.num_hidden_layers):\n",
    "    lc = LayerConductance(custom_forward2, model.longformer.encoder.layer[i])\n",
    "    layer_attributions = lc.attribute(inputs=input_embeddings, \n",
    "                                      baselines=ref_input_embeddings, \n",
    "                                      additional_forward_args=globalattention, \n",
    "                                      n_steps=5, \n",
    "                                      internal_batch_size=2)\n",
    "    layer_attrs.append(summarize_attributions(layer_attributions).cpu().detach().tolist())\n",
    "\n",
    "    # storing attributions of the token id that we would like to examine in more detail in token_to_explain\n",
    "    layer_attrs_dist.append(layer_attributions[0,token_to_explain,:].cpu().detach().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1654646706246,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "ZT1fCVlgku87"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1654646706247,
     "user": {
      "displayName": "daniel hou",
      "userId": "13623878136116974888"
     },
     "user_tz": 420
    },
    "id": "x9EzMPGmjh_7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "xticklabels=all_tokens\n",
    "yticklabels=list(range(1,13))\n",
    "ax = sns.heatmap(np.array(layer_attrs), xticklabels=xticklabels, yticklabels=yticklabels, linewidth=0.2)\n",
    "plt.xlabel('Tokens')\n",
    "plt.ylabel('Layers')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CaptumLongformerSequenceClassificationPapersBertLayers.ipynb",
   "provenance": [
    {
     "file_id": "1pptTYAJGp7tl0BhVQoTD5RGyQMEVF766",
     "timestamp": 1654644660504
    },
    {
     "file_id": "1-zUACrWJtGVU71pkZ-kswoaOj6bn93RG",
     "timestamp": 1654640879422
    }
   ]
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0b022ab327f84c64ac9c810a76e1b287": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1af916f5e2e34049b92a638978f417c7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4953f1840f5649de9d31eb95223a45ce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e2c23bd048a434e9c9e561c6eb03cff": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "514692f5f7f84154aaecd3e4b9a070f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b022ab327f84c64ac9c810a76e1b287",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_4e2c23bd048a434e9c9e561c6eb03cff",
      "value": 2
     }
    },
    "96a7a90aa4674e4196808ce4a58a078f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_bbbeb6d370d343ceba0abccc4102b8b1",
       "IPY_MODEL_514692f5f7f84154aaecd3e4b9a070f0",
       "IPY_MODEL_cb1fc6fd97f842d3993e953e1e4af976"
      ],
      "layout": "IPY_MODEL_4953f1840f5649de9d31eb95223a45ce"
     }
    },
    "bbbeb6d370d343ceba0abccc4102b8b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bce2d2221ed943518b7b1414e22da7fe",
      "placeholder": "​",
      "style": "IPY_MODEL_1af916f5e2e34049b92a638978f417c7",
      "value": "100%"
     }
    },
    "bce2d2221ed943518b7b1414e22da7fe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c1465032d27e4b5ba8bb94c69d72b65c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "cb1fc6fd97f842d3993e953e1e4af976": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e49d2d39dd8a4bf0876519f43a498a97",
      "placeholder": "​",
      "style": "IPY_MODEL_c1465032d27e4b5ba8bb94c69d72b65c",
      "value": " 2/2 [00:00&lt;00:00,  5.21it/s]"
     }
    },
    "e49d2d39dd8a4bf0876519f43a498a97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
