{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "A key feature of Transformer neural networks is the attention feature. This attention is generally a sequence length x sequence length matrix output for every layer, batch and head of an input. As such, for every layer, batch, head, we can find out information about each token, whether its about what tokens a particular token attends to, or the most attended to token for each matrix. This notebook takes an example from a dataset, and explores the attentions of each token in depth. Notably, we find out what tokens each token attends to the most and what tokens that get the most attention, across all layers and heads."
      ],
      "metadata": {
        "id": "m1Bc0bw-mNkB"
      },
      "id": "m1Bc0bw-mNkB"
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "kZTPlUo8Wp1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b271781-8e7f-4e8f-c7ef-757af8c01574"
      },
      "id": "kZTPlUo8Wp1T",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import sys\n",
        "# sys.path.append('/content/drive/My Drive/{}'.format(\"cogs402longformer/\"))"
      ],
      "metadata": {
        "id": "AlcBiC0nWtJE"
      },
      "id": "AlcBiC0nWtJE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Install and Import Dependencies"
      ],
      "metadata": {
        "id": "-nmZtunIuDIM"
      },
      "id": "-nmZtunIuDIM"
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets --quiet"
      ],
      "metadata": {
        "id": "s_6vceLhTIBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98a824fd-1cfe-4acc-ef2d-643bc902322a"
      },
      "id": "s_6vceLhTIBf",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 365 kB 7.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 100.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 115 kB 104.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 89.5 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 95.3 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ipywidgets --quiet"
      ],
      "metadata": {
        "id": "g9mGhCoRTK_G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60b9bffc-965d-451f-965f-75e8f350997c"
      },
      "id": "g9mGhCoRTK_G",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l\r\u001b[K     |▏                               | 10 kB 33.4 MB/s eta 0:00:01\r\u001b[K     |▍                               | 20 kB 9.8 MB/s eta 0:00:01\r\u001b[K     |▋                               | 30 kB 13.8 MB/s eta 0:00:01\r\u001b[K     |▉                               | 40 kB 8.3 MB/s eta 0:00:01\r\u001b[K     |█                               | 51 kB 9.0 MB/s eta 0:00:01\r\u001b[K     |█▎                              | 61 kB 10.6 MB/s eta 0:00:01\r\u001b[K     |█▌                              | 71 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |█▊                              | 81 kB 10.7 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 92 kB 8.8 MB/s eta 0:00:01\r\u001b[K     |██                              | 102 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 112 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 122 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 133 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███                             | 143 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 153 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███▍                            | 163 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 174 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 184 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████                            | 194 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 204 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 215 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 225 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 235 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████                           | 245 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 256 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████▍                          | 266 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 276 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████▉                          | 286 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████                          | 296 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 307 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████▌                         | 317 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 327 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 337 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████                         | 348 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 358 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████▌                        | 368 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████▊                        | 378 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████                        | 389 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████▏                       | 399 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 409 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 419 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████▉                       | 430 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 440 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 450 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 460 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 471 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 481 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████                      | 491 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 501 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████▌                     | 512 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 522 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 532 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 542 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 552 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 563 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 573 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 583 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 593 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 604 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 614 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 624 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 634 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▏                  | 645 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▍                  | 655 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 665 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 675 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 686 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▏                 | 696 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▍                 | 706 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 716 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████▉                 | 727 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 737 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 747 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▌                | 757 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████▊                | 768 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 778 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 788 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 798 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 808 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 819 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 829 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 839 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▍              | 849 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 860 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 870 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 880 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 890 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 901 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 911 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 921 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 931 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 942 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 952 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 962 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████▉            | 972 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 983 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 993 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 1.0 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 1.0 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 1.0 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 1.0 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 1.0 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▌          | 1.1 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▊          | 1.1 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 1.1 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▏         | 1.1 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▍         | 1.1 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 1.1 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▉         | 1.1 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.1 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.1 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.1 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 1.2 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 1.2 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.2 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 1.2 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▌       | 1.2 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▊       | 1.2 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.2 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.2 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.2 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.2 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.3 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.3 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.3 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.3 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 1.3 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▊     | 1.3 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.3 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.3 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.3 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.4 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.4 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.4 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.4 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▍   | 1.4 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 1.4 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 1.4 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.4 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▎  | 1.4 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 1.4 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▊  | 1.5 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.5 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▎ | 1.5 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 1.5 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▊ | 1.5 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.5 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 1.5 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▍| 1.5 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▋| 1.5 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 1.6 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.6 MB 9.6 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "M-qXH2JkTMdA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28bfa255-5c88-47c1-bd13-3badd06b15dc"
      },
      "id": "M-qXH2JkTMdA",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 9.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.9.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 55.8 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.9.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n",
            "Installing collected packages: tokenizers, transformers\n",
            "Successfully installed tokenizers-0.12.1 transformers-4.22.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b774ad0-b725-4910-9050-423edf160ebd",
      "metadata": {
        "id": "9b774ad0-b725-4910-9050-423edf160ebd"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Import Dataset and Model"
      ],
      "metadata": {
        "id": "hSSPoSRn6r9A"
      },
      "id": "hSSPoSRn6r9A"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import the Reserach Papers dataset"
      ],
      "metadata": {
        "id": "hfHRqrpw_VlN"
      },
      "id": "hfHRqrpw_VlN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "66cc97f5-7e3a-476c-9858-5643eeaa6675",
      "metadata": {
        "id": "66cc97f5-7e3a-476c-9858-5643eeaa6675",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 232,
          "referenced_widgets": [
            "348e26a35773433f9af6c66e8b2062ce",
            "c0757ff78220438f9a31d1ebc0713ebb",
            "f0c7705644f94593826ad4f9f701b856",
            "388694d265c747ae84b2aac6c98393ca",
            "47e752177a6343ac9d21e137cbbe14f9",
            "2a6b996d2dc8435ea348476c1ca1480e",
            "92a857a6e22b467aaff96c900767d1e1",
            "6f733a726808498293069cfba37480d4",
            "339d250cc3294fe59effc79a113e5dde",
            "7b6b9fd0cd624cfc86c3002d4dee721d",
            "491628585358421a85c3ab9b1f91610f",
            "a1402dcbda72454bb59afe4533ecd258",
            "2664f3e8dd514c7e91a734ec09f327b8",
            "90468f8b120b4f6b973e79fc11398e82",
            "b700b5245e17421a9d108cc3e72a5ab4",
            "052a1dc0347241d9be256d7555cc4e30",
            "4e6c738eea564dcfb39825da03ce8920",
            "9ad9553934664533bb3c9766ddf861e3",
            "1a00966110804e8b9d4ef91f09b7cce9",
            "c14cb9bc189344599f640a212d28ca22",
            "64503a1f9dce4415818bdf3066785f26",
            "f573268e822e441592e5333b9f78b72e",
            "f0c412046fb649fc80bbc0c6542437a9",
            "1187826e293f486f8e2dfdc1a2e97cf1",
            "c1835b81be1e425097e53636f8197f6e",
            "78be2670cf544233bdad53a9582c15a0",
            "87bbb54fe3e54547a906d2b0fba71095",
            "db14b41a9b594a8aa9d8fbcdf8a8a0de",
            "d51901cfc903479c8c7e0cc538581f1b",
            "5e6aebe3a5414949bf8d9b53a6f6dd0c",
            "c35677775e334eaca07503b3f62c92af",
            "0a746881b6ce4ac7a477ad502fbfe4cb",
            "a3b3a1d9527c49e69a3c3d0fc64594d3",
            "272fc32b90c5417289de2fb6c63d406c",
            "d9eb52445156467897edd5ac51bbf5aa",
            "32a34884830b45179c6bd8db50f1b369",
            "c13099dcce374bb9ac020086bfc94fe1",
            "bd01cb009e1b48c2a6d15356da74dd3a",
            "a4d13f72b9b5459b8a7aa3ece1ea5dd0",
            "16e27eac089048d0bcdfe6846304f60b",
            "527c3e60175743c4bfb360d535e743f6",
            "a44d7cb0802d4bc5a884587dda09a532",
            "585998d4cfc04847bbfb1b5f34054c59",
            "7218e94823b64456ad0add357ec97f14",
            "ca15c95a8f204ccbab20c43782318263",
            "2e73acbc9f1c4ebcb3a1be8d6c23df9a",
            "2b7bba0b5dfb4531b94d893c73c78e81",
            "46ce169281ad4f788378abbb75182a78",
            "148a727a895440de9fc194de1cc0db75",
            "e30d5b4766324fba8c785a945da19682",
            "c1c2c3b3f33644c998dcaff70887b3ec",
            "10bf386daba64491a6f4b59ef5caf43f",
            "6cb149b31c0c4b50b1fb78584ed028a7",
            "27e8f6f6872c4664a483f553807f074a",
            "f505cb2974c8458ab0bf71c824938bb0"
          ]
        },
        "outputId": "6d0f53d8-1315-48ce-f934-6dadd71f4678"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moving 0 files to the new cache system\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "348e26a35773433f9af6c66e8b2062ce"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/694 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a1402dcbda72454bb59afe4533ecd258"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f0c412046fb649fc80bbc0c6542437a9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "272fc32b90c5417289de2fb6c63d406c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ca15c95a8f204ccbab20c43782318263"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "import datasets\n",
        "from datasets import load_dataset\n",
        "from transformers import LongformerForSequenceClassification, AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "\n",
        "def longformer_finetuned_notes():\n",
        "    test = torch.load(\"/content/drive/MyDrive/cogs402longformer/fakeclinicalnotes/models/full_augmented_lr2e-5_dropout3_10_trained_threshold.pt\")\n",
        "    model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', state_dict=test['state_dict'], num_labels = 2)\n",
        "    return model\n",
        "\n",
        "def preprocess_function(tokenizer, example, max_length):\n",
        "    example['text'] = [str(x).lower() for x in example['text']]\n",
        "    example.update(tokenizer(example['text'], padding='max_length', max_length=max_length, truncation=True))\n",
        "    return example\n",
        "\n",
        "def get_notes_dataset(split):\n",
        "    max_length = 2048\n",
        "    # dataset = load_dataset(\"danielhou13/cogs402datafake\")[dataset_type]\n",
        "    dataset = split\n",
        "    \n",
        "    # tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "    dataset = dataset.map(lambda x: preprocess_function(tokenizer, x, max_length), batched=True)\n",
        "    setattr(dataset, 'input_columns', ['input_ids', 'attention_mask'])\n",
        "    setattr(dataset, 'target_columns', ['labels'])\n",
        "    setattr(dataset, 'max_length', max_length)\n",
        "    setattr(dataset, 'tokenizer', tokenizer)\n",
        "    return dataset\n",
        "\n",
        "# def notes_test_set():\n",
        "#     return get_notes_dataset('test')\n",
        "\n",
        "# def notes_train_set():\n",
        "#     return get_notes_dataset('train')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load notes model and dataset and preprocess it"
      ],
      "metadata": {
        "id": "srzj_2BeNGOK"
      },
      "id": "srzj_2BeNGOK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "24ad54a3-db97-47e3-8cc7-417d4db2c99b",
      "metadata": {
        "id": "24ad54a3-db97-47e3-8cc7-417d4db2c99b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 205,
          "referenced_widgets": [
            "072e1c44349d437483d40c767aee6e1b",
            "1f02583789f945d1a2f80bc6c2abcd30",
            "1dd5001b0f464e6096ccf78078da4f3d",
            "be9e1d76c7d0430db1cb1a7c5a802794",
            "e5370ca40ccb4b85889aea500442aec7",
            "1c5e97e97e574dd1b061368beae65f6d",
            "759c5a4a42ea4d4e950d1ac56b81a8ae",
            "c3b0575affbc45c6bfd8d4ccc6e2a21d",
            "14e1e5f288ea474b858f7468df984bce",
            "9d6494b07df44e5b8ac3c34885eded0b",
            "37e71a45d1af4639989bb4cf2039c992",
            "5b9189252598429eac2c0499b3930494",
            "90ccc3447c2144c1826291d5f041a8c7",
            "ead39028d87941e38820816a6bf968a3",
            "85958f37cbb24980a4bad5f1cfe4744e",
            "12ecf64ae6864036b6f52580c23596d2",
            "bc7182505e9e4f5e8761c9a803ced605",
            "5673a8b38d7d4b4a91a80b5d0b66213c",
            "7e785b97dbd24951a38ff6d7498f1aef",
            "c0107c8245be463d962c9c76dcde4833",
            "d0e499f19a9c4f218fe410993f151dd1",
            "d8d386340e2848b0bcede4148d338e06"
          ]
        },
        "outputId": "6c1eb3e2-61b6-4df4-acc3-54592aa91f3b"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?ba/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "072e1c44349d437483d40c767aee6e1b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['input_ids', 'attention_mask', 'labels']\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/597M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5b9189252598429eac2c0499b3930494"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['longformer_model.encoder.layer.6.attention.self.key.weight', 'longformer_model.encoder.layer.9.attention.self.value.bias', 'longformer_model.embeddings.position_ids', 'longformer_model.encoder.layer.11.output.LayerNorm.bias', 'longformer_model.encoder.layer.1.output.dense.weight', 'longformer_model.encoder.layer.4.output.LayerNorm.bias', 'longformer_model.encoder.layer.9.output.dense.weight', 'longformer_model.encoder.layer.9.attention.self.key_global.bias', 'longformer_model.encoder.layer.7.attention.self.key_global.bias', 'longformer_model.encoder.layer.8.attention.self.query_global.bias', 'longformer_model.encoder.layer.8.output.dense.bias', 'longformer_model.embeddings.token_type_embeddings.weight', 'longformer_model.encoder.layer.9.attention.self.query.bias', 'longformer_model.encoder.layer.5.attention.output.dense.weight', 'longformer_model.encoder.layer.10.output.LayerNorm.weight', 'longformer_model.encoder.layer.0.attention.self.value.weight', 'longformer_model.encoder.layer.1.attention.self.value_global.bias', 'longformer_model.encoder.layer.5.attention.self.value_global.weight', 'longformer_model.encoder.layer.10.output.dense.weight', 'longformer_model.encoder.layer.6.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.4.attention.self.value_global.bias', 'longformer_model.encoder.layer.3.attention.self.value.weight', 'longformer_model.encoder.layer.7.attention.self.value.bias', 'longformer_model.encoder.layer.7.intermediate.dense.weight', 'longformer_model.encoder.layer.7.output.dense.bias', 'longformer_model.encoder.layer.1.attention.self.key_global.bias', 'longformer_model.encoder.layer.8.attention.output.dense.bias', 'longformer_model.encoder.layer.10.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.self.query.bias', 'longformer_model.encoder.layer.11.intermediate.dense.weight', 'longformer_model.encoder.layer.0.intermediate.dense.weight', 'longformer_model.encoder.layer.3.output.LayerNorm.weight', 'longformer_model.encoder.layer.10.attention.self.value.bias', 'longformer_model.encoder.layer.7.attention.self.query_global.weight', 'longformer_model.encoder.layer.11.attention.self.value_global.bias', 'longformer_model.encoder.layer.4.attention.self.key.weight', 'longformer_model.encoder.layer.1.attention.output.dense.bias', 'longformer_model.encoder.layer.2.intermediate.dense.bias', 'longformer_model.encoder.layer.10.attention.output.dense.weight', 'longformer_model.encoder.layer.8.attention.self.query_global.weight', 'fc.weight', 'longformer_model.encoder.layer.0.attention.self.key.weight', 'longformer_model.encoder.layer.1.attention.self.value.weight', 'longformer_model.encoder.layer.9.intermediate.dense.bias', 'longformer_model.encoder.layer.0.attention.self.value.bias', 'longformer_model.encoder.layer.8.attention.self.query.weight', 'longformer_model.encoder.layer.3.attention.self.query_global.bias', 'longformer_model.encoder.layer.7.attention.output.dense.bias', 'longformer_model.encoder.layer.2.attention.self.key.bias', 'longformer_model.encoder.layer.6.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.10.attention.self.value_global.weight', 'longformer_model.encoder.layer.2.output.LayerNorm.weight', 'longformer_model.encoder.layer.10.attention.self.value_global.bias', 'longformer_model.encoder.layer.3.attention.self.query.bias', 'longformer_model.encoder.layer.2.intermediate.dense.weight', 'longformer_model.encoder.layer.9.attention.self.value_global.bias', 'longformer_model.encoder.layer.11.attention.self.query_global.weight', 'longformer_model.encoder.layer.8.output.dense.weight', 'longformer_model.encoder.layer.9.attention.self.key.weight', 'longformer_model.encoder.layer.5.attention.self.query_global.bias', 'longformer_model.encoder.layer.9.attention.self.query_global.bias', 'longformer_model.encoder.layer.1.attention.self.query_global.weight', 'longformer_model.encoder.layer.3.output.dense.weight', 'longformer_model.encoder.layer.7.attention.self.query.weight', 'longformer_model.encoder.layer.3.output.dense.bias', 'longformer_model.encoder.layer.8.attention.output.dense.weight', 'longformer_model.encoder.layer.9.output.LayerNorm.bias', 'longformer_model.encoder.layer.7.attention.self.key_global.weight', 'longformer_model.encoder.layer.11.attention.output.dense.weight', 'longformer_model.encoder.layer.9.attention.self.query.weight', 'longformer_model.encoder.layer.4.attention.output.dense.weight', 'longformer_model.encoder.layer.0.attention.self.query_global.bias', 'longformer_model.encoder.layer.6.attention.self.query.bias', 'longformer_model.encoder.layer.10.attention.self.key.bias', 'longformer_model.encoder.layer.11.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.output.LayerNorm.bias', 'longformer_model.encoder.layer.9.attention.self.value.weight', 'longformer_model.encoder.layer.2.attention.self.key.weight', 'longformer_model.encoder.layer.3.intermediate.dense.bias', 'longformer_model.encoder.layer.5.output.dense.bias', 'longformer_model.encoder.layer.0.output.dense.weight', 'longformer_model.encoder.layer.0.attention.self.key.bias', 'longformer_model.encoder.layer.7.attention.self.value_global.bias', 'longformer_model.encoder.layer.5.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.6.output.dense.bias', 'longformer_model.encoder.layer.5.attention.self.value_global.bias', 'longformer_model.encoder.layer.6.attention.self.value_global.weight', 'longformer_model.encoder.layer.3.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.8.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.8.output.LayerNorm.bias', 'longformer_model.encoder.layer.6.intermediate.dense.weight', 'longformer_model.encoder.layer.10.attention.self.key_global.bias', 'longformer_model.encoder.layer.5.attention.self.query_global.weight', 'longformer_model.encoder.layer.3.attention.self.value_global.bias', 'longformer_model.encoder.layer.2.attention.self.value_global.weight', 'longformer_model.encoder.layer.0.attention.self.query_global.weight', 'longformer_model.encoder.layer.0.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.self.key_global.bias', 'longformer_model.encoder.layer.8.attention.self.value_global.weight', 'longformer_model.encoder.layer.0.attention.self.key_global.bias', 'longformer_model.encoder.layer.0.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.0.attention.self.key_global.weight', 'longformer_model.encoder.layer.2.attention.self.key_global.bias', 'longformer_model.encoder.layer.1.output.LayerNorm.bias', 'longformer_model.encoder.layer.3.attention.self.query_global.weight', 'longformer_model.encoder.layer.4.attention.output.dense.bias', 'longformer_model.encoder.layer.7.output.dense.weight', 'longformer_model.encoder.layer.6.attention.self.value.weight', 'longformer_model.encoder.layer.5.output.dense.weight', 'longformer_model.encoder.layer.2.attention.self.query.weight', 'longformer_model.encoder.layer.10.attention.self.query_global.bias', 'longformer_model.encoder.layer.11.attention.self.query.weight', 'longformer_model.encoder.layer.4.output.dense.bias', 'longformer_model.encoder.layer.1.attention.self.query.weight', 'longformer_model.encoder.layer.2.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.self.query.weight', 'longformer_model.encoder.layer.1.attention.output.dense.weight', 'longformer_model.encoder.layer.0.attention.self.query.weight', 'longformer_model.encoder.layer.0.output.LayerNorm.weight', 'longformer_model.encoder.layer.6.attention.self.key_global.weight', 'longformer_model.encoder.layer.2.attention.self.query.bias', 'longformer_model.encoder.layer.2.output.dense.bias', 'longformer_model.encoder.layer.0.attention.self.value_global.weight', 'longformer_model.encoder.layer.8.output.LayerNorm.weight', 'longformer_model.encoder.layer.4.intermediate.dense.bias', 'longformer_model.encoder.layer.7.output.LayerNorm.bias', 'longformer_model.encoder.layer.9.output.dense.bias', 'longformer_model.encoder.layer.4.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.7.attention.self.value.weight', 'longformer_model.encoder.layer.5.attention.self.value.weight', 'longformer_model.encoder.layer.0.output.dense.bias', 'longformer_model.encoder.layer.11.output.dense.bias', 'longformer_model.encoder.layer.5.output.LayerNorm.weight', 'longformer_model.encoder.layer.0.attention.output.dense.bias', 'longformer_model.encoder.layer.9.attention.self.key_global.weight', 'longformer_model.encoder.layer.8.intermediate.dense.weight', 'longformer_model.encoder.layer.10.attention.self.key.weight', 'longformer_model.encoder.layer.11.attention.output.LayerNorm.weight', 'fc.bias', 'longformer_model.encoder.layer.9.attention.output.dense.bias', 'longformer_model.encoder.layer.10.attention.output.dense.bias', 'longformer_model.encoder.layer.7.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.8.attention.self.key.bias', 'longformer_model.encoder.layer.2.output.dense.weight', 'longformer_model.encoder.layer.8.intermediate.dense.bias', 'longformer_model.encoder.layer.2.attention.self.query_global.bias', 'longformer_model.encoder.layer.4.attention.self.value.weight', 'longformer_model.encoder.layer.1.attention.self.key.weight', 'longformer_model.encoder.layer.10.attention.self.query_global.weight', 'longformer_model.encoder.layer.11.intermediate.dense.bias', 'longformer_model.encoder.layer.4.attention.self.query.weight', 'longformer_model.encoder.layer.10.attention.self.value.weight', 'longformer_model.encoder.layer.2.attention.self.value.bias', 'longformer_model.encoder.layer.7.attention.self.key.bias', 'longformer_model.encoder.layer.10.output.LayerNorm.bias', 'longformer_model.encoder.layer.4.attention.self.key_global.weight', 'longformer_model.encoder.layer.8.attention.self.value.bias', 'longformer_model.encoder.layer.10.attention.self.query.weight', 'longformer_model.embeddings.LayerNorm.bias', 'longformer_model.encoder.layer.0.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.7.attention.self.value_global.weight', 'longformer_model.encoder.layer.9.attention.self.key.bias', 'longformer_model.encoder.layer.11.attention.self.query.bias', 'longformer_model.encoder.layer.10.intermediate.dense.bias', 'longformer_model.encoder.layer.11.attention.output.dense.bias', 'longformer_model.encoder.layer.3.attention.self.value.bias', 'longformer_model.encoder.layer.11.attention.self.key_global.bias', 'longformer_model.encoder.layer.2.attention.self.value_global.bias', 'longformer_model.encoder.layer.0.intermediate.dense.bias', 'longformer_model.encoder.layer.2.attention.output.dense.bias', 'longformer_model.encoder.layer.0.attention.output.dense.weight', 'longformer_model.encoder.layer.3.attention.self.key.weight', 'longformer_model.encoder.layer.1.attention.self.query_global.bias', 'longformer_model.encoder.layer.6.output.dense.weight', 'longformer_model.encoder.layer.1.attention.self.query.bias', 'longformer_model.encoder.layer.2.attention.output.dense.weight', 'longformer_model.encoder.layer.3.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.4.intermediate.dense.weight', 'longformer_model.encoder.layer.8.attention.self.key_global.bias', 'longformer_model.encoder.layer.5.attention.self.key_global.weight', 'longformer_model.encoder.layer.11.attention.self.query_global.bias', 'longformer_model.encoder.layer.4.attention.self.value.bias', 'longformer_model.encoder.layer.9.attention.self.query_global.weight', 'longformer_model.encoder.layer.9.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.2.attention.self.value.weight', 'longformer_model.encoder.layer.6.intermediate.dense.bias', 'longformer_model.encoder.layer.9.attention.output.dense.weight', 'longformer_model.encoder.layer.7.attention.self.query.bias', 'longformer_model.encoder.layer.3.output.LayerNorm.bias', 'longformer_model.encoder.layer.3.attention.self.key.bias', 'longformer_model.encoder.layer.4.attention.self.key_global.bias', 'longformer_model.encoder.layer.9.intermediate.dense.weight', 'longformer_model.encoder.layer.8.attention.self.key.weight', 'longformer_model.encoder.layer.11.attention.self.key.weight', 'longformer_model.encoder.layer.11.attention.self.value.bias', 'dense.bias', 'longformer_model.encoder.layer.11.attention.self.value.weight', 'longformer_model.encoder.layer.6.attention.self.query.weight', 'longformer_model.encoder.layer.4.attention.self.query_global.weight', 'longformer_model.encoder.layer.7.attention.self.query_global.bias', 'longformer_model.encoder.layer.11.attention.self.key.bias', 'longformer_model.embeddings.position_embeddings.weight', 'longformer_model.encoder.layer.1.intermediate.dense.bias', 'longformer_model.encoder.layer.1.output.LayerNorm.weight', 'longformer_model.encoder.layer.8.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.5.attention.self.key.weight', 'longformer_model.encoder.layer.8.attention.self.value.weight', 'longformer_model.encoder.layer.4.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.7.attention.self.key.weight', 'longformer_model.encoder.layer.7.attention.output.dense.weight', 'longformer_model.encoder.layer.4.attention.self.key.bias', 'longformer_model.encoder.layer.6.attention.output.dense.bias', 'longformer_model.encoder.layer.7.output.LayerNorm.weight', 'longformer_model.encoder.layer.2.attention.self.query_global.weight', 'longformer_model.encoder.layer.3.attention.self.key_global.weight', 'longformer_model.encoder.layer.1.attention.self.key.bias', 'longformer_model.encoder.layer.5.intermediate.dense.bias', 'longformer_model.encoder.layer.1.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.output.dense.bias', 'longformer_model.encoder.layer.2.attention.self.key_global.weight', 'longformer_model.encoder.layer.6.attention.self.value.bias', 'longformer_model.encoder.layer.4.attention.self.query_global.bias', 'longformer_model.encoder.layer.11.output.dense.weight', 'longformer_model.encoder.layer.6.attention.self.query_global.weight', 'longformer_model.encoder.layer.4.output.dense.weight', 'longformer_model.encoder.layer.6.output.LayerNorm.weight', 'longformer_model.encoder.layer.11.attention.self.key_global.weight', 'longformer_model.encoder.layer.3.attention.output.dense.weight', 'longformer_model.encoder.layer.3.attention.self.value_global.weight', 'longformer_model.encoder.layer.7.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.4.output.LayerNorm.weight', 'longformer_model.encoder.layer.2.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.1.attention.self.key_global.weight', 'longformer_model.encoder.layer.1.output.dense.bias', 'longformer_model.encoder.layer.1.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.1.attention.self.value_global.weight', 'longformer_model.encoder.layer.5.attention.self.key.bias', 'longformer_model.encoder.layer.7.intermediate.dense.bias', 'longformer_model.encoder.layer.9.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.1.attention.self.value.bias', 'longformer_model.encoder.layer.6.attention.self.key_global.bias', 'longformer_model.encoder.layer.8.attention.self.value_global.bias', 'longformer_model.encoder.layer.3.attention.self.query.weight', 'longformer_model.encoder.layer.5.attention.self.value.bias', 'longformer_model.embeddings.LayerNorm.weight', 'longformer_model.encoder.layer.6.attention.output.dense.weight', 'longformer_model.encoder.layer.5.intermediate.dense.weight', 'longformer_model.encoder.layer.2.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.9.output.LayerNorm.weight', 'longformer_model.encoder.layer.10.attention.self.key_global.weight', 'longformer_model.encoder.layer.9.attention.self.value_global.weight', 'longformer_model.encoder.layer.1.intermediate.dense.weight', 'longformer_model.encoder.layer.10.attention.self.query.bias', 'longformer_model.encoder.layer.3.attention.self.key_global.bias', 'longformer_model.encoder.layer.6.attention.self.key.bias', 'longformer_model.encoder.layer.0.attention.self.value_global.bias', 'longformer_model.encoder.layer.10.intermediate.dense.weight', 'longformer_model.encoder.layer.4.attention.self.value_global.weight', 'longformer_model.encoder.layer.10.output.dense.bias', 'longformer_model.encoder.layer.11.attention.self.value_global.weight', 'longformer_model.encoder.layer.11.output.LayerNorm.weight', 'dense.weight', 'longformer_model.encoder.layer.3.intermediate.dense.weight', 'longformer_model.encoder.layer.8.attention.self.key_global.weight', 'longformer_model.embeddings.word_embeddings.weight', 'longformer_model.encoder.layer.3.attention.output.dense.bias', 'longformer_model.encoder.layer.4.attention.self.query.bias', 'longformer_model.encoder.layer.0.attention.self.query.bias', 'longformer_model.encoder.layer.6.output.LayerNorm.bias', 'longformer_model.encoder.layer.6.attention.self.query_global.bias', 'longformer_model.encoder.layer.10.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.6.attention.self.value_global.bias', 'longformer_model.encoder.layer.8.attention.self.query.bias']\n",
            "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['longformer.encoder.layer.9.attention.output.LayerNorm.weight', 'longformer.encoder.layer.8.attention.self.key.weight', 'longformer.encoder.layer.11.intermediate.dense.bias', 'longformer.encoder.layer.7.attention.self.query.bias', 'longformer.encoder.layer.10.attention.output.LayerNorm.weight', 'longformer.encoder.layer.10.attention.output.dense.weight', 'longformer.encoder.layer.0.attention.self.value_global.weight', 'longformer.encoder.layer.4.attention.self.key_global.weight', 'longformer.encoder.layer.11.output.dense.weight', 'longformer.encoder.layer.0.output.dense.bias', 'longformer.encoder.layer.4.attention.output.LayerNorm.weight', 'longformer.encoder.layer.2.attention.self.value.weight', 'longformer.encoder.layer.1.intermediate.dense.bias', 'longformer.encoder.layer.7.attention.self.key_global.bias', 'longformer.encoder.layer.10.attention.self.key_global.bias', 'longformer.encoder.layer.5.attention.self.key.bias', 'longformer.encoder.layer.1.attention.self.query_global.weight', 'longformer.encoder.layer.1.attention.self.query.bias', 'longformer.encoder.layer.9.attention.self.value.weight', 'longformer.encoder.layer.5.output.dense.bias', 'longformer.encoder.layer.6.attention.self.key.weight', 'longformer.encoder.layer.7.output.dense.bias', 'longformer.encoder.layer.1.attention.self.query_global.bias', 'longformer.encoder.layer.10.attention.self.value_global.weight', 'longformer.encoder.layer.11.attention.self.key_global.weight', 'longformer.encoder.layer.3.output.dense.weight', 'longformer.encoder.layer.2.attention.self.query_global.weight', 'longformer.encoder.layer.0.attention.self.key_global.weight', 'longformer.encoder.layer.0.attention.self.value_global.bias', 'longformer.encoder.layer.6.intermediate.dense.bias', 'longformer.encoder.layer.1.attention.self.key.bias', 'longformer.embeddings.LayerNorm.weight', 'longformer.encoder.layer.8.attention.output.LayerNorm.weight', 'longformer.encoder.layer.10.attention.self.key.weight', 'longformer.encoder.layer.11.intermediate.dense.weight', 'longformer.encoder.layer.1.attention.self.key_global.bias', 'longformer.encoder.layer.1.output.LayerNorm.weight', 'longformer.encoder.layer.8.attention.output.dense.weight', 'longformer.encoder.layer.1.output.dense.bias', 'longformer.encoder.layer.4.attention.self.value.bias', 'longformer.encoder.layer.0.attention.self.key_global.bias', 'longformer.encoder.layer.6.attention.self.key_global.bias', 'longformer.encoder.layer.11.attention.self.query.bias', 'longformer.encoder.layer.1.attention.self.value_global.weight', 'longformer.encoder.layer.2.output.LayerNorm.bias', 'longformer.encoder.layer.7.attention.self.query_global.weight', 'longformer.encoder.layer.7.intermediate.dense.bias', 'longformer.encoder.layer.1.attention.self.key.weight', 'longformer.encoder.layer.0.attention.self.query_global.weight', 'longformer.encoder.layer.7.attention.self.value.weight', 'longformer.encoder.layer.1.attention.self.key_global.weight', 'longformer.encoder.layer.6.output.LayerNorm.bias', 'longformer.encoder.layer.5.attention.self.query_global.bias', 'longformer.encoder.layer.4.attention.self.query_global.weight', 'longformer.encoder.layer.9.attention.self.query_global.weight', 'longformer.encoder.layer.3.attention.self.value_global.bias', 'longformer.encoder.layer.11.attention.output.LayerNorm.bias', 'longformer.encoder.layer.6.output.dense.weight', 'longformer.encoder.layer.8.output.dense.bias', 'longformer.encoder.layer.5.intermediate.dense.weight', 'longformer.encoder.layer.10.attention.self.value.weight', 'longformer.encoder.layer.4.output.dense.bias', 'longformer.encoder.layer.9.attention.self.key.bias', 'longformer.encoder.layer.11.attention.self.query_global.weight', 'longformer.encoder.layer.4.attention.output.dense.bias', 'longformer.encoder.layer.6.attention.self.key_global.weight', 'longformer.encoder.layer.7.output.dense.weight', 'longformer.encoder.layer.0.intermediate.dense.bias', 'longformer.encoder.layer.8.attention.self.query.bias', 'longformer.encoder.layer.11.output.dense.bias', 'longformer.encoder.layer.2.attention.self.key_global.weight', 'longformer.encoder.layer.5.attention.self.query_global.weight', 'longformer.encoder.layer.9.attention.self.key_global.bias', 'longformer.encoder.layer.10.attention.self.value.bias', 'longformer.encoder.layer.0.output.LayerNorm.bias', 'longformer.encoder.layer.3.attention.self.key.bias', 'longformer.encoder.layer.4.attention.self.query_global.bias', 'longformer.encoder.layer.8.attention.self.value.weight', 'classifier.dense.bias', 'longformer.encoder.layer.9.intermediate.dense.bias', 'longformer.encoder.layer.2.attention.self.query_global.bias', 'longformer.encoder.layer.5.attention.self.query.bias', 'longformer.encoder.layer.7.attention.self.key.bias', 'longformer.encoder.layer.6.attention.self.value_global.weight', 'longformer.encoder.layer.7.output.LayerNorm.weight', 'longformer.encoder.layer.1.output.LayerNorm.bias', 'longformer.encoder.layer.2.attention.self.query.weight', 'longformer.encoder.layer.5.attention.self.value_global.weight', 'longformer.encoder.layer.2.attention.self.value.bias', 'longformer.encoder.layer.4.attention.self.query.bias', 'longformer.encoder.layer.2.attention.self.key.bias', 'longformer.encoder.layer.5.output.dense.weight', 'longformer.encoder.layer.9.intermediate.dense.weight', 'longformer.encoder.layer.7.attention.output.dense.bias', 'longformer.encoder.layer.0.attention.self.query.bias', 'longformer.encoder.layer.4.attention.output.LayerNorm.bias', 'longformer.encoder.layer.7.attention.self.key.weight', 'longformer.encoder.layer.3.attention.self.value_global.weight', 'longformer.encoder.layer.6.attention.self.query_global.weight', 'longformer.encoder.layer.10.attention.output.dense.bias', 'longformer.encoder.layer.1.attention.output.LayerNorm.bias', 'longformer.encoder.layer.3.output.LayerNorm.bias', 'longformer.encoder.layer.1.attention.output.dense.weight', 'longformer.encoder.layer.5.attention.self.value_global.bias', 'longformer.encoder.layer.11.attention.self.key.bias', 'longformer.encoder.layer.9.attention.self.query.bias', 'longformer.encoder.layer.10.intermediate.dense.weight', 'longformer.encoder.layer.1.intermediate.dense.weight', 'longformer.encoder.layer.11.attention.output.LayerNorm.weight', 'longformer.encoder.layer.11.attention.self.value_global.bias', 'longformer.encoder.layer.4.attention.output.dense.weight', 'longformer.encoder.layer.7.attention.output.LayerNorm.weight', 'longformer.encoder.layer.9.attention.self.value.bias', 'longformer.encoder.layer.7.attention.self.query_global.bias', 'longformer.encoder.layer.5.attention.self.key_global.bias', 'longformer.encoder.layer.5.attention.self.key.weight', 'longformer.encoder.layer.4.attention.self.key.weight', 'longformer.encoder.layer.3.attention.self.key.weight', 'longformer.encoder.layer.2.attention.self.value_global.weight', 'longformer.encoder.layer.8.output.dense.weight', 'longformer.encoder.layer.10.output.dense.weight', 'longformer.encoder.layer.3.attention.self.query.bias', 'longformer.encoder.layer.4.attention.self.key_global.bias', 'longformer.encoder.layer.4.attention.self.query.weight', 'longformer.encoder.layer.8.attention.self.key_global.bias', 'longformer.encoder.layer.6.attention.output.dense.bias', 'longformer.encoder.layer.8.intermediate.dense.bias', 'longformer.encoder.layer.8.attention.self.query.weight', 'longformer.encoder.layer.9.output.LayerNorm.weight', 'longformer.encoder.layer.7.attention.output.LayerNorm.bias', 'longformer.encoder.layer.9.attention.self.query.weight', 'longformer.encoder.layer.7.attention.self.value.bias', 'longformer.encoder.layer.7.attention.output.dense.weight', 'longformer.embeddings.LayerNorm.bias', 'longformer.encoder.layer.1.attention.output.dense.bias', 'longformer.encoder.layer.2.intermediate.dense.weight', 'longformer.encoder.layer.3.attention.self.query_global.weight', 'longformer.encoder.layer.3.output.dense.bias', 'longformer.encoder.layer.10.attention.self.value_global.bias', 'longformer.encoder.layer.5.attention.self.key_global.weight', 'longformer.encoder.layer.0.attention.output.dense.bias', 'longformer.encoder.layer.3.output.LayerNorm.weight', 'longformer.encoder.layer.5.attention.self.value.weight', 'longformer.encoder.layer.5.attention.output.dense.bias', 'longformer.encoder.layer.7.attention.self.value_global.bias', 'longformer.encoder.layer.6.intermediate.dense.weight', 'longformer.encoder.layer.4.attention.self.value_global.bias', 'longformer.encoder.layer.2.attention.output.dense.bias', 'longformer.encoder.layer.2.attention.self.query.bias', 'longformer.encoder.layer.8.attention.self.value.bias', 'longformer.encoder.layer.0.attention.output.LayerNorm.weight', 'longformer.encoder.layer.10.output.LayerNorm.bias', 'longformer.encoder.layer.0.intermediate.dense.weight', 'longformer.encoder.layer.0.attention.self.key.bias', 'longformer.encoder.layer.1.attention.output.LayerNorm.weight', 'longformer.encoder.layer.3.attention.self.key_global.weight', 'longformer.encoder.layer.5.output.LayerNorm.weight', 'longformer.encoder.layer.0.attention.self.key.weight', 'longformer.encoder.layer.6.attention.output.dense.weight', 'longformer.encoder.layer.2.output.dense.weight', 'longformer.encoder.layer.9.attention.self.value_global.bias', 'longformer.encoder.layer.9.output.dense.bias', 'longformer.encoder.layer.6.attention.self.value.weight', 'longformer.encoder.layer.2.output.LayerNorm.weight', 'longformer.encoder.layer.10.attention.self.query_global.bias', 'longformer.encoder.layer.10.output.LayerNorm.weight', 'longformer.encoder.layer.9.attention.output.LayerNorm.bias', 'longformer.encoder.layer.3.attention.self.key_global.bias', 'longformer.encoder.layer.8.output.LayerNorm.bias', 'longformer.encoder.layer.2.output.dense.bias', 'longformer.encoder.layer.10.attention.self.query_global.weight', 'longformer.encoder.layer.1.output.dense.weight', 'longformer.encoder.layer.5.attention.self.value.bias', 'classifier.out_proj.weight', 'longformer.encoder.layer.5.intermediate.dense.bias', 'longformer.encoder.layer.5.attention.output.dense.weight', 'longformer.encoder.layer.0.attention.self.query.weight', 'longformer.encoder.layer.1.attention.self.value_global.bias', 'longformer.encoder.layer.4.attention.self.value_global.weight', 'longformer.encoder.layer.4.intermediate.dense.bias', 'longformer.embeddings.token_type_embeddings.weight', 'longformer.encoder.layer.2.attention.output.dense.weight', 'longformer.encoder.layer.9.output.LayerNorm.bias', 'longformer.encoder.layer.3.attention.output.LayerNorm.weight', 'longformer.encoder.layer.2.attention.self.key_global.bias', 'longformer.encoder.layer.11.attention.output.dense.bias', 'longformer.encoder.layer.1.attention.self.value.bias', 'longformer.encoder.layer.8.attention.output.dense.bias', 'longformer.encoder.layer.10.output.dense.bias', 'longformer.encoder.layer.3.attention.output.LayerNorm.bias', 'longformer.encoder.layer.8.output.LayerNorm.weight', 'longformer.encoder.layer.0.output.dense.weight', 'longformer.embeddings.word_embeddings.weight', 'longformer.encoder.layer.2.attention.output.LayerNorm.weight', 'longformer.encoder.layer.9.attention.output.dense.weight', 'longformer.encoder.layer.10.intermediate.dense.bias', 'longformer.encoder.layer.9.attention.self.key_global.weight', 'longformer.encoder.layer.11.attention.self.value.bias', 'longformer.encoder.layer.8.attention.self.query_global.weight', 'longformer.encoder.layer.10.attention.self.key.bias', 'longformer.encoder.layer.6.attention.self.query.weight', 'longformer.encoder.layer.5.output.LayerNorm.bias', 'longformer.encoder.layer.7.output.LayerNorm.bias', 'longformer.encoder.layer.2.attention.output.LayerNorm.bias', 'longformer.encoder.layer.4.intermediate.dense.weight', 'longformer.encoder.layer.9.attention.self.query_global.bias', 'longformer.encoder.layer.0.attention.output.dense.weight', 'longformer.encoder.layer.3.intermediate.dense.weight', 'longformer.encoder.layer.6.attention.self.key.bias', 'longformer.encoder.layer.8.attention.self.key.bias', 'longformer.encoder.layer.10.attention.self.key_global.weight', 'longformer.encoder.layer.6.attention.output.LayerNorm.weight', 'longformer.encoder.layer.9.attention.self.value_global.weight', 'longformer.encoder.layer.4.output.LayerNorm.bias', 'longformer.encoder.layer.4.output.LayerNorm.weight', 'longformer.encoder.layer.0.attention.self.query_global.bias', 'longformer.encoder.layer.6.output.dense.bias', 'longformer.encoder.layer.11.attention.self.key.weight', 'longformer.encoder.layer.11.attention.self.value_global.weight', 'longformer.encoder.layer.8.attention.self.value_global.weight', 'longformer.encoder.layer.1.attention.self.value.weight', 'longformer.encoder.layer.3.attention.self.value.bias', 'longformer.encoder.layer.9.output.dense.weight', 'longformer.encoder.layer.3.attention.self.value.weight', 'longformer.encoder.layer.4.attention.self.value.weight', 'longformer.encoder.layer.11.attention.output.dense.weight', 'longformer.encoder.layer.7.attention.self.value_global.weight', 'longformer.encoder.layer.11.output.LayerNorm.weight', 'longformer.encoder.layer.0.attention.self.value.bias', 'longformer.encoder.layer.6.attention.self.value.bias', 'longformer.encoder.layer.11.output.LayerNorm.bias', 'longformer.encoder.layer.3.attention.self.query.weight', 'longformer.encoder.layer.11.attention.self.query.weight', 'longformer.encoder.layer.4.attention.self.key.bias', 'longformer.encoder.layer.6.output.LayerNorm.weight', 'longformer.encoder.layer.8.intermediate.dense.weight', 'longformer.encoder.layer.11.attention.self.key_global.bias', 'longformer.encoder.layer.6.attention.output.LayerNorm.bias', 'classifier.out_proj.bias', 'longformer.encoder.layer.2.attention.self.value_global.bias', 'longformer.encoder.layer.2.intermediate.dense.bias', 'longformer.encoder.layer.5.attention.self.query.weight', 'longformer.encoder.layer.3.attention.self.query_global.bias', 'longformer.encoder.layer.7.attention.self.query.weight', 'longformer.encoder.layer.10.attention.self.query.bias', 'longformer.encoder.layer.3.attention.output.dense.bias', 'longformer.encoder.layer.8.attention.self.value_global.bias', 'longformer.encoder.layer.6.attention.self.query.bias', 'classifier.dense.weight', 'longformer.encoder.layer.2.attention.self.key.weight', 'longformer.encoder.layer.3.intermediate.dense.bias', 'longformer.encoder.layer.8.attention.self.query_global.bias', 'longformer.encoder.layer.1.attention.self.query.weight', 'longformer.encoder.layer.11.attention.self.value.weight', 'longformer.encoder.layer.0.output.LayerNorm.weight', 'longformer.encoder.layer.7.intermediate.dense.weight', 'longformer.encoder.layer.4.output.dense.weight', 'longformer.encoder.layer.9.attention.self.key.weight', 'longformer.encoder.layer.10.attention.output.LayerNorm.bias', 'longformer.encoder.layer.6.attention.self.value_global.bias', 'longformer.encoder.layer.10.attention.self.query.weight', 'longformer.encoder.layer.6.attention.self.query_global.bias', 'longformer.encoder.layer.5.attention.output.LayerNorm.weight', 'longformer.encoder.layer.9.attention.output.dense.bias', 'longformer.embeddings.position_embeddings.weight', 'longformer.encoder.layer.7.attention.self.key_global.weight', 'longformer.encoder.layer.3.attention.output.dense.weight', 'longformer.encoder.layer.0.attention.output.LayerNorm.bias', 'longformer.encoder.layer.0.attention.self.value.weight', 'longformer.encoder.layer.5.attention.output.LayerNorm.bias', 'longformer.encoder.layer.11.attention.self.query_global.bias', 'longformer.encoder.layer.8.attention.output.LayerNorm.bias', 'longformer.encoder.layer.8.attention.self.key_global.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "#import dataset/split from local\n",
        "ds = pd.read_csv(\"/content/drive/MyDrive/cogs402longformer/fakeclinicalnotes/data/fake_notes.csv\")\n",
        "dataset = datasets.Dataset.from_pandas(ds)\n",
        "# cogs402_ds = dataset.train_test_split(test_size=0.20)\n",
        "# cogs402_ds['validation'] = cogs402_ds.pop('test')\n",
        "\n",
        "#preprocess and format columns into tensors\n",
        "cogs402_test = get_notes_dataset(dataset)\n",
        "columns = cogs402_test.input_columns + cogs402_test.target_columns\n",
        "print(columns)\n",
        "cogs402_test.set_format(type='torch', columns=columns)\n",
        "cogs402_test=cogs402_test.remove_columns(['text'])\n",
        "\n",
        "#import model\n",
        "model = longformer_finetuned_notes()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get attention output for example"
      ],
      "metadata": {
        "id": "Sxd71xm3nWXj"
      },
      "id": "Sxd71xm3nWXj"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't forget to allow your model to use your GPU for faster performance."
      ],
      "metadata": {
        "id": "QXqrucXRuJ3I"
      },
      "id": "QXqrucXRuJ3I"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "deaabfa2-0855-41fd-870c-7a8b91e32d44",
      "metadata": {
        "id": "deaabfa2-0855-41fd-870c-7a8b91e32d44",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "410d1bf4-6b2a-4614-a733-5bc99cac9d53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ],
      "source": [
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "print(model.device)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can select any example in your dataset, perferably one where you know it is interesting (like a false negative, a false positive or something that you found from a different part of your project)."
      ],
      "metadata": {
        "id": "89mQ9IyUuQkl"
      },
      "id": "89mQ9IyUuQkl"
    },
    {
      "cell_type": "code",
      "source": [
        "test_val = [7]\n",
        "print(test_val)\n",
        "testexam = cogs402_test[test_val]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "XnXxTGhFqYG5",
        "outputId": "0008ed7c-c6f5-474d-b389-e3c4a5b180a3"
      },
      "id": "XnXxTGhFqYG5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[7]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-77788c68d569>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtestexam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcogs402_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_val\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2164\u001b[0m         \u001b[0;34m\"\"\"Can be used to index columns (by string names) or rows (by integer index or iterable of indices or bools).\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2165\u001b[0m         return self._getitem(\n\u001b[0;32m-> 2166\u001b[0;31m             \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2167\u001b[0m         )\n\u001b[1;32m   2168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/arrow_dataset.py\u001b[0m in \u001b[0;36m_getitem\u001b[0;34m(self, key, decoded, **kwargs)\u001b[0m\n\u001b[1;32m   2147\u001b[0m         \u001b[0mformat_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mformat_kwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m         \u001b[0mformatter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_formatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mformat_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecoded\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdecoded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mformat_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2149\u001b[0;31m         \u001b[0mpa_subtable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_indices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2150\u001b[0m         formatted_output = format_table(\n\u001b[1;32m   2151\u001b[0m             \u001b[0mpa_subtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformatter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformatter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mformat_columns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_columns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_all_columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36mquery_table\u001b[0;34m(table, key, indices)\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mtable\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_rows\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         \u001b[0m_check_valid_index_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m     \u001b[0;31m# Query the main table\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mindices\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m_check_valid_index_key\u001b[0;34m(key, size)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m             \u001b[0m_check_valid_index_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m             \u001b[0m_check_valid_index_key\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/datasets/formatting/formatting.py\u001b[0m in \u001b[0;36m_check_valid_index_key\u001b[0;34m(key, size)\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIndexError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Invalid key: {key} is out of bounds for size {size}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: Invalid key: 7 is out of bounds for size 3"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(testexam[\"input_ids\"])"
      ],
      "metadata": {
        "id": "jT4sQyAegW8u"
      },
      "id": "jT4sQyAegW8u",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We stack the attentions to get an output attention tensor of shape: (layer, batch, head, seq_len, x + attention_window + 1) and a global attention tensor of shape (layer, batch, head, seq_len, x) where x is the number of global attention tokens."
      ],
      "metadata": {
        "id": "V2qcIQ6vud3Q"
      },
      "id": "V2qcIQ6vud3Q"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04c5bcaf-5fe3-4813-8444-5cfb2c63a92b",
      "metadata": {
        "id": "04c5bcaf-5fe3-4813-8444-5cfb2c63a92b"
      },
      "outputs": [],
      "source": [
        "output = model(testexam[\"input_ids\"].cuda(), attention_mask=testexam['attention_mask'].cuda(), labels=testexam['labels'].cuda(), output_attentions = True)\n",
        "batch_attn = output[-2]\n",
        "output_attentions = torch.stack(batch_attn).cpu()\n",
        "global_attention = output[-1]\n",
        "output_global_attentions = torch.stack(global_attention).cpu()\n",
        "print(\"output_attention.shape\", output_attentions.shape)\n",
        "print(\"gl_output_attention.shape\", output_global_attentions.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(testexam['labels'][0])\n",
        "print(output[1].argmax())"
      ],
      "metadata": {
        "id": "SO4yNy98t_UP"
      },
      "id": "SO4yNy98t_UP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2467c3d1-fe58-4e6d-aa98-534a6df72fcc",
      "metadata": {
        "id": "2467c3d1-fe58-4e6d-aa98-534a6df72fcc"
      },
      "outputs": [],
      "source": [
        "# print(os.getcwd())\n",
        "# yes = torch.load(\"resources/longformer_test2/epoch_3/aggregate_attn.pt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert sliding window attention to traditional format"
      ],
      "metadata": {
        "id": "soutFra9pAsZ"
      },
      "id": "soutFra9pAsZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "A unique property of the longformer model is that the matrix output for the attention is not a seq_len x seq_len output. Each token can only attend to the preceeding w/2 tokens and the succeeding w/2 tokens, dictated by whatever you choose the model's attention window w to be. Another name for this is called the sliding window attention. Therefore, we need to convert sliding attention matrix to correct seq_len x seq_len matrix to remain consistent with other types of Transformer Neural Networks.\n",
        "\n",
        "To do so, we run the following 4 functions. Our attentions will change from an output attention tensor of shape (layer, batch, head, seq_len, x + attention_window + 1) and a global attention tensor of shape (layer, batch, head, seq_len, x) to a single tensor of shape (layer, batch, head, seq_len, seq_len). More information about the functions can be found here. More information about the functions can be found [here](https://colab.research.google.com/drive/1Kxx26NtIlUzioRCHpsR8IbSz_DpRFxEZ#scrollTo=liVhkxiH9Le0)."
      ],
      "metadata": {
        "id": "OevnNprR67LK"
      },
      "id": "OevnNprR67LK"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0882a3e5-1b0c-4319-92a3-92f5b516d854",
      "metadata": {
        "id": "0882a3e5-1b0c-4319-92a3-92f5b516d854"
      },
      "outputs": [],
      "source": [
        "def create_head_matrix(output_attentions, global_attentions):\n",
        "    new_attention_matrix = torch.zeros((output_attentions.shape[0], \n",
        "                                      output_attentions.shape[0]))\n",
        "    for i in range(output_attentions.shape[0]):\n",
        "        test_non_zeroes = torch.nonzero(output_attentions[i]).squeeze()\n",
        "        test2 = output_attentions[i][test_non_zeroes[1:]]\n",
        "        new_attention_matrix_indices = test_non_zeroes[1:]-257 + i\n",
        "        new_attention_matrix[i][new_attention_matrix_indices] = test2\n",
        "        new_attention_matrix[i][0] = output_attentions[i][0]\n",
        "        new_attention_matrix[0] = global_attentions.squeeze()[:output_attentions.shape[0]]\n",
        "    return new_attention_matrix.detach().cpu().numpy()\n",
        "\n",
        "\n",
        "def attentions_all_heads(output_attentions, global_attentions):\n",
        "    new_matrix = []\n",
        "    for i in range(output_attentions.shape[0]):\n",
        "        matrix = create_head_matrix(output_attentions[i], global_attentions[i])\n",
        "        new_matrix.append(matrix)\n",
        "    return np.stack(new_matrix)\n",
        "\n",
        "\n",
        "def all_batches(output_attentions, global_attentions):\n",
        "    new_matrix = []\n",
        "    for i in range(output_attentions.shape[0]):\n",
        "        matrix = attentions_all_heads(output_attentions[i], global_attentions[i])\n",
        "        new_matrix.append(matrix)\n",
        "    return np.stack(new_matrix)\n",
        "\n",
        "def all_layers(output_attentions, global_attentions):\n",
        "    new_matrix = []\n",
        "    for i in range(output_attentions.shape[0]):\n",
        "        matrix = all_batches(output_attentions[i], global_attentions[i])\n",
        "        new_matrix.append(matrix)\n",
        "    return np.stack(new_matrix)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "converted_mat_importance = all_layers(output_attentions, output_global_attentions)"
      ],
      "metadata": {
        "id": "IpdfMEMAuvyR"
      },
      "id": "IpdfMEMAuvyR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling by Head Importance"
      ],
      "metadata": {
        "id": "iaKTv8adcAb6"
      },
      "id": "iaKTv8adcAb6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Not all heads have the same impact on the final output. Some heads may be more important than others so we scale each attention matrix by their respective head and layer. The notebook used to get head importance is [here](https://colab.research.google.com/drive/1sIEvUvCofF0puv0mRZUio3JEF0y1Ce3g?usp=sharing)."
      ],
      "metadata": {
        "id": "P-nS_AHa7Hv6"
      },
      "id": "P-nS_AHa7Hv6"
    },
    {
      "cell_type": "code",
      "source": [
        "head_importance = torch.load(\"/content/drive/MyDrive/cogs402longformer/fakeclinicalnotes/t3-visapplication/notes/head_importance.pt\")"
      ],
      "metadata": {
        "id": "UsAznmcDyBgR"
      },
      "id": "UsAznmcDyBgR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def scale_by_importance(attention_matrix, head_importance):\n",
        "  new_matrix = np.zeros_like(attention_matrix)\n",
        "  for i in range(attention_matrix.shape[0]):\n",
        "    head_importance_layer = head_importance[i]\n",
        "    for j in range(attention_matrix.shape[1]):\n",
        "      new_matrix[i,j] = attention_matrix[i,j] * np.expand_dims(head_importance_layer, axis=(1,2))\n",
        "  return new_matrix"
      ],
      "metadata": {
        "id": "_HFs_vLw0230"
      },
      "id": "_HFs_vLw0230",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "converted_mat_importance = scale_by_importance(converted_mat_importance, head_importance)"
      ],
      "metadata": {
        "id": "XUcjPfxeHbqT"
      },
      "id": "XUcjPfxeHbqT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assess Attention Matrix"
      ],
      "metadata": {
        "id": "hYYuWzo2pEdl"
      },
      "id": "hYYuWzo2pEdl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Every token's top attended\n",
        "\n",
        "Lets suppose we want the topk attended tokens for each token in each head, batch and layer. In other words, we want to know which tokens each token attends TO the most. We first need to grab the list of all tokens in our example from our input_ids as we need to display what the actual tokens are."
      ],
      "metadata": {
        "id": "drU1Kb4UpPgV"
      },
      "id": "drU1Kb4UpPgV"
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "tokenizer2 = AutoTokenizer.from_pretrained('allenai/longformer-base-4096', add_prefix_space=True)"
      ],
      "metadata": {
        "id": "-LB4-lD6MURA"
      },
      "id": "-LB4-lD6MURA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "all_stopwords = stopwords.words('english')\n",
        "all_stopwords.append(\" \")\n",
        "stopwords = set(tokenizer2.tokenize(all_stopwords, is_split_into_words =True))\n",
        "stopwords.update(all_stopwords)\n",
        "print(stopwords)"
      ],
      "metadata": {
        "id": "CxOWzlq2MWu3"
      },
      "id": "CxOWzlq2MWu3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following three functions serve to get us our top k attented tokens for each token in our attention matrix. \n",
        "\n",
        "`find_top_attention_unsummed` takes the full attention matrix (layer, batch, head, seq_len, seq_len) obtains the top k values for each token for every head,batch, and layer. Along with the values, it also returns the positions of each of the values so we know what token each value returned corresponds to. In other words, for every layer, for every item in the batch, for every head, it tells us for each token, what the top k tokens it attends to are. The output of the function are two matrices of shape (layer, batch, head, seq_len, k).\n",
        "\n",
        "`get_tokens` takes in three inputs: a matrix of indices with shape (layer, batch, head, seq_len, k), the tensor of input_ids used for prediction, and an example number, and finds the associated token at each index stored in the input matrix. The example number serves only to select an example from the batch. If we do not pass in an example as an input, we will iterate over all of the batches. The output of the function is an array of shape (layer, batch, head, seq_len, k), where each item is a token from their respective example.\n",
        "\n",
        "`highest_attended_tokens` takes in 5 inputs. It takes in a matrix of indicies, matrix of values, matrix of tokens, all with shape (layer, batch, head, seq_len, k), the tensor of input_ids used for prediction, and an example number. The example number again serves only to pick an example from the batch and if not used, will iterate over all batches. We use these matrices to create a Pandas Dataframe, where each row of the dataframe contains the current token, the position of the current token, the attended token, the position of the attended token, the attended value, the rank of the attended token w.r.t the current token, the layer, the head and the batch. "
      ],
      "metadata": {
        "id": "BX9jj2qIv8vK"
      },
      "id": "BX9jj2qIv8vK"
    },
    {
      "cell_type": "code",
      "source": [
        "# get the top k and indexes and values for each row\n",
        "def find_top_attention_unsummed(scores_mat, k):\n",
        "  indices = scores_mat.argsort(axis=4)[:, :, :, :, :-(k+1):-1]\n",
        "  vals = np.take_along_axis(scores_mat, indices, axis=4)  \n",
        "  return indices, vals\n",
        "\n",
        "#find the tokens using the index matrix and the all_tokens list to create a \n",
        "#matrix of tokens \n",
        "def get_tokens(index_matrix, example_ids, example=None):\n",
        "  \n",
        "  # Make sure our example is not out of range.\n",
        "  assert example < index_matrix.shape[1]\n",
        "\n",
        "  highest_tokens = []\n",
        "  #layer\n",
        "  for i in range(index_matrix.shape[0]):\n",
        "    row_tokens = []\n",
        "    #batch\n",
        "    for j in range(index_matrix.shape[1]):\n",
        "      batch_tokens = []\n",
        "\n",
        "      if (example is not None) and (j != example):\n",
        "        continue\n",
        "\n",
        "      all_tokens = tokenizer.convert_ids_to_tokens(example_ids[j])\n",
        "\n",
        "      #head\n",
        "      for k in range(index_matrix.shape[2]):\n",
        "        head_tokens = []\n",
        "\n",
        "        #token\n",
        "        for x in range(index_matrix.shape[3]):\n",
        "          tokens = [all_tokens[idx] for idx in index_matrix[i,j,k,x]]\n",
        "          head_tokens.append(tokens)\n",
        "        batch_tokens.append(head_tokens)\n",
        "      row_tokens.append(batch_tokens)\n",
        "    highest_tokens.append(row_tokens)\n",
        "  return np.array(highest_tokens)\n",
        "\n",
        "#format into a dataframe\n",
        "def highest_attended_tokens(index, values, tokens, example_ids, example=None):\n",
        "    dataframe=[]\n",
        "    for i in range(index.shape[0]):\n",
        "      for j in range(index.shape[1]):\n",
        "\n",
        "        if (example is not None) and (j != example):\n",
        "          continue\n",
        "\n",
        "        all_tokens = tokenizer.convert_ids_to_tokens(example_ids[j])\n",
        "\n",
        "        for k in range(index.shape[2]):\n",
        "          for x in range(index.shape[3]):\n",
        "            for y in range(index.shape[4]):\n",
        "              d = {\"token\":all_tokens[x], 'self_position':x, \n",
        "                  \"attended_token\": tokens[i,j,k,x,y],\n",
        "                  'token_position':index[i,j,k,x,y], \n",
        "                  'attention_scores':values[i,j,k,x,y],\n",
        "                  'layer':(i+1), 'head':(k+1),\n",
        "                  'rank':(y+1),\n",
        "                  'batch':j}\n",
        "              dataframe.append(d)\n",
        "    df = pd.DataFrame(dataframe)\n",
        "    return df"
      ],
      "metadata": {
        "id": "cO9aWGYxCNzu"
      },
      "id": "cO9aWGYxCNzu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combine the previous functions\n",
        "def highest_tokens(matrix, k, example_ids, example=None):\n",
        "  index, values = find_top_attention_unsummed(matrix, k)\n",
        "  highest_tokens = get_tokens(index, example_ids, example)\n",
        "  df = highest_attended_tokens(index, values, highest_tokens, example_ids, example)\n",
        "  return df"
      ],
      "metadata": {
        "id": "-TfHlPuAFALd"
      },
      "id": "-TfHlPuAFALd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Thus, for every token, we can get the top k tokens that this token attends to. Since it is a Dataframe, we can filter by batch, layer, head, rank, position, etc.,. The downsides are that its not very visually appealing despite being organized."
      ],
      "metadata": {
        "id": "vnhqif_VUqgU"
      },
      "id": "vnhqif_VUqgU"
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = highest_tokens(converted_mat_importance, 10, testexam[\"input_ids\"], 0)\n",
        "df2['attended_token'] = df2['attended_token'].str.replace('Ġ', '',)"
      ],
      "metadata": {
        "id": "5Pl_PhUcFVOb"
      },
      "id": "5Pl_PhUcFVOb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_rows', 100)\n",
        "reduced_df2 = df2[(df2['attended_token'].str.isalpha()) & ~(df2['attended_token'].isin(stopwords)) & ~(df2['attention_scores']==0)]"
      ],
      "metadata": {
        "id": "BnUQUylEa5cV"
      },
      "id": "BnUQUylEa5cV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_df2"
      ],
      "metadata": {
        "id": "bOTE2PCsyDGX"
      },
      "id": "bOTE2PCsyDGX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most ATTENDED token based on token."
      ],
      "metadata": {
        "id": "yjAW3WhV5t1K"
      },
      "id": "yjAW3WhV5t1K"
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tok = []\n",
        "\n",
        "for i in tqdm(range(df2['self_position'].max() + 1)):\n",
        "  tok.extend(list(reduced_df2 [(reduced_df2 ['self_position'] == i)][\"attended_token\"].unique()))\n",
        "token_count = np.ones(len(tok))\n",
        "\n",
        "dict_tokens_attended = {'tokens':tok, 'count':token_count}\n",
        "df_tokens_attended = pd.DataFrame(dict_tokens_attended)\n",
        "\n",
        "aggregation_function = {'count': 'sum'}\n",
        "df_tokens_attended = df_tokens_attended.groupby(df_tokens_attended['tokens']).aggregate(aggregation_function).reset_index()"
      ],
      "metadata": {
        "id": "AO07FhKpMm-x"
      },
      "id": "AO07FhKpMm-x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tokens_attended.sort_values(by='count', ascending=False)[:25]"
      ],
      "metadata": {
        "id": "s8rvkbKKMRlN"
      },
      "id": "s8rvkbKKMRlN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tokens_attended.sort_values(by='count', ascending=False)[:10].plot(kind='bar',x='tokens',y='count', title = \"count of top-k attended tokens\", figsize = (12,6), fontsize = 15)"
      ],
      "metadata": {
        "id": "RVk1t6eQBscs"
      },
      "id": "RVk1t6eQBscs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most Attended Token based on position ids. One potential problem with the above version is that tokens that appear multiple times in the example may be attended to much more frequently compared to other tokens. As such, there is the possibility of the count being inflated. To remedy this, instead of getting the unique attended tokens for each token, we get the attended token's position. We also grab the token associated with the position to make it more interpretable for the reader."
      ],
      "metadata": {
        "id": "xapzXKUR52rd"
      },
      "id": "xapzXKUR52rd"
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tokpos = []\n",
        "\n",
        "for i in tqdm(range(df2['self_position'].max() + 1)):\n",
        "  tokpos.extend(list(reduced_df2[(reduced_df2 ['self_position'] == i)][\"token_position\"].unique()))\n",
        "\n",
        "all_tokens = tokenizer.convert_ids_to_tokens(testexam[\"input_ids\"][0])\n",
        "\n",
        "token_count = np.ones(len(tokpos))\n",
        "\n",
        "dict_tokpos_attended = {'token':np.array(all_tokens)[tokpos], 'token_position':tokpos, 'count':token_count}\n",
        "df_tokpos_attended = pd.DataFrame(dict_tokpos_attended)\n",
        "df_tokpos_attended['token'] = df_tokpos_attended['token'].str.replace('Ġ', '',) "
      ],
      "metadata": {
        "id": "_UfOJfv0ViYa"
      },
      "id": "_UfOJfv0ViYa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "aggregation_function = {'count': 'sum'}\n",
        "df_tokpos_attended = df_tokpos_attended.groupby(['token',\t'token_position']).aggregate(aggregation_function).reset_index()\n",
        "df_tokpos_attended.sort_values(by='count', ascending=False)[:25]"
      ],
      "metadata": {
        "id": "PbMCEF-LVjqA"
      },
      "id": "PbMCEF-LVjqA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tokpos_attended[\"token (index)\"] = list(zip(df_tokpos_attended['token'], df_tokpos_attended['token_position']))"
      ],
      "metadata": {
        "id": "Z0nuLyZNC0vi"
      },
      "id": "Z0nuLyZNC0vi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tokpos_attended.sort_values(by='count', ascending=False)[:10].plot(kind='bar',x='token (index)',y='count', title= 'count of attended tokens by instance', figsize = (12,6), fontsize = 15)"
      ],
      "metadata": {
        "id": "JikVfifyByyX"
      },
      "id": "JikVfifyByyX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Other Direction\n",
        "\n",
        "Lets suppose we also want to find out which token attends to this token the most, as opposed to what this token attends to."
      ],
      "metadata": {
        "id": "kUfacG9RIRxD"
      },
      "id": "kUfacG9RIRxD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The code is functionally the same as the previous block with a few changes. The biggest change is in the find_top_attending_tokens, where we find the largest tokens in a different axis such that our outputs are now both of shape (layer, batch, head, k, seq_len), indicating that these tokens are the tokens with the highest amount of attention to this specific token.. \n",
        "\n",
        "However, before we pass this matrix into the next function, we swap the axes of our matrix into shape (layer, batch, head, seq_len, k) which is the same inputs that the functions above use.\n",
        "\n",
        "Finally, we adjust the column names of our dataframe as we do not want \"attended_token\" this time, but rather \"attending_token\"."
      ],
      "metadata": {
        "id": "9DXEwFcmIUKP"
      },
      "id": "9DXEwFcmIUKP"
    },
    {
      "cell_type": "code",
      "source": [
        "# get the top k and indexes and values for each row\n",
        "def find_top_attending_tokens(scores_mat, k):\n",
        "  indices = scores_mat.argsort(axis=3)[:, :, :, :-(k+1):-1, :]\n",
        "  vals = np.take_along_axis(scores_mat, indices, axis=3)  \n",
        "  return indices, vals\n",
        "\n",
        "#find the tokens using the index matrix and the all_tokens list to create a \n",
        "#matrix of tokens \n",
        "def get_tokens_attending(index_matrix, example_ids, example=None):\n",
        "  \n",
        "  # Make sure our example is not out of range.\n",
        "  assert example < index_matrix.shape[1]\n",
        "\n",
        "  highest_tokens = []\n",
        "  #layer\n",
        "  for i in range(index_matrix.shape[0]):\n",
        "    row_tokens = []\n",
        "    #batch\n",
        "    for j in range(index_matrix.shape[1]):\n",
        "      batch_tokens = []\n",
        "\n",
        "      if (example is not None) and (j != example):\n",
        "        continue\n",
        "\n",
        "      all_tokens = tokenizer.convert_ids_to_tokens(example_ids[j])\n",
        "\n",
        "      #head\n",
        "      for k in range(index_matrix.shape[2]):\n",
        "        head_tokens = []\n",
        "\n",
        "        #token\n",
        "        for x in range(index_matrix.shape[3]):\n",
        "\n",
        "          tokens = [all_tokens[idx] for idx in index_matrix[i,j,k,x]]\n",
        "          head_tokens.append(tokens)\n",
        "\n",
        "        batch_tokens.append(head_tokens)\n",
        "      row_tokens.append(batch_tokens)\n",
        "    highest_tokens.append(row_tokens)\n",
        "  return np.array(highest_tokens)\n",
        "\n",
        "#format into a dataframe\n",
        "def highest_attending_tokens(index, values, tokens, example_ids, example=None):\n",
        "    dataframe=[]\n",
        "    for i in range(index.shape[0]):\n",
        "      for j in range(index.shape[1]):\n",
        "\n",
        "        if (example is not None) and (j != example):\n",
        "          continue\n",
        "\n",
        "        all_tokens = tokenizer.convert_ids_to_tokens(example_ids[j])\n",
        "\n",
        "        for k in range(index.shape[2]):\n",
        "          for x in range(index.shape[3]):\n",
        "            for y in range(index.shape[4]):\n",
        "              d = {\"token\":all_tokens[x], 'self_position':x, \n",
        "                  \"attending_token\": tokens[i,j,k,x,y],\n",
        "                  'token_position':index[i,j,k,x,y], \n",
        "                  'attention_scores':values[i,j,k,x,y],\n",
        "                  'layer':(i+1), 'head':(k+1),\n",
        "                  'rank':(y+1),\n",
        "                  'batch':j}\n",
        "              dataframe.append(d)\n",
        "    df = pd.DataFrame(dataframe)\n",
        "    return df"
      ],
      "metadata": {
        "id": "iuaAqwbKIMms"
      },
      "id": "iuaAqwbKIMms",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#combine the previous functions\n",
        "def highest_tokens_attending(matrix, k, example_ids, example=None):\n",
        "  index, values = find_top_attending_tokens(matrix, k)\n",
        "  index = index.transpose((0,1,2,4,3))\n",
        "  values = values.transpose((0,1,2,4,3))\n",
        "  highest_tokens = get_tokens_attending(index, example_ids, example)\n",
        "  df = highest_attending_tokens(index, values, highest_tokens, example_ids, example)\n",
        "  return df"
      ],
      "metadata": {
        "id": "D75rDz32INQ5"
      },
      "id": "D75rDz32INQ5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df3 = highest_tokens_attending(converted_mat_importance, 10, testexam[\"input_ids\"], 0)\n",
        "df3['attending_token'] = df3['attending_token'].str.replace('Ġ', '',)"
      ],
      "metadata": {
        "id": "jvhvLr_xIQx4"
      },
      "id": "jvhvLr_xIQx4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reduced_df3 = df3[(df3['attending_token'].str.isalpha()) & ~(df3['attending_token'].isin(stopwords))  & ~(df3['attention_scores']==0)]\n",
        "reduced_df3"
      ],
      "metadata": {
        "id": "LHw86bnQqlW3"
      },
      "id": "LHw86bnQqlW3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most ATTENDING tokens based on token. These blocks of code are functionally identical to the version where we get the most ATTENDED to tokens. The difference here is that we grab our unique tokens for each of our input tokens using the \"attending_token\" column in dataframe reduced_df3."
      ],
      "metadata": {
        "id": "VH0HnTRE58LA"
      },
      "id": "VH0HnTRE58LA"
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tok2 = []\n",
        "\n",
        "for i in tqdm(range(df3['self_position'].max() + 1)):\n",
        "  tok2.extend(list(reduced_df3 [(reduced_df3 ['self_position'] == i)][\"attending_token\"].unique()))\n",
        "token_count2 = np.ones(len(tok2))\n",
        "\n",
        "dict_tokens_attending = {'tokens':tok2, 'count':token_count2}\n",
        "df_tokens_attending = pd.DataFrame(dict_tokens_attending)\n",
        "\n",
        "aggregation_function = {'count': 'sum'}\n",
        "df_tokens_attending = df_tokens_attending.groupby(df_tokens_attending['tokens']).aggregate(aggregation_function).reset_index()"
      ],
      "metadata": {
        "id": "Hoe6L9llrDVz"
      },
      "id": "Hoe6L9llrDVz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tokens_attending.sort_values(by='count', ascending=False)[:25]"
      ],
      "metadata": {
        "id": "OudmX1nGrWIa"
      },
      "id": "OudmX1nGrWIa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tokens_attending.sort_values(by='count', ascending=False)[:10].plot(kind='bar',x='tokens',y='count', title = \"count of top-k attending tokens\", figsize = (12,6), fontsize = 15)"
      ],
      "metadata": {
        "id": "c0ZPWM2hBkij"
      },
      "id": "c0ZPWM2hBkij",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most ATTENDING tokens based on position ids. This is functionally identical to the above version but instead of grabbing the unique ATTENDING tokens based on what the tokens are, we get the token's position instead. However, we do make sure to get the tokens themselves so we know what the position refers to."
      ],
      "metadata": {
        "id": "EfKjKTP95-qq"
      },
      "id": "EfKjKTP95-qq"
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "tokpos2 = []\n",
        "\n",
        "for i in tqdm(range(df3['self_position'].max() + 1)):\n",
        "  tokpos2.extend(list(reduced_df3[(reduced_df3 ['self_position'] == i)][\"token_position\"].unique()))\n",
        "\n",
        "all_tokens = tokenizer.convert_ids_to_tokens(testexam[\"input_ids\"][0])\n",
        "\n",
        "token_count2 = np.ones(len(tokpos2))\n",
        "\n",
        "dict_tokpos_attending = {'token':np.array(all_tokens)[tokpos2], 'token_position':tokpos2, 'count':token_count2}\n",
        "df_tokpos_attending = pd.DataFrame(dict_tokpos_attending)\n",
        "df_tokpos_attending['token'] = df_tokpos_attending['token'].str.replace('Ġ', '',) "
      ],
      "metadata": {
        "id": "OSlhqDABrmmd"
      },
      "id": "OSlhqDABrmmd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We aggregate the number of times the token appears in our dataframe, but this time, by the token position rather than the token itself. We might have a lot of tokens that appear in at least one token's most ATTENDING to, so we choose to only show the top k tokens."
      ],
      "metadata": {
        "id": "6vTTtfjebb_1"
      },
      "id": "6vTTtfjebb_1"
    },
    {
      "cell_type": "code",
      "source": [
        "aggregation_function = {'count': 'sum'}\n",
        "df_tokpos_attending = df_tokpos_attending.groupby(['token',\t'token_position']).aggregate(aggregation_function).reset_index()\n",
        "df_tokpos_attending.sort_values(by='count', ascending=False)[:25]"
      ],
      "metadata": {
        "id": "hOWc32Lprp06"
      },
      "id": "hOWc32Lprp06",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tokpos_attending[\"token (index)\"] = list(zip(df_tokpos_attending['token'], df_tokpos_attended['token_position']))"
      ],
      "metadata": {
        "id": "TJMIKdBDDhj1"
      },
      "id": "TJMIKdBDDhj1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_tokpos_attending.sort_values(by='count', ascending=False)[:10].plot(kind='bar',x='token (index)',y='count', title= 'count of attending tokens by instance', figsize = (12,6), fontsize=15)"
      ],
      "metadata": {
        "id": "xuFqhualB2f6"
      },
      "id": "xuFqhualB2f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Common Tokens"
      ],
      "metadata": {
        "id": "q1qt6jdDUwkA"
      },
      "id": "q1qt6jdDUwkA"
    },
    {
      "cell_type": "code",
      "source": [
        "s1 = pd.merge(df_tokens_attended.sort_values(by='count', ascending=False)[:10], \n",
        "              df_tokens_attending.sort_values(by='count', ascending=False)[:10], \n",
        "              how='inner', on=['tokens'])\n",
        "s1 = s1.rename(columns = {\"count_x\": \"count_attended\", \"count_y\":\"count_attending\"})\n",
        "s1"
      ],
      "metadata": {
        "id": "WoPLI9pIXG9g"
      },
      "id": "WoPLI9pIXG9g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "s2 = pd.merge(df_tokpos_attended.sort_values(by='count', ascending=False)[:10], \n",
        "              df_tokpos_attending.sort_values(by='count', ascending=False)[:10], \n",
        "              how='inner', on=['token_position'])\n",
        "s2 = s2.rename(columns = {\"count_x\": \"count_attended\", \"count_y\":\"count_attending\"})\n",
        "s2 = s2.drop(columns=['token (index)_x', 'token (index)_y', 'token_y'])\n",
        "s2"
      ],
      "metadata": {
        "id": "eId6xIMYXg8W"
      },
      "id": "eId6xIMYXg8W",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Total attention for each word"
      ],
      "metadata": {
        "id": "SppU6voZpLiM"
      },
      "id": "SppU6voZpLiM"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get the sum of the attentions for all the tokens (column-wise). In other words, find out how much every word is attended to. We then normalize the values to a range of [0,1]."
      ],
      "metadata": {
        "id": "u0ViKJAn7Ap0"
      },
      "id": "u0ViKJAn7Ap0"
    },
    {
      "cell_type": "code",
      "source": [
        "attention_matrix_importance = converted_mat_importance.sum(axis=3)\n",
        "print(attention_matrix_importance.shape)"
      ],
      "metadata": {
        "id": "EhMNdupbxFrx"
      },
      "id": "EhMNdupbxFrx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(data):\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
      ],
      "metadata": {
        "id": "namX3_nLAORx"
      },
      "id": "namX3_nLAORx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_matrix_importance = normalize(attention_matrix_importance)"
      ],
      "metadata": {
        "id": "ICgX3sa5AVh7"
      },
      "id": "ICgX3sa5AVh7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### PDF view\n",
        "\n",
        "A dataframe is good for picking out information from the example, but it isn't the best being a easy to read visualization. Its easier to see how much each word is attended to in an example if we have the actual example, with the words highlighted based on the magnitude of attention."
      ],
      "metadata": {
        "id": "kqgAjuIwph32"
      },
      "id": "kqgAjuIwph32"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "We use https://github.com/jiesutd/Text-Attention-Heatmap-Visualization to show how much each token in the example is attended to, up to the max number of tokens we specified earlier.\n",
        "\n",
        "In short, these functions iterate over the list of attentions and tokens, cleans the tokens to remove special characters, and normalizes the data if you wish for it to.\n",
        "\n",
        "This notebook serves to explore one specific example in detail. If you wish to play around and/or convert multiple examples into PDFs, I recommend going to this [notebook](https://colab.research.google.com/drive/1Bjb4iSS9tm22rRERmeLEvLcorrGG4prJ?usp=sharing)"
      ],
      "metadata": {
        "id": "DjTu0_guLI1T"
      },
      "id": "DjTu0_guLI1T"
    },
    {
      "cell_type": "code",
      "source": [
        "## convert the text/attention list to latex code, which will further generates the text heatmap based on attention weights.\n",
        "import numpy as np\n",
        "\n",
        "latex_special_token = [\"!@#$%^&*(){}\"]\n",
        "\n",
        "def generate(text_list, attention_list, latex_file, color='red', rescale_value = True):\n",
        "\tassert(len(text_list) == len(attention_list))\n",
        "\tif rescale_value:\n",
        "\t\tattention_list = rescale(attention_list)\n",
        "\tword_num = len(text_list)\n",
        "\ttext_list = clean_word(text_list)\n",
        "\twith open(latex_file,'w') as f:\n",
        "\t\tf.write(r'''\\documentclass[varwidth]{standalone}\n",
        "\\special{papersize=210mm,297mm}\n",
        "\\usepackage{color}\n",
        "\\usepackage{tcolorbox}\n",
        "\\usepackage{CJK}\n",
        "\\usepackage{adjustbox}\n",
        "\\tcbset{width=0.9\\textwidth,boxrule=0pt,colback=red,arc=0pt,auto outer arc,left=0pt,right=0pt,boxsep=5pt}\n",
        "\\begin{document}\n",
        "\\begin{CJK*}{UTF8}{gbsn}'''+'\\n')\n",
        "\t\tstring = r'''{\\setlength{\\fboxsep}{0pt}\\colorbox{white!0}{\\parbox{0.9\\textwidth}{'''+\"\\n\"\n",
        "\t\tfor idx in range(word_num):\n",
        "\t\t\tstring += \"\\\\colorbox{%s!%s}{\"%(color, attention_list[idx])+\"\\\\strut \" + text_list[idx]+\"} \"\n",
        "\t\tstring += \"\\n}}}\"\n",
        "\t\tf.write(string+'\\n')\n",
        "\t\tf.write(r'''\\end{CJK*}\n",
        "\\end{document}''')\n",
        "\n",
        "def rescale(input_list):\n",
        "\tthe_array = np.asarray(input_list)\n",
        "\tthe_max = np.max(the_array)\n",
        "\tthe_min = np.min(the_array)\n",
        "\trescale = ((the_array - the_min)/(the_max-the_min))*100\n",
        "\treturn rescale.tolist()\n",
        "\n",
        "\n",
        "def clean_word(word_list):\n",
        "\tnew_word_list = []\n",
        "\tfor word in word_list:\n",
        "\t\tfor special_sensitive in [\"\\\\\", \"^\"]:\n",
        "\t\t\tif special_sensitive in word:\n",
        "\t\t\t\tword = word.replace(special_sensitive, '')\n",
        "\t\tfor latex_sensitive in [\"%\", \"&\", \"#\", \"_\",  \"{\", \"}\"]:\n",
        "\t\t\tif latex_sensitive in word:\n",
        "\t\t\t\tword = word.replace(latex_sensitive, '\\\\' +latex_sensitive)\n",
        "\t\tnew_word_list.append(word)\n",
        "\treturn new_word_list"
      ],
      "metadata": {
        "id": "FCsAm5i3jmKA"
      },
      "id": "FCsAm5i3jmKA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_tokens2 = tokenizer.convert_ids_to_tokens(testexam[\"input_ids\"][0])"
      ],
      "metadata": {
        "id": "N-Wr_p4LsTv-"
      },
      "id": "N-Wr_p4LsTv-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Change \"papers\" in the following to news/whatever alternative dataset if you change the dataset used"
      ],
      "metadata": {
        "id": "f2EysmpsD0ms"
      },
      "id": "f2EysmpsD0ms"
    },
    {
      "cell_type": "code",
      "source": [
        "average_attention = attention_matrix_importance.squeeze().sum(axis=1)\n",
        "average_attention = average_attention.sum(axis=0)"
      ],
      "metadata": {
        "id": "goaO9arYjopz"
      },
      "id": "goaO9arYjopz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We call the main function above. It takes in a list of tokens, a list of attentions, a title, and a colour. Please change \"papers\" to whatever your project requires."
      ],
      "metadata": {
        "id": "hJ31b-GZxTg5"
      },
      "id": "hJ31b-GZxTg5"
    },
    {
      "cell_type": "code",
      "source": [
        "title_all = f\"notes_{test_val[0]}.tex\"\n",
        "generate(all_tokens2, (average_attention*100), title_all, 'red')"
      ],
      "metadata": {
        "id": "htPtu7kErWfN"
      },
      "id": "htPtu7kErWfN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets suppose you don't want to find out the attentions over all layers, but just one layer. You can do that by doing one less summation and instead picking out the layer you want immediately. Here we are picking out the last layer."
      ],
      "metadata": {
        "id": "TCKn9M3ILsEh"
      },
      "id": "TCKn9M3ILsEh"
    },
    {
      "cell_type": "code",
      "source": [
        "print(attention_matrix_importance[11].squeeze().shape)\n",
        "average_attention_final_layer = attention_matrix_importance[11].squeeze().sum(axis=0)"
      ],
      "metadata": {
        "id": "GaM018NVunZH"
      },
      "id": "GaM018NVunZH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "title_last_layer = f\"notes_{test_val[0]}_layer_12_only.tex\"\n",
        "generate(all_tokens2, (average_attention_final_layer*100), title_last_layer, 'red')"
      ],
      "metadata": {
        "id": "fSl21NvVyW-n"
      },
      "id": "fSl21NvVyW-n",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Highest attended tokens\n",
        "\n",
        "Here we are getting the tokens with the largest total attention scores for a specific example in a batch, over all heads and all layers. These functions are roughly the same as the functions in the section \"Every token's top attended\". Here we have modified the functions as we have one less axis to deal with and are just finding out which tokens have the largest attention score."
      ],
      "metadata": {
        "id": "F6pfY5SAp2ni"
      },
      "id": "F6pfY5SAp2ni"
    },
    {
      "cell_type": "markdown",
      "source": [
        "`find_top_attention` takes the summed attention matrix (layer, batch, head, seq_len) obtains the top k values for every head, batch, and layer. Along with the values, it also returns the positions of each value so we know what token each value returned corresponds to. The output of the function are two arrays of shape (layer, batch, head, k)."
      ],
      "metadata": {
        "id": "VNQg3Ham5t8L"
      },
      "id": "VNQg3Ham5t8L"
    },
    {
      "cell_type": "code",
      "source": [
        "def find_top_attention(scores_mat, k):\n",
        "  indices = scores_mat.argsort(axis=3)[:, :, :, :-(k+1):-1]\n",
        "  vals = np.take_along_axis(scores_mat, indices, axis=3)\n",
        "  return indices, vals"
      ],
      "metadata": {
        "id": "h6nLRpgBltnk"
      },
      "id": "h6nLRpgBltnk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`get_tokens2` takes in three inputs: a matrix of indices with shape (layer, batch, head, k), the tensor of input_ids used for prediction, and an example number, and finds the associated token at each index stored in the input matrix. The example number serves only to select an example from the batch. If we do not pass in an example as an input, we will iterate over all of the batches. The output of the function is an array of shape (layer, batch, head, k), where each item is a token from the respective example."
      ],
      "metadata": {
        "id": "FD54nkWS6O_H"
      },
      "id": "FD54nkWS6O_H"
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokens2(index_matrix, example_ids, example=None):\n",
        "\n",
        "  # Make sure our example is not out of range.\n",
        "  assert example < index_matrix.shape[1]\n",
        "\n",
        "  highest_tokens = []\n",
        "  #layer\n",
        "  for i in range(index_matrix.shape[0]):\n",
        "    row_tokens = []\n",
        "    #batch\n",
        "    for j in range(index_matrix.shape[1]):\n",
        "      batch_tokens = []\n",
        "\n",
        "      if example is not None and j != example:\n",
        "        continue\n",
        "\n",
        "      all_tokens = tokenizer.convert_ids_to_tokens(example_ids[j])\n",
        "\n",
        "      #head\n",
        "      for k in range(index_matrix.shape[2]):\n",
        "        tokens = [all_tokens[idx] for idx in index_matrix[i,j,k]]\n",
        "        batch_tokens.append(tokens)\n",
        "      row_tokens.append(batch_tokens)\n",
        "    highest_tokens.append(row_tokens)\n",
        "  return np.array(highest_tokens)"
      ],
      "metadata": {
        "id": "NSv7L_PVvLdZ"
      },
      "id": "NSv7L_PVvLdZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`highest_attended_dataframe` takes in 4 inputs. It takes in a matrix of indicies, matrix of values, matrix of tokens, all with shape (layer, batch, head, k), and an example number. The example number again serves only to pick an example from the batch and if not used, will iterate over all batches. We use these matrices to create a Pandas Dataframe, where each row of the dataframe contains the current token, the position of the current token, the value of the current token, and the rank of the token w.r.t. the current layer, head and batch. We also save the layer number, the head number and the batch in the dataframe. "
      ],
      "metadata": {
        "id": "rl2V1kNP6tlE"
      },
      "id": "rl2V1kNP6tlE"
    },
    {
      "cell_type": "code",
      "source": [
        "def highest_attended_dataframe(index, values, tokens, example=None):\n",
        "\n",
        "    dataframe=[]\n",
        "    #layer\n",
        "    for i in range(index.shape[0]):\n",
        "      #batch\n",
        "      for j in range(index.shape[1]):\n",
        "        if example is not None and j != example:\n",
        "          continue\n",
        "        #head\n",
        "        for k in range(index.shape[2]):\n",
        "          #token\n",
        "          for x in range(index.shape[3]):\n",
        "            d = {\"token\":tokens[i,j,k,x], 'position':index[i,j,k,x], \n",
        "                'attention_scores':values[i,j,k,x],\n",
        "                 'layer':(i+1), 'head':(k+1),\n",
        "                 'rank':(x+1),\n",
        "                 'batch':j}\n",
        "            dataframe.append(d)\n",
        "    df = pd.DataFrame(dataframe)\n",
        "    return df"
      ],
      "metadata": {
        "id": "YCWTK-E49C6e"
      },
      "id": "YCWTK-E49C6e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here is the function we use to wrap together our three functions above. It takes in a matrix of shape (layer, batch, head, seq_len, seq_len), a number k, the example input ids, and the example number in the batch."
      ],
      "metadata": {
        "id": "seLmFh4Gxe_f"
      },
      "id": "seLmFh4Gxe_f"
    },
    {
      "cell_type": "code",
      "source": [
        "def highest_attentions_summed(matrix, k, example_ids, example=None):\n",
        "  index, values = find_top_attention(matrix, k)\n",
        "  highest_tokens = get_tokens2(index, example_ids, example)\n",
        "  df_highest = highest_attended_dataframe(index, values, highest_tokens, example)\n",
        "  return df_highest"
      ],
      "metadata": {
        "id": "u7XhkMteJGAo"
      },
      "id": "u7XhkMteJGAo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_highest = highest_attentions_summed(attention_matrix_importance, 5, testexam[\"input_ids\"],  0)\n",
        "df_highest"
      ],
      "metadata": {
        "id": "qdnmbBl2_m2p"
      },
      "id": "qdnmbBl2_m2p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_highest_reduced = df_highest[(df_highest['token'].str.isalpha()) & ~(df_highest['token'].isin(stopwords))  & ~(df_highest['token']==0)]\n",
        "df_highest_reduced"
      ],
      "metadata": {
        "id": "98gcjYjKBjeD"
      },
      "id": "98gcjYjKBjeD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_highest[df_highest[\"layer\"]==12]['token'].value_counts()"
      ],
      "metadata": {
        "id": "w36y6YG1BhpC"
      },
      "id": "w36y6YG1BhpC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using this, we could get visualizations such as seeing the trend of the topk token's attentions across layers or heads."
      ],
      "metadata": {
        "id": "wXzU3ayvG7vH"
      },
      "id": "wXzU3ayvG7vH"
    },
    {
      "cell_type": "code",
      "source": [
        "df_highest[df_highest[\"layer\"]==12]"
      ],
      "metadata": {
        "id": "fdz5ZJ_RIfSA"
      },
      "id": "fdz5ZJ_RIfSA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can visualize the results using Pandas and seaborn (extension of matplotlib) by creating the dataframe you wish to visualize and setting your x,y values with the appropriate column(s). We import ipywidgets so we have a degree of control over what visualizations we want to look at without having to constantly rerun the code. Primarily we want the select multiple widget as we have many heads and layers that we may possibly want to examine the trends for. \n",
        "\n",
        "**We are assuming that there is only one example in the batch.**"
      ],
      "metadata": {
        "id": "0q3jqu51cjQ7"
      },
      "id": "0q3jqu51cjQ7"
    },
    {
      "cell_type": "code",
      "source": [
        "head_values = range(1,13)\n",
        "layer_values = range(1,13)\n",
        "\n",
        "import ipywidgets as widgets\n",
        "\n",
        "ddhead = widgets.SelectMultiple(\n",
        "    options=head_values,\n",
        "    #rows=10,\n",
        "    description='heads',\n",
        "    disabled=False\n",
        ")\n",
        "ddlayer = widgets.SelectMultiple(\n",
        "    options=layer_values,\n",
        "    #rows=10,\n",
        "    description='layer',\n",
        "    disabled=False\n",
        ")\n"
      ],
      "metadata": {
        "id": "f1PxxP2JTV9i"
      },
      "id": "f1PxxP2JTV9i",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we plot the trend of attentions for the topk tokens across any and all heads and any and all layers. We create a function that filters the dataframe based on the values you select and creates a lineplot to visualize the trend. You can select which head you want or what layer you want by clicking on the corresponding item in the list. The scale of the y-axis will adjust accordingly depending on your selection, but can be fixed if needed. Selecting multiple heads and layers may take a bit to process."
      ],
      "metadata": {
        "id": "odRXpEXmelaV"
      },
      "id": "odRXpEXmelaV"
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "ui = widgets.HBox([ddhead,ddlayer])\n",
        "\n",
        "def drawplot(layer, head):\n",
        "  sns.set(rc = {'figure.figsize':(40,15)})\n",
        "\n",
        "  # if you want to keep the scale consistent for any config, uncomment this line\n",
        "  # plt.ylim(-0.01, 1.05)\n",
        "\n",
        "  sns.lineplot(x= 'rank', y = \"attention_scores\", data=df_highest[(df_highest['layer'].isin(layer)) & (df_highest['head'].isin(head))], hue='layer', style='head', palette='viridis', legend='full')\n",
        "\n",
        "out= widgets.interactive_output(drawplot, {'layer':ddlayer, 'head':ddhead})\n",
        "display(ui,out)"
      ],
      "metadata": {
        "id": "UUTKzi6LHHA2"
      },
      "id": "UUTKzi6LHHA2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Selective Information\n",
        "\n",
        "We can get the attention of a token at a position for each layer and head as well as the rank of the token within that head. Take one example at a time as each example has different tokens. \n",
        "\n",
        "Pros: can isolate for layers and/or heads. Cons: not much context for the attention scores"
      ],
      "metadata": {
        "id": "rL1qTv1W8ugZ"
      },
      "id": "rL1qTv1W8ugZ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "`position_attention` takes in 4 inputs. It takes in a matrix of shape (layer, batch, head, seq_len), the position number you wish to look at, the tensor of input_ids used for prediction and an example number. The example number again serves only to pick an example from the batch and if not used, will iterate over all batches. We use these matrices to create a Pandas Dataframe, where each row of the dataframe contains a token, the position the token, the value of the current token, and the rank of the token w.r.t. the current layer, head and batch. We also save the layer number, the head number and the batch in the dataframe for each row. \n",
        "\n",
        "While this looks similar to the previous dataframe, this is restricted to what particular position you pass in as an argument and may show you tokens with a rank number higher than your threshold"
      ],
      "metadata": {
        "id": "OXRpi0V-81A7"
      },
      "id": "OXRpi0V-81A7"
    },
    {
      "cell_type": "code",
      "source": [
        "def position_attention(agg_matrix, position, example_ids, example=None):\n",
        "  dataframe = []\n",
        "  # Make sure our example is not out of range.\n",
        "  if example is not None:\n",
        "    assert example < agg_matrix.shape[1]\n",
        "\n",
        "  if example is not None:\n",
        "    new_mat = agg_matrix[:, example, :]\n",
        "    all_tokens = tokenizer.convert_ids_to_tokens(example_ids[example])\n",
        "    new_mat = new_mat.squeeze()\n",
        "    #layer\n",
        "    for i in range(new_mat.shape[0]):\n",
        "      #head\n",
        "      for j in range(new_mat.shape[1]):\n",
        "        temp = new_mat[i,j].argsort()[::-1]\n",
        "        temp = np.where(temp==position)[0].squeeze() + 1\n",
        "        d = {\"token\":all_tokens[position], 'position':position, \n",
        "            'attention_scores':new_mat[i,j,position], 'layer':(i+1), 'head':(j+1),\n",
        "            'rank':temp, 'batch':example}\n",
        "        dataframe.append(d)\n",
        "  else:\n",
        "    new_mat = agg_matrix\n",
        "    #layer\n",
        "    for i in range(new_mat.shape[0]):\n",
        "      #batch\n",
        "      for j in range(new_mat.shape[1]):\n",
        "        all_tokens = tokenizer.convert_ids_to_tokens(example_ids[j])\n",
        "        #head\n",
        "        for k in range(new_mat.shape[2]):\n",
        "          temp = new_mat[i,j,k].argsort()[::-1]\n",
        "          temp = np.where(temp==position)[0].squeeze() + 1\n",
        "          d = {\"token\":all_tokens[position], 'position':position, \n",
        "              'attention_scores':new_mat[i,j,k,position], 'layer':(i+1), 'head':(k+1),\n",
        "              'rank':temp, 'batch':j}\n",
        "          dataframe.append(d)\n",
        "  df = pd.DataFrame(dataframe)\n",
        "  return df"
      ],
      "metadata": {
        "id": "jrjjYyR-siot"
      },
      "id": "jrjjYyR-siot",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "position_df = position_attention(attention_matrix_importance, 1782, testexam[\"input_ids\"], 0)\n",
        "position_df[position_df[\"head\"]==9]"
      ],
      "metadata": {
        "id": "YpSZKQ0nvt5T"
      },
      "id": "YpSZKQ0nvt5T",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are visualizing the attention distribution for the token at position 0 over all layers. We create a function that filters the dataframe based on the values you select and creates a scatterplot. Whatever values you selected will be displayed in the visualization. You can select which head you wish to visualize, and if you want to pick more than one, you can crtl-click or shift click to select multiple. The scale of the y-axis will adjust accordingly depending on your selection, but can be fixed if needed. Selecting multiple heads may take a bit to process."
      ],
      "metadata": {
        "id": "Crz4zrvr-o-o"
      },
      "id": "Crz4zrvr-o-o"
    },
    {
      "cell_type": "code",
      "source": [
        "ui2 = widgets.HBox([ddhead])\n",
        "\n",
        "def drawplot2(head):\n",
        "  sns.set(rc = {'figure.figsize':(35,15)})\n",
        "\n",
        "  # if you want to keep the scale consistent for any config, uncomment this line\n",
        "  # plt.ylim(-0.01, 1.05)\n",
        "\n",
        "  sns.scatterplot(x= 'layer', y = \"attention_scores\", data=position_df[position_df['head'].isin(head)], hue=\"head\", style='head', s=75, palette='viridis', legend='full')\n",
        "\n",
        "out2= widgets.interactive_output(drawplot2, {'head':ddhead})\n",
        "display(ui2,out2)"
      ],
      "metadata": {
        "id": "arbMEHsqEexa"
      },
      "id": "arbMEHsqEexa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### The Complete Package\n",
        "\n",
        "If really needed, you can just have the full matrix of the position, ranks, attention scores, layers, and heads of each token per example. \n",
        "\n",
        "Pros: can search up whatever is needed. Has access to all the information and can be extracted for comparisons like the previous two dataframes.\n",
        "\n",
        "Cons: have to know what you want and manually look it up."
      ],
      "metadata": {
        "id": "XwrbWhoC8bbv"
      },
      "id": "XwrbWhoC8bbv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "`full_matrix` takes in 3 inputs. It takes in a matrix of shape (layer, batch, head, seq_len, k), the tensor of input_ids used for prediction and an example number. The example number again serves only to pick an example from the batch and if not used, will iterate over all batches. We use these matrices to create a Pandas Dataframe, where each row of the dataframe contains a token, the position the token, the value of the current token, and the rank of the token w.r.t. the current layer, head and batch. We also save the layer number, the head number and the batch in the dataframe. "
      ],
      "metadata": {
        "id": "7L4hHC9M-1PL"
      },
      "id": "7L4hHC9M-1PL"
    },
    {
      "cell_type": "code",
      "source": [
        "def full_matrix(agg_matrix, example_ids, example=None):\n",
        "  dataframe=[]\n",
        "\n",
        "  if example is not None:\n",
        "    new_mat = agg_matrix[:, example]\n",
        "    new_mat = new_mat.squeeze()\n",
        "    print(new_mat.shape)\n",
        "    all_tokens = tokenizer.convert_ids_to_tokens(example_ids[example])\n",
        "\n",
        "    #layer\n",
        "    for i in range(new_mat.shape[0]):\n",
        "      #head\n",
        "      for j in range(new_mat.shape[1]):\n",
        "        temp = new_mat[i,j].argsort()[::-1]      \n",
        "        #token\n",
        "        for k in range(new_mat.shape[2]):\n",
        "          temp2 = np.where(temp==k)[0].squeeze() + 1\n",
        "          d = {\"token\":all_tokens[k], 'position':k, \n",
        "              'attention_scores':new_mat[i,j,k], 'layer':(i+1), 'head':(j+1),\n",
        "              'rank':temp2}\n",
        "          dataframe.append(d)\n",
        "  else:\n",
        "    new_mat = agg_matrix\n",
        "    print(new_mat.shape)\n",
        "    \n",
        "    #layer\n",
        "    for i in range(new_mat.shape[0]):\n",
        "      #batch\n",
        "      for j in range(new_mat.shape[1]):\n",
        "\n",
        "        all_tokens = tokenizer.convert_ids_to_tokens(example_ids[j])\n",
        "\n",
        "        #head\n",
        "        for k in range(new_mat.shape[2]):\n",
        "          temp = new_mat[i,j,k].argsort()[::-1]      \n",
        "          #token\n",
        "          for x in range(new_mat.shape[3]):\n",
        "            temp2 = np.where(temp==x)[0].squeeze() + 1\n",
        "            d = {\"token\":all_tokens[x], 'position':x, \n",
        "                'attention_scores':new_mat[i,j,k,x], 'layer':(i+1), 'head':(k+1),\n",
        "                'rank':temp2, 'batch':j}\n",
        "            dataframe.append(d)\n",
        "  df = pd.DataFrame(dataframe)\n",
        "  return df"
      ],
      "metadata": {
        "id": "n8WZcz_w7Chc"
      },
      "id": "n8WZcz_w7Chc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_mat = full_matrix(attention_matrix_importance, testexam[\"input_ids\"])"
      ],
      "metadata": {
        "id": "mn8NifO00kND"
      },
      "id": "mn8NifO00kND",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_mat[full_mat['layer']==12]"
      ],
      "metadata": {
        "id": "-kn2bEWf8BN0"
      },
      "id": "-kn2bEWf8BN0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this example, we look at the attention distribution for all tokens over all heads and layers. We create a function that filters the dataframe based on the values you select and creates a scatterplot. Whatever values you selected will be displayed in the visualization. You can select which head you want or what layer you want by clicking on the corresponding item in the list. Furthermore, if you want to select multiple, you can by using ctrl click or shift click. The scale of the y-axis will adjust accordingly depending on your selection, but can be fixed if needed.Selecting multiple heads may take a bit of time to process."
      ],
      "metadata": {
        "id": "29Va4kVnhiyT"
      },
      "id": "29Va4kVnhiyT"
    },
    {
      "cell_type": "code",
      "source": [
        "ui = widgets.HBox([ddhead,ddlayer])\n",
        "\n",
        "def drawplotfull(layer, head):\n",
        "  sns.set(rc = {'figure.figsize':(40,20)})\n",
        "\n",
        "  # if you want to keep the scale consistent for any config, uncomment this line\n",
        "  # plt.ylim(-0.01, 1.05)\n",
        "\n",
        "  sns.scatterplot(x= 'position', y = \"attention_scores\", data=full_mat[(full_mat['layer'].isin(layer)) & (full_mat['head'].isin(head))], hue='layer', style='head', palette='viridis', markers=True, legend='full')\n",
        "\n",
        "out= widgets.interactive_output(drawplotfull, {'layer':ddlayer, 'head':ddhead})\n",
        "display(ui,out)\n"
      ],
      "metadata": {
        "id": "DigndFupQ2EH"
      },
      "id": "DigndFupQ2EH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we show an example of how to get the top k attended tokens or filtered by position_ids like we did previously."
      ],
      "metadata": {
        "id": "cMgY-RhNGkis"
      },
      "id": "cMgY-RhNGkis"
    },
    {
      "cell_type": "code",
      "source": [
        "full_mat[(full_mat['rank']>=1) & (full_mat['rank'] <= 10)].sort_values(by = ['layer', 'head', 'rank'])"
      ],
      "metadata": {
        "id": "H2LPXz8VGcIT"
      },
      "id": "H2LPXz8VGcIT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "full_mat[(full_mat['position']==0)]"
      ],
      "metadata": {
        "id": "5tMCyMgXGr07"
      },
      "id": "5tMCyMgXGr07",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "From here, we know how to get the visualizations for the dataframe. Slight modifications are needed to use the correct dataframe. Otherwise, you can modify the visualization to display whichever piece of information you want as there are other graphs this works on such as boxplots, histograms, piecharts, etc. "
      ],
      "metadata": {
        "id": "GL_T_4Q5GlyZ"
      },
      "id": "GL_T_4Q5GlyZ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "348e26a35773433f9af6c66e8b2062ce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c0757ff78220438f9a31d1ebc0713ebb",
              "IPY_MODEL_f0c7705644f94593826ad4f9f701b856",
              "IPY_MODEL_388694d265c747ae84b2aac6c98393ca"
            ],
            "layout": "IPY_MODEL_47e752177a6343ac9d21e137cbbe14f9"
          }
        },
        "c0757ff78220438f9a31d1ebc0713ebb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a6b996d2dc8435ea348476c1ca1480e",
            "placeholder": "​",
            "style": "IPY_MODEL_92a857a6e22b467aaff96c900767d1e1",
            "value": ""
          }
        },
        "f0c7705644f94593826ad4f9f701b856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6f733a726808498293069cfba37480d4",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_339d250cc3294fe59effc79a113e5dde",
            "value": 0
          }
        },
        "388694d265c747ae84b2aac6c98393ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b6b9fd0cd624cfc86c3002d4dee721d",
            "placeholder": "​",
            "style": "IPY_MODEL_491628585358421a85c3ab9b1f91610f",
            "value": " 0/0 [00:00&lt;?, ?it/s]"
          }
        },
        "47e752177a6343ac9d21e137cbbe14f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a6b996d2dc8435ea348476c1ca1480e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92a857a6e22b467aaff96c900767d1e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6f733a726808498293069cfba37480d4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "339d250cc3294fe59effc79a113e5dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b6b9fd0cd624cfc86c3002d4dee721d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "491628585358421a85c3ab9b1f91610f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1402dcbda72454bb59afe4533ecd258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2664f3e8dd514c7e91a734ec09f327b8",
              "IPY_MODEL_90468f8b120b4f6b973e79fc11398e82",
              "IPY_MODEL_b700b5245e17421a9d108cc3e72a5ab4"
            ],
            "layout": "IPY_MODEL_052a1dc0347241d9be256d7555cc4e30"
          }
        },
        "2664f3e8dd514c7e91a734ec09f327b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e6c738eea564dcfb39825da03ce8920",
            "placeholder": "​",
            "style": "IPY_MODEL_9ad9553934664533bb3c9766ddf861e3",
            "value": "Downloading: 100%"
          }
        },
        "90468f8b120b4f6b973e79fc11398e82": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1a00966110804e8b9d4ef91f09b7cce9",
            "max": 694,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c14cb9bc189344599f640a212d28ca22",
            "value": 694
          }
        },
        "b700b5245e17421a9d108cc3e72a5ab4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64503a1f9dce4415818bdf3066785f26",
            "placeholder": "​",
            "style": "IPY_MODEL_f573268e822e441592e5333b9f78b72e",
            "value": " 694/694 [00:00&lt;00:00, 25.1kB/s]"
          }
        },
        "052a1dc0347241d9be256d7555cc4e30": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e6c738eea564dcfb39825da03ce8920": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ad9553934664533bb3c9766ddf861e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1a00966110804e8b9d4ef91f09b7cce9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c14cb9bc189344599f640a212d28ca22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64503a1f9dce4415818bdf3066785f26": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f573268e822e441592e5333b9f78b72e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f0c412046fb649fc80bbc0c6542437a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1187826e293f486f8e2dfdc1a2e97cf1",
              "IPY_MODEL_c1835b81be1e425097e53636f8197f6e",
              "IPY_MODEL_78be2670cf544233bdad53a9582c15a0"
            ],
            "layout": "IPY_MODEL_87bbb54fe3e54547a906d2b0fba71095"
          }
        },
        "1187826e293f486f8e2dfdc1a2e97cf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db14b41a9b594a8aa9d8fbcdf8a8a0de",
            "placeholder": "​",
            "style": "IPY_MODEL_d51901cfc903479c8c7e0cc538581f1b",
            "value": "Downloading: 100%"
          }
        },
        "c1835b81be1e425097e53636f8197f6e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e6aebe3a5414949bf8d9b53a6f6dd0c",
            "max": 898823,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c35677775e334eaca07503b3f62c92af",
            "value": 898823
          }
        },
        "78be2670cf544233bdad53a9582c15a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0a746881b6ce4ac7a477ad502fbfe4cb",
            "placeholder": "​",
            "style": "IPY_MODEL_a3b3a1d9527c49e69a3c3d0fc64594d3",
            "value": " 899k/899k [00:00&lt;00:00, 2.77MB/s]"
          }
        },
        "87bbb54fe3e54547a906d2b0fba71095": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db14b41a9b594a8aa9d8fbcdf8a8a0de": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d51901cfc903479c8c7e0cc538581f1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e6aebe3a5414949bf8d9b53a6f6dd0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c35677775e334eaca07503b3f62c92af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0a746881b6ce4ac7a477ad502fbfe4cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a3b3a1d9527c49e69a3c3d0fc64594d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "272fc32b90c5417289de2fb6c63d406c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d9eb52445156467897edd5ac51bbf5aa",
              "IPY_MODEL_32a34884830b45179c6bd8db50f1b369",
              "IPY_MODEL_c13099dcce374bb9ac020086bfc94fe1"
            ],
            "layout": "IPY_MODEL_bd01cb009e1b48c2a6d15356da74dd3a"
          }
        },
        "d9eb52445156467897edd5ac51bbf5aa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4d13f72b9b5459b8a7aa3ece1ea5dd0",
            "placeholder": "​",
            "style": "IPY_MODEL_16e27eac089048d0bcdfe6846304f60b",
            "value": "Downloading: 100%"
          }
        },
        "32a34884830b45179c6bd8db50f1b369": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_527c3e60175743c4bfb360d535e743f6",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a44d7cb0802d4bc5a884587dda09a532",
            "value": 456318
          }
        },
        "c13099dcce374bb9ac020086bfc94fe1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_585998d4cfc04847bbfb1b5f34054c59",
            "placeholder": "​",
            "style": "IPY_MODEL_7218e94823b64456ad0add357ec97f14",
            "value": " 456k/456k [00:00&lt;00:00, 879kB/s]"
          }
        },
        "bd01cb009e1b48c2a6d15356da74dd3a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4d13f72b9b5459b8a7aa3ece1ea5dd0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16e27eac089048d0bcdfe6846304f60b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "527c3e60175743c4bfb360d535e743f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a44d7cb0802d4bc5a884587dda09a532": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "585998d4cfc04847bbfb1b5f34054c59": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7218e94823b64456ad0add357ec97f14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ca15c95a8f204ccbab20c43782318263": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2e73acbc9f1c4ebcb3a1be8d6c23df9a",
              "IPY_MODEL_2b7bba0b5dfb4531b94d893c73c78e81",
              "IPY_MODEL_46ce169281ad4f788378abbb75182a78"
            ],
            "layout": "IPY_MODEL_148a727a895440de9fc194de1cc0db75"
          }
        },
        "2e73acbc9f1c4ebcb3a1be8d6c23df9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e30d5b4766324fba8c785a945da19682",
            "placeholder": "​",
            "style": "IPY_MODEL_c1c2c3b3f33644c998dcaff70887b3ec",
            "value": "Downloading: 100%"
          }
        },
        "2b7bba0b5dfb4531b94d893c73c78e81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10bf386daba64491a6f4b59ef5caf43f",
            "max": 1355863,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6cb149b31c0c4b50b1fb78584ed028a7",
            "value": 1355863
          }
        },
        "46ce169281ad4f788378abbb75182a78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_27e8f6f6872c4664a483f553807f074a",
            "placeholder": "​",
            "style": "IPY_MODEL_f505cb2974c8458ab0bf71c824938bb0",
            "value": " 1.36M/1.36M [00:00&lt;00:00, 2.79MB/s]"
          }
        },
        "148a727a895440de9fc194de1cc0db75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e30d5b4766324fba8c785a945da19682": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c1c2c3b3f33644c998dcaff70887b3ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10bf386daba64491a6f4b59ef5caf43f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6cb149b31c0c4b50b1fb78584ed028a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "27e8f6f6872c4664a483f553807f074a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f505cb2974c8458ab0bf71c824938bb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "072e1c44349d437483d40c767aee6e1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1f02583789f945d1a2f80bc6c2abcd30",
              "IPY_MODEL_1dd5001b0f464e6096ccf78078da4f3d",
              "IPY_MODEL_be9e1d76c7d0430db1cb1a7c5a802794"
            ],
            "layout": "IPY_MODEL_e5370ca40ccb4b85889aea500442aec7"
          }
        },
        "1f02583789f945d1a2f80bc6c2abcd30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c5e97e97e574dd1b061368beae65f6d",
            "placeholder": "​",
            "style": "IPY_MODEL_759c5a4a42ea4d4e950d1ac56b81a8ae",
            "value": "100%"
          }
        },
        "1dd5001b0f464e6096ccf78078da4f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c3b0575affbc45c6bfd8d4ccc6e2a21d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14e1e5f288ea474b858f7468df984bce",
            "value": 1
          }
        },
        "be9e1d76c7d0430db1cb1a7c5a802794": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9d6494b07df44e5b8ac3c34885eded0b",
            "placeholder": "​",
            "style": "IPY_MODEL_37e71a45d1af4639989bb4cf2039c992",
            "value": " 1/1 [00:00&lt;00:00, 15.35ba/s]"
          }
        },
        "e5370ca40ccb4b85889aea500442aec7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c5e97e97e574dd1b061368beae65f6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "759c5a4a42ea4d4e950d1ac56b81a8ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c3b0575affbc45c6bfd8d4ccc6e2a21d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14e1e5f288ea474b858f7468df984bce": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9d6494b07df44e5b8ac3c34885eded0b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "37e71a45d1af4639989bb4cf2039c992": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b9189252598429eac2c0499b3930494": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90ccc3447c2144c1826291d5f041a8c7",
              "IPY_MODEL_ead39028d87941e38820816a6bf968a3",
              "IPY_MODEL_85958f37cbb24980a4bad5f1cfe4744e"
            ],
            "layout": "IPY_MODEL_12ecf64ae6864036b6f52580c23596d2"
          }
        },
        "90ccc3447c2144c1826291d5f041a8c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc7182505e9e4f5e8761c9a803ced605",
            "placeholder": "​",
            "style": "IPY_MODEL_5673a8b38d7d4b4a91a80b5d0b66213c",
            "value": "Downloading: 100%"
          }
        },
        "ead39028d87941e38820816a6bf968a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e785b97dbd24951a38ff6d7498f1aef",
            "max": 597257159,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c0107c8245be463d962c9c76dcde4833",
            "value": 597257159
          }
        },
        "85958f37cbb24980a4bad5f1cfe4744e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d0e499f19a9c4f218fe410993f151dd1",
            "placeholder": "​",
            "style": "IPY_MODEL_d8d386340e2848b0bcede4148d338e06",
            "value": " 597M/597M [00:08&lt;00:00, 70.5MB/s]"
          }
        },
        "12ecf64ae6864036b6f52580c23596d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bc7182505e9e4f5e8761c9a803ced605": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5673a8b38d7d4b4a91a80b5d0b66213c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7e785b97dbd24951a38ff6d7498f1aef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c0107c8245be463d962c9c76dcde4833": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d0e499f19a9c4f218fe410993f151dd1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d8d386340e2848b0bcede4148d338e06": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}