{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "This notebook performs [Integrated Gradients](https://arxiv.org/abs/1703.01365) over the entire dataset and aggregates all of the attributions with respect to the positive class. We aggregate using either the complete longformer embeddings, or the word and position embeddings. The notebook outputs a csv file containg tokens and the sum of the attributions over the entire dataset and is used in the [longformer embedding](https://colab.research.google.com/drive/1BQh3H0nvMAtHD36xzKFpqTbByk_H0Cv9?usp=sharing)."
      ],
      "metadata": {
        "id": "kEJ0R0uy0l47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "b_HI-3J_1Gfg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43d14000-e734-4bf4-bacd-14ac03ae49fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import dependencies"
      ],
      "metadata": {
        "id": "HkZ5bD2_z-0C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/{}'.format(\"cogs402longformer/\"))"
      ],
      "metadata": {
        "id": "Jck92aCS1c3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers --quiet"
      ],
      "metadata": {
        "id": "zMjzQIFJ2P_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install captum --quiet"
      ],
      "metadata": {
        "id": "Uno0qwr12UTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets --quiet"
      ],
      "metadata": {
        "id": "OaOSPMJE3ONc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""
      ],
      "metadata": {
        "id": "hRSNYTrRIPkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrRRZJ6-0_Il"
      },
      "outputs": [],
      "source": [
        "from captum.attr import visualization as viz\n",
        "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\n",
        "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
        "\n",
        "import torch\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import model"
      ],
      "metadata": {
        "id": "USHRv2j70Fb4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b5V31lsc0_Il"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nzBfB-v0_Im",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d48f1700-d7fc-4fb3-b9e1-fc959b6a6e8d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['longformer_model.encoder.layer.0.output.LayerNorm.bias', 'longformer_model.encoder.layer.9.output.dense.bias', 'longformer_model.embeddings.position_embeddings.weight', 'dense.weight', 'longformer_model.encoder.layer.2.attention.self.value.bias', 'longformer_model.encoder.layer.4.attention.self.key_global.weight', 'longformer_model.encoder.layer.4.attention.output.dense.bias', 'longformer_model.encoder.layer.10.attention.self.key.bias', 'longformer_model.encoder.layer.4.attention.self.key_global.bias', 'longformer_model.embeddings.token_type_embeddings.weight', 'longformer_model.encoder.layer.3.attention.self.query.weight', 'longformer_model.encoder.layer.7.attention.self.key_global.weight', 'longformer_model.encoder.layer.0.attention.self.query.bias', 'longformer_model.encoder.layer.7.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.10.output.LayerNorm.weight', 'longformer_model.encoder.layer.1.output.dense.weight', 'longformer_model.encoder.layer.3.attention.self.query_global.bias', 'longformer_model.encoder.layer.6.intermediate.dense.weight', 'longformer_model.encoder.layer.0.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.8.output.LayerNorm.weight', 'longformer_model.encoder.layer.1.attention.self.query_global.weight', 'longformer_model.encoder.layer.1.attention.output.dense.weight', 'longformer_model.encoder.layer.6.attention.self.value_global.weight', 'longformer_model.encoder.layer.9.attention.self.query.weight', 'longformer_model.embeddings.word_embeddings.weight', 'longformer_model.encoder.layer.1.attention.self.key_global.bias', 'longformer_model.encoder.layer.5.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.11.attention.self.value_global.weight', 'longformer_model.encoder.layer.6.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.self.value.bias', 'longformer_model.embeddings.LayerNorm.bias', 'longformer_model.encoder.layer.6.intermediate.dense.bias', 'longformer_model.encoder.layer.9.attention.self.value_global.weight', 'longformer_model.encoder.layer.1.attention.self.value.weight', 'longformer_model.encoder.layer.2.attention.self.query.bias', 'longformer_model.encoder.layer.9.attention.self.key.bias', 'longformer_model.encoder.layer.5.attention.self.key_global.bias', 'longformer_model.encoder.layer.10.attention.self.value.weight', 'longformer_model.encoder.layer.5.attention.self.key.bias', 'longformer_model.encoder.layer.8.attention.self.value_global.bias', 'longformer_model.encoder.layer.11.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.1.attention.self.query_global.bias', 'longformer_model.encoder.layer.2.attention.self.value.weight', 'longformer_model.encoder.layer.2.attention.self.key_global.weight', 'longformer_model.encoder.layer.2.attention.self.value_global.bias', 'longformer_model.encoder.layer.3.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.11.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.11.attention.self.value.bias', 'longformer_model.encoder.layer.2.output.LayerNorm.weight', 'longformer_model.encoder.layer.2.attention.output.dense.weight', 'longformer_model.encoder.layer.11.output.LayerNorm.bias', 'longformer_model.encoder.layer.8.attention.self.query.bias', 'longformer_model.encoder.layer.11.attention.self.value_global.bias', 'longformer_model.encoder.layer.4.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.output.dense.bias', 'longformer_model.encoder.layer.9.intermediate.dense.weight', 'longformer_model.encoder.layer.9.attention.self.key_global.weight', 'longformer_model.encoder.layer.9.output.dense.weight', 'longformer_model.encoder.layer.4.intermediate.dense.weight', 'longformer_model.encoder.layer.7.output.LayerNorm.weight', 'longformer_model.encoder.layer.2.attention.self.query_global.bias', 'longformer_model.encoder.layer.3.output.LayerNorm.weight', 'longformer_model.encoder.layer.9.attention.self.key.weight', 'longformer_model.encoder.layer.3.attention.self.key.bias', 'longformer_model.encoder.layer.11.attention.self.value.weight', 'longformer_model.encoder.layer.6.attention.self.key_global.bias', 'longformer_model.encoder.layer.0.attention.self.key.weight', 'longformer_model.encoder.layer.10.attention.self.value.bias', 'longformer_model.encoder.layer.3.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.10.attention.self.query_global.weight', 'longformer_model.encoder.layer.2.attention.self.key.weight', 'longformer_model.encoder.layer.9.output.LayerNorm.weight', 'longformer_model.encoder.layer.4.attention.self.value_global.weight', 'longformer_model.encoder.layer.4.output.dense.weight', 'longformer_model.encoder.layer.3.attention.self.value.bias', 'longformer_model.encoder.layer.9.attention.self.value.weight', 'longformer_model.encoder.layer.11.attention.self.query.bias', 'longformer_model.encoder.layer.7.attention.self.key.bias', 'longformer_model.encoder.layer.5.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.8.output.dense.bias', 'longformer_model.encoder.layer.6.attention.self.key.weight', 'longformer_model.encoder.layer.4.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.3.attention.self.query_global.weight', 'longformer_model.encoder.layer.2.attention.self.value_global.weight', 'longformer_model.encoder.layer.8.attention.self.key.weight', 'longformer_model.encoder.layer.1.intermediate.dense.bias', 'longformer_model.encoder.layer.6.attention.self.key_global.weight', 'longformer_model.encoder.layer.0.attention.self.value.bias', 'longformer_model.encoder.layer.4.attention.output.dense.weight', 'longformer_model.encoder.layer.8.attention.self.key.bias', 'longformer_model.encoder.layer.9.attention.output.dense.weight', 'longformer_model.encoder.layer.3.intermediate.dense.weight', 'longformer_model.encoder.layer.8.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.1.attention.self.key.weight', 'longformer_model.encoder.layer.3.attention.self.value.weight', 'longformer_model.encoder.layer.10.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.2.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.self.key.weight', 'longformer_model.encoder.layer.8.output.dense.weight', 'longformer_model.encoder.layer.6.output.LayerNorm.bias', 'longformer_model.encoder.layer.6.attention.self.key.bias', 'longformer_model.encoder.layer.7.attention.self.key_global.bias', 'longformer_model.encoder.layer.1.output.LayerNorm.bias', 'longformer_model.embeddings.LayerNorm.weight', 'longformer_model.encoder.layer.7.output.LayerNorm.bias', 'longformer_model.encoder.layer.6.attention.self.query_global.weight', 'longformer_model.encoder.layer.10.attention.self.key_global.bias', 'longformer_model.encoder.layer.9.attention.self.value.bias', 'longformer_model.encoder.layer.0.output.dense.bias', 'longformer_model.encoder.layer.4.output.LayerNorm.weight', 'longformer_model.encoder.layer.7.attention.self.query.bias', 'longformer_model.encoder.layer.7.attention.self.value_global.weight', 'longformer_model.encoder.layer.5.attention.output.dense.weight', 'longformer_model.encoder.layer.2.attention.self.query_global.weight', 'longformer_model.encoder.layer.8.attention.self.value_global.weight', 'longformer_model.encoder.layer.4.intermediate.dense.bias', 'longformer_model.encoder.layer.5.attention.self.value_global.bias', 'longformer_model.encoder.layer.3.attention.self.value_global.weight', 'longformer_model.encoder.layer.6.attention.output.dense.bias', 'longformer_model.encoder.layer.0.attention.self.value.weight', 'longformer_model.encoder.layer.4.output.dense.bias', 'longformer_model.encoder.layer.6.output.dense.weight', 'longformer_model.encoder.layer.7.attention.self.query_global.weight', 'longformer_model.encoder.layer.11.attention.self.query.weight', 'longformer_model.encoder.layer.8.attention.self.key_global.bias', 'longformer_model.encoder.layer.9.attention.self.query.bias', 'longformer_model.encoder.layer.2.attention.self.key.bias', 'longformer_model.encoder.layer.11.attention.self.query_global.bias', 'longformer_model.encoder.layer.2.attention.output.dense.bias', 'longformer_model.encoder.layer.3.attention.self.key_global.bias', 'longformer_model.encoder.layer.0.output.dense.weight', 'longformer_model.encoder.layer.7.attention.self.key.weight', 'longformer_model.encoder.layer.11.output.LayerNorm.weight', 'longformer_model.encoder.layer.4.attention.self.query_global.weight', 'longformer_model.encoder.layer.5.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.output.dense.bias', 'longformer_model.encoder.layer.6.attention.self.query.weight', 'longformer_model.encoder.layer.11.attention.output.dense.bias', 'longformer_model.encoder.layer.0.intermediate.dense.weight', 'longformer_model.encoder.layer.6.attention.self.value.weight', 'longformer_model.encoder.layer.3.attention.output.dense.weight', 'longformer_model.encoder.layer.1.attention.self.query.bias', 'longformer_model.encoder.layer.9.attention.self.query_global.bias', 'longformer_model.encoder.layer.1.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.8.intermediate.dense.bias', 'longformer_model.encoder.layer.10.intermediate.dense.bias', 'longformer_model.encoder.layer.4.attention.self.query_global.bias', 'longformer_model.encoder.layer.9.attention.self.query_global.weight', 'longformer_model.encoder.layer.1.output.dense.bias', 'longformer_model.encoder.layer.10.attention.self.key.weight', 'longformer_model.encoder.layer.3.attention.self.value_global.bias', 'longformer_model.encoder.layer.10.attention.self.value_global.bias', 'longformer_model.encoder.layer.3.attention.output.dense.bias', 'longformer_model.encoder.layer.8.attention.self.key_global.weight', 'longformer_model.encoder.layer.4.attention.self.value.bias', 'longformer_model.encoder.layer.4.attention.self.key.bias', 'longformer_model.encoder.layer.4.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.self.query_global.bias', 'longformer_model.encoder.layer.7.output.dense.bias', 'dense.bias', 'longformer_model.encoder.layer.10.attention.self.query.weight', 'longformer_model.encoder.layer.2.intermediate.dense.bias', 'longformer_model.encoder.layer.8.attention.self.query_global.bias', 'longformer_model.encoder.layer.8.attention.self.value.weight', 'longformer_model.encoder.layer.0.attention.self.query.weight', 'longformer_model.encoder.layer.10.attention.output.dense.bias', 'longformer_model.encoder.layer.0.attention.self.query_global.weight', 'longformer_model.encoder.layer.1.attention.output.dense.bias', 'longformer_model.encoder.layer.7.intermediate.dense.bias', 'longformer_model.encoder.layer.5.attention.self.value.weight', 'longformer_model.encoder.layer.3.attention.self.query.bias', 'longformer_model.encoder.layer.7.attention.output.dense.weight', 'longformer_model.encoder.layer.10.output.LayerNorm.bias', 'longformer_model.encoder.layer.3.output.LayerNorm.bias', 'longformer_model.encoder.layer.8.intermediate.dense.weight', 'longformer_model.encoder.layer.1.attention.self.query.weight', 'longformer_model.encoder.layer.2.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.2.attention.self.key_global.bias', 'longformer_model.encoder.layer.6.output.LayerNorm.weight', 'longformer_model.encoder.layer.11.output.dense.weight', 'longformer_model.encoder.layer.10.output.dense.weight', 'longformer_model.encoder.layer.9.attention.self.key_global.bias', 'longformer_model.encoder.layer.5.output.LayerNorm.weight', 'longformer_model.encoder.layer.11.output.dense.bias', 'longformer_model.encoder.layer.10.attention.self.query_global.bias', 'longformer_model.encoder.layer.10.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.0.attention.output.dense.bias', 'longformer_model.encoder.layer.5.attention.self.query_global.weight', 'longformer_model.encoder.layer.1.intermediate.dense.weight', 'longformer_model.encoder.layer.7.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.1.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.7.intermediate.dense.weight', 'longformer_model.encoder.layer.10.output.dense.bias', 'longformer_model.encoder.layer.6.attention.self.query.bias', 'fc.weight', 'fc.bias', 'longformer_model.encoder.layer.5.intermediate.dense.bias', 'longformer_model.encoder.layer.11.attention.output.dense.weight', 'longformer_model.encoder.layer.10.intermediate.dense.weight', 'longformer_model.encoder.layer.8.attention.self.query.weight', 'longformer_model.encoder.layer.11.attention.self.query_global.weight', 'longformer_model.encoder.layer.0.output.LayerNorm.weight', 'longformer_model.encoder.layer.2.output.dense.bias', 'longformer_model.encoder.layer.4.attention.self.query.bias', 'longformer_model.encoder.layer.7.attention.self.value.weight', 'longformer_model.encoder.layer.9.attention.output.LayerNorm.bias', 'longformer_model.embeddings.position_ids', 'longformer_model.encoder.layer.3.intermediate.dense.bias', 'longformer_model.encoder.layer.8.attention.self.value.bias', 'longformer_model.encoder.layer.0.attention.self.key_global.weight', 'longformer_model.encoder.layer.6.attention.self.value_global.bias', 'longformer_model.encoder.layer.11.attention.self.key_global.weight', 'longformer_model.encoder.layer.3.attention.self.key_global.weight', 'longformer_model.encoder.layer.7.attention.self.query_global.bias', 'longformer_model.encoder.layer.0.attention.self.key.bias', 'longformer_model.encoder.layer.2.output.LayerNorm.bias', 'longformer_model.encoder.layer.9.attention.output.dense.bias', 'longformer_model.encoder.layer.7.attention.self.value.bias', 'longformer_model.encoder.layer.6.output.dense.bias', 'longformer_model.encoder.layer.8.attention.self.query_global.weight', 'longformer_model.encoder.layer.1.output.LayerNorm.weight', 'longformer_model.encoder.layer.5.attention.self.key_global.weight', 'longformer_model.encoder.layer.2.intermediate.dense.weight', 'longformer_model.encoder.layer.9.attention.self.value_global.bias', 'longformer_model.encoder.layer.11.attention.self.key.weight', 'longformer_model.encoder.layer.4.attention.self.value.weight', 'longformer_model.encoder.layer.11.attention.self.key_global.bias', 'longformer_model.encoder.layer.0.attention.self.value_global.bias', 'longformer_model.encoder.layer.4.attention.self.key.weight', 'longformer_model.encoder.layer.1.attention.self.key_global.weight', 'longformer_model.encoder.layer.4.attention.self.value_global.bias', 'longformer_model.encoder.layer.8.output.LayerNorm.bias', 'longformer_model.encoder.layer.3.attention.self.key.weight', 'longformer_model.encoder.layer.5.attention.self.query.bias', 'longformer_model.encoder.layer.5.intermediate.dense.weight', 'longformer_model.encoder.layer.7.output.dense.weight', 'longformer_model.encoder.layer.4.attention.self.query.weight', 'longformer_model.encoder.layer.7.attention.self.value_global.bias', 'longformer_model.encoder.layer.9.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.10.attention.self.query.bias', 'longformer_model.encoder.layer.10.attention.self.key_global.weight', 'longformer_model.encoder.layer.0.attention.self.query_global.bias', 'longformer_model.encoder.layer.10.attention.self.value_global.weight', 'longformer_model.encoder.layer.0.attention.self.key_global.bias', 'longformer_model.encoder.layer.6.attention.self.value.bias', 'longformer_model.encoder.layer.2.attention.self.query.weight', 'longformer_model.encoder.layer.5.output.dense.weight', 'longformer_model.encoder.layer.3.output.dense.weight', 'longformer_model.encoder.layer.0.attention.output.dense.weight', 'longformer_model.encoder.layer.0.intermediate.dense.bias', 'longformer_model.encoder.layer.1.attention.self.value.bias', 'longformer_model.encoder.layer.1.attention.self.value_global.bias', 'longformer_model.encoder.layer.1.attention.self.value_global.weight', 'longformer_model.encoder.layer.1.attention.self.key.bias', 'longformer_model.encoder.layer.5.attention.self.query.weight', 'longformer_model.encoder.layer.6.attention.output.dense.weight', 'longformer_model.encoder.layer.7.attention.output.dense.bias', 'longformer_model.encoder.layer.8.attention.output.dense.weight', 'longformer_model.encoder.layer.7.attention.self.query.weight', 'longformer_model.encoder.layer.2.output.dense.weight', 'longformer_model.encoder.layer.8.attention.output.dense.bias', 'longformer_model.encoder.layer.6.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.0.attention.self.value_global.weight', 'longformer_model.encoder.layer.11.attention.self.key.bias', 'longformer_model.encoder.layer.11.intermediate.dense.weight', 'longformer_model.encoder.layer.10.attention.output.dense.weight', 'longformer_model.encoder.layer.0.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.11.intermediate.dense.bias', 'longformer_model.encoder.layer.5.attention.self.value_global.weight', 'longformer_model.encoder.layer.8.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.3.output.dense.bias', 'longformer_model.encoder.layer.9.intermediate.dense.bias', 'longformer_model.encoder.layer.9.output.LayerNorm.bias', 'longformer_model.encoder.layer.6.attention.self.query_global.bias']\n",
            "- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['longformer.encoder.layer.8.attention.self.query.weight', 'longformer.encoder.layer.4.attention.self.query_global.bias', 'longformer.encoder.layer.5.attention.self.key.weight', 'longformer.encoder.layer.4.attention.self.query.weight', 'longformer.encoder.layer.0.attention.output.dense.bias', 'longformer.encoder.layer.11.attention.self.query.bias', 'longformer.encoder.layer.3.attention.self.query.bias', 'longformer.encoder.layer.10.attention.self.key.bias', 'longformer.encoder.layer.0.attention.self.value_global.weight', 'longformer.encoder.layer.5.attention.self.value.weight', 'longformer.encoder.layer.7.attention.self.key.bias', 'longformer.encoder.layer.9.output.dense.weight', 'longformer.encoder.layer.9.output.LayerNorm.bias', 'longformer.encoder.layer.7.output.LayerNorm.bias', 'longformer.encoder.layer.4.attention.output.dense.bias', 'longformer.encoder.layer.9.attention.self.value.bias', 'longformer.encoder.layer.3.attention.output.LayerNorm.weight', 'longformer.encoder.layer.3.output.dense.weight', 'longformer.encoder.layer.10.attention.output.LayerNorm.weight', 'longformer.encoder.layer.0.intermediate.dense.weight', 'longformer.encoder.layer.4.output.LayerNorm.bias', 'longformer.encoder.layer.1.intermediate.dense.weight', 'longformer.encoder.layer.1.output.dense.bias', 'longformer.encoder.layer.2.attention.self.value_global.bias', 'longformer.encoder.layer.3.attention.self.key_global.weight', 'longformer.embeddings.LayerNorm.bias', 'longformer.encoder.layer.9.attention.output.LayerNorm.bias', 'longformer.encoder.layer.9.attention.self.key.weight', 'longformer.encoder.layer.4.attention.output.dense.weight', 'longformer.encoder.layer.0.attention.output.LayerNorm.weight', 'longformer.encoder.layer.3.attention.self.key_global.bias', 'longformer.encoder.layer.6.attention.output.dense.bias', 'longformer.encoder.layer.9.attention.self.key_global.weight', 'longformer.encoder.layer.3.attention.output.LayerNorm.bias', 'longformer.encoder.layer.8.attention.output.LayerNorm.bias', 'longformer.encoder.layer.0.output.LayerNorm.weight', 'longformer.encoder.layer.7.attention.self.query.bias', 'longformer.encoder.layer.2.attention.self.query_global.weight', 'longformer.encoder.layer.9.output.LayerNorm.weight', 'longformer.encoder.layer.3.intermediate.dense.weight', 'longformer.embeddings.LayerNorm.weight', 'longformer.encoder.layer.2.attention.self.value.bias', 'longformer.encoder.layer.2.output.LayerNorm.weight', 'longformer.encoder.layer.2.attention.self.key.bias', 'longformer.encoder.layer.6.output.dense.bias', 'longformer.encoder.layer.0.attention.self.query_global.bias', 'longformer.encoder.layer.11.attention.output.dense.weight', 'longformer.encoder.layer.8.output.dense.bias', 'longformer.encoder.layer.4.attention.self.key.weight', 'longformer.encoder.layer.7.intermediate.dense.weight', 'longformer.encoder.layer.4.attention.self.key.bias', 'longformer.encoder.layer.4.intermediate.dense.bias', 'longformer.encoder.layer.9.attention.self.key_global.bias', 'longformer.encoder.layer.1.output.LayerNorm.weight', 'longformer.encoder.layer.10.attention.self.query_global.bias', 'longformer.encoder.layer.10.output.LayerNorm.weight', 'longformer.encoder.layer.7.attention.self.key_global.weight', 'longformer.encoder.layer.6.output.LayerNorm.bias', 'longformer.encoder.layer.0.attention.output.dense.weight', 'longformer.encoder.layer.6.attention.self.query.bias', 'longformer.encoder.layer.10.output.dense.weight', 'longformer.encoder.layer.3.attention.self.query_global.bias', 'longformer.encoder.layer.6.attention.self.query.weight', 'longformer.encoder.layer.10.output.LayerNorm.bias', 'longformer.encoder.layer.0.output.dense.bias', 'longformer.encoder.layer.1.attention.output.LayerNorm.bias', 'longformer.encoder.layer.5.attention.output.LayerNorm.weight', 'longformer.encoder.layer.1.attention.self.key_global.weight', 'longformer.encoder.layer.6.attention.self.key_global.weight', 'longformer.encoder.layer.10.output.dense.bias', 'longformer.encoder.layer.8.intermediate.dense.bias', 'longformer.encoder.layer.11.output.LayerNorm.weight', 'longformer.encoder.layer.4.attention.self.value.weight', 'longformer.encoder.layer.0.attention.self.query.bias', 'longformer.encoder.layer.3.attention.self.query.weight', 'longformer.encoder.layer.5.attention.self.key_global.bias', 'longformer.encoder.layer.6.attention.self.key_global.bias', 'longformer.encoder.layer.4.attention.self.key_global.weight', 'longformer.encoder.layer.7.attention.self.query_global.weight', 'longformer.encoder.layer.6.output.LayerNorm.weight', 'longformer.encoder.layer.3.attention.self.value.bias', 'longformer.encoder.layer.8.attention.self.query.bias', 'longformer.encoder.layer.8.output.LayerNorm.weight', 'longformer.encoder.layer.2.attention.output.LayerNorm.weight', 'longformer.encoder.layer.2.intermediate.dense.bias', 'longformer.encoder.layer.6.attention.output.LayerNorm.bias', 'longformer.encoder.layer.7.attention.self.key.weight', 'longformer.encoder.layer.5.attention.self.value_global.weight', 'longformer.encoder.layer.7.attention.self.key_global.bias', 'longformer.encoder.layer.7.attention.output.dense.weight', 'longformer.encoder.layer.3.output.LayerNorm.weight', 'longformer.encoder.layer.10.attention.self.query.weight', 'longformer.encoder.layer.0.attention.self.key_global.bias', 'longformer.encoder.layer.2.attention.self.value_global.weight', 'longformer.encoder.layer.8.attention.output.LayerNorm.weight', 'longformer.encoder.layer.2.output.dense.weight', 'longformer.encoder.layer.10.intermediate.dense.bias', 'longformer.encoder.layer.7.attention.output.LayerNorm.bias', 'longformer.encoder.layer.4.intermediate.dense.weight', 'longformer.encoder.layer.11.attention.self.query_global.weight', 'longformer.encoder.layer.1.attention.self.query_global.bias', 'longformer.encoder.layer.6.attention.self.value_global.bias', 'longformer.encoder.layer.8.attention.self.query_global.weight', 'longformer.encoder.layer.6.attention.self.query_global.bias', 'longformer.encoder.layer.0.attention.self.value_global.bias', 'classifier.out_proj.bias', 'longformer.encoder.layer.9.attention.output.dense.weight', 'longformer.encoder.layer.3.attention.self.key.weight', 'longformer.encoder.layer.6.attention.output.LayerNorm.weight', 'longformer.encoder.layer.2.attention.self.query_global.bias', 'longformer.encoder.layer.4.attention.self.value_global.weight', 'longformer.encoder.layer.7.output.dense.weight', 'longformer.encoder.layer.8.attention.self.key_global.bias', 'longformer.encoder.layer.10.attention.self.value.bias', 'longformer.encoder.layer.8.attention.self.value.bias', 'longformer.encoder.layer.8.attention.self.query_global.bias', 'longformer.encoder.layer.2.attention.self.key_global.weight', 'longformer.encoder.layer.6.output.dense.weight', 'longformer.encoder.layer.10.attention.self.value_global.weight', 'longformer.embeddings.word_embeddings.weight', 'longformer.encoder.layer.11.attention.self.value_global.weight', 'longformer.encoder.layer.1.attention.self.query_global.weight', 'longformer.encoder.layer.7.attention.self.value.weight', 'longformer.encoder.layer.9.intermediate.dense.weight', 'longformer.encoder.layer.1.attention.self.key.bias', 'longformer.encoder.layer.5.output.LayerNorm.weight', 'longformer.encoder.layer.3.attention.self.value.weight', 'longformer.encoder.layer.5.attention.self.query.bias', 'longformer.encoder.layer.1.attention.output.dense.weight', 'longformer.encoder.layer.4.attention.self.query_global.weight', 'longformer.encoder.layer.1.attention.output.dense.bias', 'longformer.encoder.layer.11.output.dense.bias', 'longformer.encoder.layer.3.attention.self.value_global.weight', 'longformer.encoder.layer.10.intermediate.dense.weight', 'longformer.encoder.layer.11.attention.self.key.bias', 'longformer.encoder.layer.0.attention.self.value.bias', 'longformer.encoder.layer.6.attention.self.value_global.weight', 'longformer.encoder.layer.6.attention.output.dense.weight', 'longformer.encoder.layer.8.attention.self.value.weight', 'longformer.encoder.layer.8.attention.self.key.weight', 'longformer.encoder.layer.2.attention.output.LayerNorm.bias', 'longformer.encoder.layer.10.attention.self.key.weight', 'longformer.encoder.layer.5.attention.self.query.weight', 'longformer.encoder.layer.5.intermediate.dense.bias', 'longformer.encoder.layer.0.attention.self.key.weight', 'longformer.encoder.layer.11.attention.self.query_global.bias', 'longformer.encoder.layer.11.output.dense.weight', 'longformer.encoder.layer.1.output.LayerNorm.bias', 'longformer.encoder.layer.5.attention.self.query_global.weight', 'longformer.encoder.layer.6.attention.self.value.weight', 'longformer.encoder.layer.5.attention.output.LayerNorm.bias', 'longformer.encoder.layer.9.attention.self.key.bias', 'longformer.encoder.layer.8.attention.output.dense.bias', 'longformer.encoder.layer.1.attention.output.LayerNorm.weight', 'longformer.encoder.layer.6.attention.self.query_global.weight', 'longformer.encoder.layer.7.output.dense.bias', 'longformer.encoder.layer.7.attention.self.value.bias', 'longformer.encoder.layer.2.attention.self.query.weight', 'longformer.encoder.layer.10.attention.self.query_global.weight', 'longformer.encoder.layer.1.attention.self.value_global.bias', 'longformer.encoder.layer.4.attention.output.LayerNorm.weight', 'longformer.encoder.layer.7.attention.self.value_global.bias', 'longformer.encoder.layer.11.attention.self.query.weight', 'longformer.encoder.layer.9.attention.self.query.bias', 'longformer.encoder.layer.4.attention.output.LayerNorm.bias', 'longformer.encoder.layer.8.attention.self.key_global.weight', 'longformer.encoder.layer.10.attention.self.value.weight', 'longformer.encoder.layer.2.output.dense.bias', 'longformer.encoder.layer.10.attention.output.dense.weight', 'longformer.encoder.layer.11.attention.self.key.weight', 'longformer.encoder.layer.7.attention.self.value_global.weight', 'longformer.encoder.layer.0.attention.self.value.weight', 'longformer.encoder.layer.2.attention.output.dense.bias', 'longformer.encoder.layer.1.attention.self.key.weight', 'longformer.encoder.layer.1.attention.self.value_global.weight', 'longformer.encoder.layer.9.output.dense.bias', 'longformer.encoder.layer.11.attention.self.value.bias', 'longformer.encoder.layer.5.intermediate.dense.weight', 'longformer.encoder.layer.1.attention.self.value.weight', 'longformer.encoder.layer.7.intermediate.dense.bias', 'longformer.encoder.layer.5.attention.self.key.bias', 'longformer.encoder.layer.0.output.dense.weight', 'longformer.encoder.layer.6.attention.self.value.bias', 'longformer.encoder.layer.11.attention.self.value.weight', 'longformer.encoder.layer.11.intermediate.dense.weight', 'longformer.encoder.layer.11.attention.output.LayerNorm.bias', 'longformer.encoder.layer.1.attention.self.query.weight', 'longformer.encoder.layer.11.output.LayerNorm.bias', 'longformer.encoder.layer.1.intermediate.dense.bias', 'longformer.embeddings.position_embeddings.weight', 'longformer.encoder.layer.9.attention.output.dense.bias', 'longformer.encoder.layer.11.attention.output.dense.bias', 'longformer.encoder.layer.2.attention.self.key.weight', 'longformer.encoder.layer.6.intermediate.dense.bias', 'longformer.encoder.layer.0.output.LayerNorm.bias', 'longformer.encoder.layer.9.attention.self.value_global.weight', 'longformer.encoder.layer.11.attention.self.value_global.bias', 'longformer.encoder.layer.3.output.LayerNorm.bias', 'longformer.encoder.layer.11.intermediate.dense.bias', 'longformer.encoder.layer.8.output.dense.weight', 'longformer.encoder.layer.6.attention.self.key.weight', 'longformer.encoder.layer.1.attention.self.key_global.bias', 'longformer.encoder.layer.7.attention.self.query_global.bias', 'longformer.encoder.layer.9.attention.output.LayerNorm.weight', 'longformer.encoder.layer.11.attention.self.key_global.bias', 'longformer.encoder.layer.9.intermediate.dense.bias', 'longformer.encoder.layer.4.attention.self.key_global.bias', 'classifier.out_proj.weight', 'longformer.encoder.layer.11.attention.output.LayerNorm.weight', 'longformer.encoder.layer.7.output.LayerNorm.weight', 'longformer.encoder.layer.2.attention.output.dense.weight', 'longformer.encoder.layer.5.attention.output.dense.bias', 'longformer.encoder.layer.5.output.dense.weight', 'longformer.encoder.layer.5.attention.self.key_global.weight', 'longformer.encoder.layer.9.attention.self.query_global.bias', 'longformer.encoder.layer.9.attention.self.value_global.bias', 'longformer.encoder.layer.10.attention.self.key_global.bias', 'longformer.encoder.layer.8.attention.output.dense.weight', 'longformer.encoder.layer.0.attention.self.query_global.weight', 'longformer.encoder.layer.0.attention.output.LayerNorm.bias', 'longformer.encoder.layer.1.attention.self.value.bias', 'longformer.encoder.layer.2.attention.self.query.bias', 'longformer.encoder.layer.2.attention.self.key_global.bias', 'longformer.encoder.layer.3.attention.self.key.bias', 'longformer.encoder.layer.10.attention.self.value_global.bias', 'longformer.encoder.layer.0.attention.self.key.bias', 'longformer.encoder.layer.2.attention.self.value.weight', 'longformer.encoder.layer.4.attention.self.value_global.bias', 'longformer.encoder.layer.10.attention.self.key_global.weight', 'longformer.encoder.layer.9.attention.self.value.weight', 'longformer.encoder.layer.2.intermediate.dense.weight', 'longformer.encoder.layer.5.output.dense.bias', 'longformer.encoder.layer.5.output.LayerNorm.bias', 'longformer.encoder.layer.0.intermediate.dense.bias', 'longformer.encoder.layer.2.output.LayerNorm.bias', 'longformer.encoder.layer.6.intermediate.dense.weight', 'longformer.encoder.layer.10.attention.output.LayerNorm.bias', 'longformer.encoder.layer.9.attention.self.query_global.weight', 'longformer.encoder.layer.0.attention.self.query.weight', 'longformer.encoder.layer.4.attention.self.value.bias', 'longformer.encoder.layer.5.attention.output.dense.weight', 'longformer.encoder.layer.8.output.LayerNorm.bias', 'longformer.encoder.layer.4.attention.self.query.bias', 'longformer.encoder.layer.3.attention.output.dense.weight', 'longformer.encoder.layer.9.attention.self.query.weight', 'longformer.encoder.layer.1.attention.self.query.bias', 'longformer.encoder.layer.5.attention.self.value.bias', 'longformer.encoder.layer.3.attention.self.query_global.weight', 'longformer.embeddings.token_type_embeddings.weight', 'longformer.encoder.layer.3.intermediate.dense.bias', 'longformer.encoder.layer.5.attention.self.value_global.bias', 'longformer.encoder.layer.0.attention.self.key_global.weight', 'longformer.encoder.layer.7.attention.output.LayerNorm.weight', 'longformer.encoder.layer.8.attention.self.value_global.weight', 'longformer.encoder.layer.8.attention.self.value_global.bias', 'longformer.encoder.layer.8.attention.self.key.bias', 'longformer.encoder.layer.7.attention.output.dense.bias', 'longformer.encoder.layer.3.attention.output.dense.bias', 'longformer.encoder.layer.4.output.dense.weight', 'longformer.encoder.layer.10.attention.self.query.bias', 'longformer.encoder.layer.7.attention.self.query.weight', 'longformer.encoder.layer.3.attention.self.value_global.bias', 'longformer.encoder.layer.1.output.dense.weight', 'longformer.encoder.layer.4.output.dense.bias', 'longformer.encoder.layer.4.output.LayerNorm.weight', 'classifier.dense.weight', 'longformer.encoder.layer.8.intermediate.dense.weight', 'longformer.encoder.layer.11.attention.self.key_global.weight', 'classifier.dense.bias', 'longformer.encoder.layer.5.attention.self.query_global.bias', 'longformer.encoder.layer.10.attention.output.dense.bias', 'longformer.encoder.layer.3.output.dense.bias', 'longformer.encoder.layer.6.attention.self.key.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from transformers import LongformerForSequenceClassification, LongformerTokenizer, LongformerConfig\n",
        "# replace <PATH-TO-SAVED-MODEL> with the real path of the saved model\n",
        "model_path = 'danielhou13/longformer-finetuned_papers_v2'\n",
        "# model_path = 'danielhou13/longformer-finetuned-news-cogs402'\n",
        "\n",
        "# load model\n",
        "test = torch.load(\"/content/drive/MyDrive/cogs402longformer/fakeclinicalnotes/models/full_augmented_lr2e-5_dropout3_10_trained_threshold.pt\")\n",
        "model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', state_dict=test['state_dict'], num_labels = 2)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "model.zero_grad()\n",
        "\n",
        "# load tokenizer\n",
        "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import Dataset"
      ],
      "metadata": {
        "id": "BdY6GsO00RG_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we import the notes dataset"
      ],
      "metadata": {
        "id": "e_gGIPpwkmbH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6aQM9nM0_Ip",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 84,
          "referenced_widgets": [
            "b5b3538ee8ae496986e088fa81bc3e99",
            "1b7dd46ac519464dbf630fcf89743649",
            "67082ad5672043079286993881ce7af1",
            "0dc4688cd6ff4fc8ad3f9b5d2eaeb01c",
            "976471339e40487e8a4de3b39b4036c4",
            "f05d59b07db2415da87cc48f7edf0dda",
            "be8290071b7f409e82c1a88aed4e00fb",
            "c6d144feb2c845d2aaa66348684ed541",
            "bfd5949f6929455694b80ea3260c7798",
            "43eda6eb9ef044f1af44377080a5f3ea",
            "d35603213ab8444f91b731dd4050d2f8"
          ]
        },
        "outputId": "bff90214-fe58-44bb-db74-cae77b4afd7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:datasets.builder:Using custom data configuration danielhou13--cogs402datafake-c20c2db2d92a66bc\n",
            "WARNING:datasets.builder:Reusing dataset parquet (/root/.cache/huggingface/datasets/danielhou13___parquet/danielhou13--cogs402datafake-c20c2db2d92a66bc/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b5b3538ee8ae496986e088fa81bc3e99"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "import numpy as np\n",
        "# cogs402_ds = load_dataset(\"danielhou13/cogs402datafake\")[\"train\"]\n",
        "\n",
        "ds = pd.read_csv(\"/content/drive/MyDrive/cogs402longformer/fakeclinicalnotes/data/fake_notes.csv\")\n",
        "dataset = datasets.Dataset.from_pandas(ds)\n",
        "cogs402_ds = dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we import the news dataset"
      ],
      "metadata": {
        "id": "oLMmOW7Lm40A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cogs402_ds = load_dataset(\"danielhou13/cogs402dataset2\")[\"validation\"]"
      ],
      "metadata": {
        "id": "58zzpZYRWRDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Getting the Attributions"
      ],
      "metadata": {
        "id": "-IMLrVRDP3CP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For our Integrated Gradients, we need to create a custom forward pass of our model. Specifically we want the softmaxed logits which represent the probability of predicting that class."
      ],
      "metadata": {
        "id": "uX2TF2ayP6X_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5-heo2y0_Im"
      },
      "outputs": [],
      "source": [
        "def predict(inputs, position_ids=None, token_type_ids=None, attention_mask=None):\n",
        "    output = model(inputs,\n",
        "                   position_ids=position_ids,\n",
        "                   token_type_ids=token_type_ids,\n",
        "                   attention_mask=attention_mask)\n",
        "    return output.logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "axwpHq-y0_Io"
      },
      "outputs": [],
      "source": [
        "#set 1 if we are dealing with a positive class, and 0 if dealing with negative class\n",
        "def custom_forward(inputs, position_ids=None, token_type_ids=None, attention_mask=None):\n",
        "    preds = predict(inputs,\n",
        "                   position_ids=position_ids,\n",
        "                   token_type_ids=token_type_ids,\n",
        "                   attention_mask=attention_mask)\n",
        "    return torch.softmax(preds, dim = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create functions that give us the input ids, position ids and token_type_ids for the text we want to examine. It also creates a baseline for use in our integrated gradients.\n",
        "\n",
        "**Note: The function used to create the token type ids is the exact same as the longformer implementation when no token type ids. It is not necessary to create token_type_ids unless you are doing Integrated Gradients using multi-embedding as we need the baselines.**"
      ],
      "metadata": {
        "id": "bqbvfvXv0VTp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAAjmDRl0_In"
      },
      "outputs": [],
      "source": [
        "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
        "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
        "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgBSBpz-0_In"
      },
      "outputs": [],
      "source": [
        "max_length = 2046\n",
        "def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id):\n",
        "    text = text.lower()\n",
        "    text_ids = tokenizer.encode(text, truncation = True, add_special_tokens=False, max_length = max_length)\n",
        "    # construct input token ids\n",
        "    input_ids = [cls_token_id] + text_ids + [sep_token_id]\n",
        "    # construct reference token ids \n",
        "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n",
        "\n",
        "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(text_ids)\n",
        "\n",
        "def construct_input_ref_pos_id_pair(input_ids):\n",
        "    seq_length = input_ids.size(1)\n",
        "\n",
        "    #taken from the longformer implementation\n",
        "    mask = input_ids.ne(ref_token_id).int()\n",
        "    incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask\n",
        "    position_ids = incremental_indices.long().squeeze() + ref_token_id\n",
        "\n",
        "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
        "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
        "\n",
        "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "    return position_ids, ref_position_ids\n",
        "\n",
        "def construct_input_ref_token_type_pair(input_ids):\n",
        "    seq_len = input_ids.size(1)\n",
        "\n",
        "    # same as the tensor the model creates when you do not pass in token_type_ids as input.\n",
        "    token_type_ids = torch.zeros(seq_len, dtype=torch.long, device=device).unsqueeze(0).expand_as(input_ids)\n",
        "    \n",
        "    ref_token_type_ids = torch.zeros_like(token_type_ids, device=device)\n",
        "\n",
        "    return token_type_ids, ref_token_type_ids\n",
        "    \n",
        "def construct_attention_mask(input_ids):\n",
        "    return torch.ones_like(input_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform Layer Integrated Gradients using the longformer's embeddings. This can easily be adjusted to use longformer word embeddings, position and token_type embeddings."
      ],
      "metadata": {
        "id": "bQaYaSDf0buh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4uDuDrip0_Ip",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca7bef44-c7d5-4dce-f489-e23c75257a9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/captum/attr/_core/layer/layer_integrated_gradients.py:103: UserWarning: Multiple layers provided. Please ensure that each layer is**not** solely solely dependent on the outputs ofanother layer. Please refer to the documentation for moredetail.\n",
            "  \"Multiple layers provided. Please ensure that each layer is\"\n"
          ]
        }
      ],
      "source": [
        "lig = LayerIntegratedGradients(custom_forward, model.longformer.embeddings)\n",
        "lig2 = LayerIntegratedGradients(custom_forward, \\\n",
        "                                [model.longformer.embeddings.word_embeddings, \\\n",
        "                                 model.longformer.embeddings.position_embeddings,\\\n",
        "                                 model.longformer.embeddings.token_type_embeddings])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helper function to sum the attributions and normalize into an array of length (seq_len)."
      ],
      "metadata": {
        "id": "TYWjb7C2QTvJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lIeE9P7b0_Ir"
      },
      "outputs": [],
      "source": [
        "def summarize_attributions(attributions):\n",
        "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
        "    attributions = attributions / torch.linalg.norm(attributions)\n",
        "    return attributions"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We iterate over the entire dataset, getting the input_ids, position_ids and their baselines, performing integrated gradients, summing the attributions, and finally creating a dataframe to store the attributions and respective tokens. After we create the dataframe, get the aggregate attributions for each token in the example and save it in a list of dataframes."
      ],
      "metadata": {
        "id": "bqYiK1U9IOeP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "aggregate_attrib = []\n",
        "aggregation_function = {'attribution': 'sum'}\n",
        "\n",
        "for i in tqdm(range(len(cogs402_ds))):\n",
        "\n",
        "  #get input ids, position ids and attention mask for integrated gradients\n",
        "  text = cogs402_ds[i]['text']\n",
        "  input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n",
        "  position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
        "  attention_mask = construct_attention_mask(input_ids)\n",
        "\n",
        "  indices = input_ids[0].detach().tolist()\n",
        "  all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
        "\n",
        "  # perform integrated gradients\n",
        "  attributions = lig.attribute(inputs=input_ids,\n",
        "                                    baselines=ref_input_ids,\n",
        "                                    additional_forward_args=(position_ids, None, attention_mask),\n",
        "                                    target=1,\n",
        "                                    n_steps=50,\n",
        "                                    internal_batch_size = 2)\n",
        "  \n",
        "  #get the attributions\n",
        "  attributions_sum = summarize_attributions(attributions)\n",
        "  \n",
        "  #convert into dataframe\n",
        "  d = {\"tokens\":all_tokens, \"attribution\":attributions_sum[:len(all_tokens)].cpu()}  \n",
        "  df_attrib = pd.DataFrame(d)\n",
        "\n",
        "  #aggregate the duplicate tokens\n",
        "  df_attrib = df_attrib.groupby(df_attrib['tokens']).aggregate(aggregation_function)\n",
        "\n",
        "  #add to list of dataframes\n",
        "  aggregate_attrib.append(df_attrib)"
      ],
      "metadata": {
        "id": "CjL5YobvYmNr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bae823b0-35aa-4621-b921-c0c694f93264"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 12/12 [05:03<00:00, 25.27s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we have the implementation for the multi-embedding version. The only difference is that we have two attributions that we want to find the aggregate for, the position and word embeddings. \n",
        "\n",
        "**Note: despite passing in the token_type_ids and the baseline as inputs, we will not be able to get attributions for it as the input and the baseline are the same. It returns a tensor of nan values.**\n",
        "\n",
        "We create dataframes for both the word and position attributions to store the attributions and their respective token. We then aggregate the attributions based on the token for both dataframes. Finally, we append the position and word dataframes in their own separate list of dataframes."
      ],
      "metadata": {
        "id": "DDFBPe2C4Vtk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# aggregate_attrib = []\n",
        "# aggregate_pos = []\n",
        "\n",
        "# aggregation_function = {'attribution': 'sum'}\n",
        "\n",
        "# for i in tqdm(range(len(cogs402_ds)), position = 0, leave = True):\n",
        "  \n",
        "#   #get input_ids, position_ids, and the attention masks for the integrated gradients\n",
        "#   text = cogs402_ds[i]['text']\n",
        "\n",
        "#   input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n",
        "#   token_type_ids, ref_token_type_ids = construct_input_ref_token_type_pair(input_ids)\n",
        "#   position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
        "#   attention_mask = construct_attention_mask(input_ids)\n",
        "\n",
        "#   indices = input_ids[0].detach().tolist()\n",
        "#   all_tokens = tokenizer.convert_ids_to_tokens(indices)\n",
        "\n",
        "#   # compute integrated gradients\n",
        "#   attributions2 = lig2.attribute(inputs=(input_ids, position_ids, token_type_ids),\n",
        "#                                baselines=(ref_input_ids, ref_position_ids, ref_token_type_ids),\n",
        "#                                target=1,\n",
        "#                                additional_forward_args=attention_mask,\n",
        "#                                n_steps=20,\n",
        "#                                internal_batch_size = 2)\n",
        "  \n",
        "#   # get the attributions for the words and position ids\n",
        "#   attributions_word = summarize_attributions(attributions2[0])\n",
        "#   attributions_position = summarize_attributions(attributions2[1])\n",
        "\n",
        "#   # convert them both into dataframes \n",
        "#   d = {\"tokens\":all_tokens, \"attribution\":attributions_word[:len(all_tokens)].cpu()}  \n",
        "#   d2 = {\"tokens\":all_tokens, \"attribution\":attributions_position[:len(all_tokens)].cpu()}  \n",
        "  \n",
        "#   df_attrib = pd.DataFrame(d)\n",
        "#   df_attrib2 = pd.DataFrame(d2)\n",
        "\n",
        "#   #aggregate the attributions for duplicate tokens\n",
        "#   df_attrib = df_attrib.groupby(df_attrib['tokens']).aggregate(aggregation_function)\n",
        "#   df_attrib2 = df_attrib2.groupby(df_attrib2['tokens']).aggregate(aggregation_function)\n",
        "\n",
        "#   aggregate_attrib.append(df_attrib)\n",
        "#   aggregate_pos.append(df_attrib2)"
      ],
      "metadata": {
        "id": "EMuSIP5D4UIC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To get the aggregate attributions for every token over the entire dataset, we concatenate the list of dataframes we stored, sum up the attributions of duplicate tokens and divide by the number of items in each list."
      ],
      "metadata": {
        "id": "_g6kqbTXCS-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def combinedataframe(listframes, aggregation_func):\n",
        "  df_attrib = pd.concat(listframes)\n",
        "  df_attrib = df_attrib.reset_index(level=0)\n",
        "  df_attrib = df_attrib.groupby(df_attrib['tokens']).aggregate(aggregation_func)\n",
        "  df_attrib['attribution'] = df_attrib['attribution'].div(len(listframes))\n",
        "  highest_attrib_tokens_all = df_attrib.sort_values(by=['attribution'], ascending=False).reset_index()\n",
        "  return highest_attrib_tokens_all"
      ],
      "metadata": {
        "id": "JgulY85m4dOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_attrib = combinedataframe(aggregate_attrib, aggregation_function)\n",
        "df_attrib['tokens'] = df_attrib['tokens'].str.replace('Ġ', '')\n",
        "df_attrib\n",
        "# df_attrib_pos = combinedataframe(aggregate_pos, aggregation_function)"
      ],
      "metadata": {
        "id": "AH_T49HMF6Cn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "outputId": "22064563-efe7-4d3c-e4f2-84d107b6d540"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         tokens  attribution\n",
              "0           and     1.225943\n",
              "1             ,     0.568776\n",
              "2     discharge     0.372953\n",
              "3           his     0.308544\n",
              "4          with     0.273136\n",
              "...         ...          ...\n",
              "2767          [    -0.460796\n",
              "2768         to    -0.485024\n",
              "2769        the    -0.535409\n",
              "2770          ĉ    -0.596677\n",
              "2771               -0.646922\n",
              "\n",
              "[2772 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-50cc48b5-ad44-4fdb-b63f-1604f6f0a230\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>attribution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>and</td>\n",
              "      <td>1.225943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>,</td>\n",
              "      <td>0.568776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>discharge</td>\n",
              "      <td>0.372953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>his</td>\n",
              "      <td>0.308544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>with</td>\n",
              "      <td>0.273136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>[</td>\n",
              "      <td>-0.460796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>to</td>\n",
              "      <td>-0.485024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2769</th>\n",
              "      <td>the</td>\n",
              "      <td>-0.535409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2770</th>\n",
              "      <td>ĉ</td>\n",
              "      <td>-0.596677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2771</th>\n",
              "      <td></td>\n",
              "      <td>-0.646922</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2772 rows × 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50cc48b5-ad44-4fdb-b63f-1604f6f0a230')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-50cc48b5-ad44-4fdb-b63f-1604f6f0a230 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-50cc48b5-ad44-4fdb-b63f-1604f6f0a230');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masking the Tokens"
      ],
      "metadata": {
        "id": "q6EV8v8DN0mo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from transformers import AutoTokenizer\n",
        "nltk.download('stopwords')\n",
        "tokenizer2 = AutoTokenizer.from_pretrained('allenai/longformer-base-4096', add_prefix_space=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-LB4-lD6MURA",
        "outputId": "7167ca3b-a631-4d51-c63e-04523434039c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "all_stopwords = stopwords.words('english')\n",
        "all_stopwords.append(\" \")\n",
        "stopwords = set(tokenizer2.tokenize(all_stopwords, is_split_into_words =True))\n",
        "stopwords.update(all_stopwords)\n",
        "print(stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CxOWzlq2MWu3",
        "outputId": "863efb8c-89c8-45d8-dc4f-30865d5f2e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'more', 'Ġain', 'Ġyou', 'most', \"it's\", 'd', 'Ġto', 'on', 'Ġmust', \"mightn't\", 'Ġwho', 'Ġabout', 'Ġwhat', 'because', 'Ġoff', 'Ġwon', 'needn', 'Ġnow', 'Ġthem', 'Ġhadn', 'Ġthere', 'wasn', 'there', \"wasn't\", 'Ġnor', 'when', 'Ġhad', 'Ġhe', 'Ġwhich', 'Ġhas', 'the', 'Ġother', \"'s\", 'Ġhis', 'Ġour', 'Ġhasn', \"'d\", 'your', 'Ġbeing', 'to', \"you'll\", 'n', 'himself', 'will', \"wouldn't\", 'such', 'above', 'nor', 'Ġt', 'it', 'Ġsuch', 'Ġhow', 'was', 'shouldn', 'Ġhim', 'some', 'Ġagainst', 'Ġbe', 'have', 'Ġit', 'Ġthan', 'Ġbelow', \"aren't\", 'out', 'Ġyourselves', 'll', 'Ġwere', 'both', 'Ġthese', 'Ġbeen', 'an', 'doesn', 'is', 'Ġourselves', 'Ġif', 'very', 'Ġyourself', 'ain', 'Ġof', 'Ġeach', 'below', 'Ġam', 'whom', 'after', 'Ġ', 'ourselves', 'Ġuntil', 'own', 'these', 'Ġtoo', 'isn', 'Ġo', 'Ġso', 'Ġwasn', 'Ġboth', 'at', 'or', 'Ġsh', 'Ġher', 'through', 'and', 'm', 'Ġdo', 'her', 'once', 'didn', 'Ġwhom', 'their', 'too', 'that', 'Ġin', 'Ġwas', \"that'll\", 'Ġonly', 'all', 'Ġmy', 'itself', 'Ġdoesn', 'Ġshouldn', 'what', 'Ġm', 'Ġvery', 'no', 'Ġve', 'Ġshe', 'Ġneed', 'he', 'can', 'Ġthis', 'Ġthe', \"'ll\", 'Ġan', 'couldn', 'those', 'his', 'doing', \"doesn't\", 'ma', 'shan', 'don', 'Ġthat', \"you'd\", 'here', 'does', 'Ġfew', 'ours', 'Ġthemselves', 'Ġthrough', 'Ġma', 'under', 'a', \"'re\", 'Ġunder', 'then', 'which', 'any', 'won', 'Ġwe', 'had', \"hasn't\", 'be', 'where', 'Ġthose', 'Ġi', 'yourself', 't', 'Ġsame', \"shan't\", 'Ġtheirs', 'only', 'Ġours', 'during', 'each', 'Ġout', 'Ġll', 'again', 'Ġwhere', 'has', 'same', 'Ġall', 'myself', 'by', 'as', 'Ġa', 'o', 'further', 'against', 'other', 'Ġthey', 'Ġweren', 'Ġany', 'Ġinto', 'now', \"hadn't\", 'Ġhave', 'few', 'so', 'Ġduring', 'who', 'Ġtheir', \"don't\", \"couldn't\", 'are', 'into', 'hasn', 'Ġno', \"you're\", 'if', 'Ġmyself', 'its', 'Ġafter', 'Ġat', 'been', \"isn't\", 'am', 'Ġy', 'Ġabove', 'Ġwouldn', 'not', 'Ġdown', 'Ġits', 'Ġfurther', 'than', \"'t\", 'yourselves', 'being', 'did', 'Ġdoes', 'y', 'Ġover', 'were', 'Ġherself', 'Ġsome', \"haven't\", 'Ġis', 'Ġonce', 'Ġbefore', 'Ġyour', 'until', 'in', 'do', 'with', 'how', 'wouldn', 'i', \"you've\", 'Ġhers', 'Ġand', 'Ġshould', 'Ġdidn', 'Ġjust', 'Ġmost', \"needn't\", \"weren't\", 'Ġon', 'Ġd', 'Ġhaven', 'my', 'Ġfor', 'Ġs', 'Ġor', 'just', 'Ġmore', 'Ġup', 'Ġby', 'having', 'them', 'Ġdon', 'Ġare', 'Ġbetween', 'but', 'Ġmight', 'up', 'Ġnot', 'our', 'this', 'Ġthen', 'should', 'herself', 'mightn', 'Ġwhy', 'mustn', \"mustn't\", 'Ġhere', 'themselves', 'haven', 'Ġaren', 'while', 'Ġas', 'she', 'you', 'weren', 're', \"'ve\", 'before', \"didn't\", 'Ġbut', 'Ġitself', \"she's\", 'hers', 'over', 'hadn', 'Ġre', 's', 'between', 've', 'about', 'Ġwill', 'Ġyours', ' ', 'Ġdoing', 'Ġbecause', 'me', 'of', 'Ġme', \"won't\", 'Ġfrom', \"should've\", 'Ġagain', 'Ġisn', 'Ġcan', 'for', 'aren', 'why', 'yours', 'Ġwhile', 'Ġhaving', 'Ġwhen', 'Ġcouldn', 'we', 'down', \"shouldn't\", 'from', 'him', 'Ġown', 'Ġdid', 'they', 'Ġwith', 'Ġhimself', 'theirs', 'off'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are only showing the top 15 highest attributions, in other words, the tokens that have the most influence in the model predicting positive. If you are running integrated gradients using the longformer embeddings, this will be attributions for those embeddings. If you are running Integrated Gradients using word, position, and token_type embeddings, these will be the word embeddings."
      ],
      "metadata": {
        "id": "kxj_mmUxlx0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_attrib[:20]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "1bXA7aeS5AWx",
        "outputId": "b014f67d-99a5-44e1-d709-2c235b44077f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       tokens  attribution\n",
              "0         and     1.225943\n",
              "1           ,     0.568776\n",
              "2   discharge     0.372953\n",
              "3         his     0.308544\n",
              "4        with     0.273136\n",
              "5           ]     0.267823\n",
              "6        very     0.261354\n",
              "7          be     0.259413\n",
              "8        name     0.234457\n",
              "9        2020     0.178871\n",
              "10        ine     0.160587\n",
              "11         mg     0.158104\n",
              "12        not     0.139545\n",
              "13          /     0.139275\n",
              "14         no     0.132754\n",
              "15          2     0.132327\n",
              "16     mother     0.124459\n",
              "17     female     0.120808\n",
              "18       they     0.120760\n",
              "19         of     0.118935"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c2158583-ca02-48db-a553-d745b123e46d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>attribution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>and</td>\n",
              "      <td>1.225943</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>,</td>\n",
              "      <td>0.568776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>discharge</td>\n",
              "      <td>0.372953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>his</td>\n",
              "      <td>0.308544</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>with</td>\n",
              "      <td>0.273136</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>]</td>\n",
              "      <td>0.267823</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>very</td>\n",
              "      <td>0.261354</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>be</td>\n",
              "      <td>0.259413</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>name</td>\n",
              "      <td>0.234457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2020</td>\n",
              "      <td>0.178871</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ine</td>\n",
              "      <td>0.160587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>mg</td>\n",
              "      <td>0.158104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>not</td>\n",
              "      <td>0.139545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>/</td>\n",
              "      <td>0.139275</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>no</td>\n",
              "      <td>0.132754</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>2</td>\n",
              "      <td>0.132327</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>mother</td>\n",
              "      <td>0.124459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>female</td>\n",
              "      <td>0.120808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>they</td>\n",
              "      <td>0.120760</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>of</td>\n",
              "      <td>0.118935</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2158583-ca02-48db-a553-d745b123e46d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c2158583-ca02-48db-a553-d745b123e46d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c2158583-ca02-48db-a553-d745b123e46d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_attrib[(df_attrib['tokens'].str.isalpha()) & ~(df_attrib['tokens'].isin(stopwords))][:20].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "qzNbPyTYO8X7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "outputId": "6015e85e-38de-4b4b-db6f-5b6af7456f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       tokens  attribution\n",
              "0   discharge     0.372953\n",
              "1        name     0.234457\n",
              "2         ine     0.160587\n",
              "3          mg     0.158104\n",
              "4      mother     0.124459\n",
              "5      female     0.120808\n",
              "6       first     0.110170\n",
              "7          un     0.103989\n",
              "8      things     0.097076\n",
              "9   diagnosis     0.066723\n",
              "10        use     0.058021\n",
              "11    anxiety     0.057081\n",
              "12    pattern     0.055849\n",
              "13      would     0.049452\n",
              "14        may     0.048598\n",
              "15          r     0.047368\n",
              "16      prior     0.047259\n",
              "17         ad     0.044949\n",
              "18     father     0.044539\n",
              "19    parents     0.040283"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f056dd24-41d8-4cb4-885f-0a6a106603ef\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>attribution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>discharge</td>\n",
              "      <td>0.372953</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>name</td>\n",
              "      <td>0.234457</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ine</td>\n",
              "      <td>0.160587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>mg</td>\n",
              "      <td>0.158104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>mother</td>\n",
              "      <td>0.124459</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>female</td>\n",
              "      <td>0.120808</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>first</td>\n",
              "      <td>0.110170</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>un</td>\n",
              "      <td>0.103989</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>things</td>\n",
              "      <td>0.097076</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>diagnosis</td>\n",
              "      <td>0.066723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>use</td>\n",
              "      <td>0.058021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>anxiety</td>\n",
              "      <td>0.057081</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>pattern</td>\n",
              "      <td>0.055849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>would</td>\n",
              "      <td>0.049452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>may</td>\n",
              "      <td>0.048598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>r</td>\n",
              "      <td>0.047368</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>prior</td>\n",
              "      <td>0.047259</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>ad</td>\n",
              "      <td>0.044949</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>father</td>\n",
              "      <td>0.044539</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>parents</td>\n",
              "      <td>0.040283</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f056dd24-41d8-4cb4-885f-0a6a106603ef')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f056dd24-41d8-4cb4-885f-0a6a106603ef button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f056dd24-41d8-4cb4-885f-0a6a106603ef');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are showing the 15 highest attributions for the position embeddings. Note that running integrated gradients using the longformer embeddings rather than the word, position and token_type embeddings will not have this output."
      ],
      "metadata": {
        "id": "EpF2ZfDBRssb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_attrib_pos[:15]"
      ],
      "metadata": {
        "id": "UgvpXJBMb84H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are only showing the top 15 lowest attributions, in other words, the tokens that have the most influence in the model predicting negative. If you are running integrated gradients using the longformer embeddings, this will be attributions for those embeddings. If you are running Integrated Gradients using word, position and token_type embeddings, these will be the word embeddings."
      ],
      "metadata": {
        "id": "94_GDXHsl0vs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_attrib[:-19:-1]"
      ],
      "metadata": {
        "id": "T7mHIG0NuG0X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "outputId": "1dea6d05-d858-4880-a65d-d45cbb0f7249"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        tokens  attribution\n",
              "2771              -0.646922\n",
              "2770         ĉ    -0.596677\n",
              "2769       the    -0.535409\n",
              "2768        to    -0.485024\n",
              "2767         [    -0.460796\n",
              "2766      that    -0.425058\n",
              "2765        he    -0.286452\n",
              "2764      this    -0.258939\n",
              "2763  hospital    -0.252264\n",
              "2762      name    -0.240356\n",
              "2761         .    -0.235462\n",
              "2760        we    -0.218915\n",
              "2759         a    -0.208033\n",
              "2758        is    -0.201268\n",
              "2757      also    -0.195440\n",
              "2756       her    -0.182083\n",
              "2755  disorder    -0.129558\n",
              "2754        at    -0.125935"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-89ee250b-07bb-415f-b713-c9354c830d10\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>attribution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2771</th>\n",
              "      <td></td>\n",
              "      <td>-0.646922</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2770</th>\n",
              "      <td>ĉ</td>\n",
              "      <td>-0.596677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2769</th>\n",
              "      <td>the</td>\n",
              "      <td>-0.535409</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2768</th>\n",
              "      <td>to</td>\n",
              "      <td>-0.485024</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2767</th>\n",
              "      <td>[</td>\n",
              "      <td>-0.460796</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2766</th>\n",
              "      <td>that</td>\n",
              "      <td>-0.425058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2765</th>\n",
              "      <td>he</td>\n",
              "      <td>-0.286452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2764</th>\n",
              "      <td>this</td>\n",
              "      <td>-0.258939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2763</th>\n",
              "      <td>hospital</td>\n",
              "      <td>-0.252264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2762</th>\n",
              "      <td>name</td>\n",
              "      <td>-0.240356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2761</th>\n",
              "      <td>.</td>\n",
              "      <td>-0.235462</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2760</th>\n",
              "      <td>we</td>\n",
              "      <td>-0.218915</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2759</th>\n",
              "      <td>a</td>\n",
              "      <td>-0.208033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2758</th>\n",
              "      <td>is</td>\n",
              "      <td>-0.201268</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2757</th>\n",
              "      <td>also</td>\n",
              "      <td>-0.195440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2756</th>\n",
              "      <td>her</td>\n",
              "      <td>-0.182083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2755</th>\n",
              "      <td>disorder</td>\n",
              "      <td>-0.129558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2754</th>\n",
              "      <td>at</td>\n",
              "      <td>-0.125935</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-89ee250b-07bb-415f-b713-c9354c830d10')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-89ee250b-07bb-415f-b713-c9354c830d10 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-89ee250b-07bb-415f-b713-c9354c830d10');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_attrib[(df_attrib['tokens'].str.isalpha()) & ~(df_attrib['tokens'].isin(stopwords))][:-19:-1].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Pe1rbT6EPBhV",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 614
        },
        "outputId": "8fbca182-fd24-4f20-fae8-66c90ed26d21"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "       tokens  attribution\n",
              "0           ĉ    -0.596677\n",
              "1    hospital    -0.252264\n",
              "2        name    -0.240356\n",
              "3        also    -0.195440\n",
              "4    disorder    -0.129558\n",
              "5        time    -0.125816\n",
              "6        able    -0.123044\n",
              "7       first    -0.117680\n",
              "8        home    -0.104151\n",
              "9   admission    -0.101625\n",
              "10       itle    -0.092742\n",
              "11       care    -0.075398\n",
              "12       part    -0.067505\n",
              "13       help    -0.062158\n",
              "14        flu    -0.060587\n",
              "15     mental    -0.060198\n",
              "16    support    -0.058090\n",
              "17    thought    -0.056677"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-88736302-e300-4aeb-8702-1c4a0a880664\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tokens</th>\n",
              "      <th>attribution</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ĉ</td>\n",
              "      <td>-0.596677</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>hospital</td>\n",
              "      <td>-0.252264</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>name</td>\n",
              "      <td>-0.240356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>also</td>\n",
              "      <td>-0.195440</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>disorder</td>\n",
              "      <td>-0.129558</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>time</td>\n",
              "      <td>-0.125816</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>able</td>\n",
              "      <td>-0.123044</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>first</td>\n",
              "      <td>-0.117680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>home</td>\n",
              "      <td>-0.104151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>admission</td>\n",
              "      <td>-0.101625</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>itle</td>\n",
              "      <td>-0.092742</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>care</td>\n",
              "      <td>-0.075398</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>part</td>\n",
              "      <td>-0.067505</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>help</td>\n",
              "      <td>-0.062158</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>flu</td>\n",
              "      <td>-0.060587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>mental</td>\n",
              "      <td>-0.060198</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>support</td>\n",
              "      <td>-0.058090</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>thought</td>\n",
              "      <td>-0.056677</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-88736302-e300-4aeb-8702-1c4a0a880664')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-88736302-e300-4aeb-8702-1c4a0a880664 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-88736302-e300-4aeb-8702-1c4a0a880664');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are showing the 15 lowest attributions for the position embeddings. Note that running integrated gradients using the longformer embeddings rather than the word, position and token_type embeddings will not have this output."
      ],
      "metadata": {
        "id": "tKusjNjBStgY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_attrib_pos[:-14:-1]"
      ],
      "metadata": {
        "id": "bzhn7k4zcDHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the pandas dataframe into a csv to access it in the future without having to run through the entire dataset. Change the path and file name to one fitting your project."
      ],
      "metadata": {
        "id": "RgPYHilmlsGh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # longformer embeddings\n",
        "# df_attrib.to_csv('/content/drive/MyDrive/cogs402longformer/fakeclinicalnotes/results/notes_attributions/longformer_emb_notes.csv')\n",
        "\n",
        "# Word + position embeddings for the papers dataset\n",
        "# df_attrib.to_csv(/content/drive/MyDrive/cogs402longformer/fakeclinicalnotes/results/notes_attributions/word_emb_notes.csv')\n",
        "# df_attrib_pos.to_csv(/content/drive/MyDrive/cogs402longformer/fakeclinicalnotes/results/notes_attributions/pos_emb_notes.csv')\n"
      ],
      "metadata": {
        "id": "6WTv-OJrGKvF"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "file_extension": ".py",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "mimetype": "text/x-python",
    "name": "python",
    "npconvert_exporter": "python",
    "pygments_lexer": "ipython3",
    "version": 3,
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "b5b3538ee8ae496986e088fa81bc3e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1b7dd46ac519464dbf630fcf89743649",
              "IPY_MODEL_67082ad5672043079286993881ce7af1",
              "IPY_MODEL_0dc4688cd6ff4fc8ad3f9b5d2eaeb01c"
            ],
            "layout": "IPY_MODEL_976471339e40487e8a4de3b39b4036c4"
          }
        },
        "1b7dd46ac519464dbf630fcf89743649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f05d59b07db2415da87cc48f7edf0dda",
            "placeholder": "​",
            "style": "IPY_MODEL_be8290071b7f409e82c1a88aed4e00fb",
            "value": "100%"
          }
        },
        "67082ad5672043079286993881ce7af1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6d144feb2c845d2aaa66348684ed541",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bfd5949f6929455694b80ea3260c7798",
            "value": 1
          }
        },
        "0dc4688cd6ff4fc8ad3f9b5d2eaeb01c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43eda6eb9ef044f1af44377080a5f3ea",
            "placeholder": "​",
            "style": "IPY_MODEL_d35603213ab8444f91b731dd4050d2f8",
            "value": " 1/1 [00:00&lt;00:00, 27.82it/s]"
          }
        },
        "976471339e40487e8a4de3b39b4036c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f05d59b07db2415da87cc48f7edf0dda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "be8290071b7f409e82c1a88aed4e00fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c6d144feb2c845d2aaa66348684ed541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bfd5949f6929455694b80ea3260c7798": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "43eda6eb9ef044f1af44377080a5f3ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d35603213ab8444f91b731dd4050d2f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}