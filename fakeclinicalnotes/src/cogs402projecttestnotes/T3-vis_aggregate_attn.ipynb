{"cells":[{"cell_type":"markdown","source":["This notebook is a part of the [T3-vis](https://arxiv.org/abs/2108.13587) implmentation for visualizing Transformer Neural Networks. Here, using the dataset, model and the functions, we predict over the validation set and grab the attentions for every example. We then aggregate all the attentions together, normalize, then save the file for use in another notebook where we display the aggregate attention to look for patterns."],"metadata":{"id":"3oTbQ8d0v0c1"},"id":"3oTbQ8d0v0c1"},{"cell_type":"markdown","source":["### Importing Packages"],"metadata":{"id":"rJphs4TJ4Yql"},"id":"rJphs4TJ4Yql"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kZTPlUo8Wp1T","executionInfo":{"status":"ok","timestamp":1659556717653,"user_tz":420,"elapsed":1213,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"8b3f45d5-83e1-47dc-bb45-1b67bf506f80"},"id":"kZTPlUo8Wp1T","execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# import sys\n","# sys.path.append('/content/drive/My Drive/{}'.format(\"cogs402longformer/\"))"],"metadata":{"id":"AlcBiC0nWtJE","executionInfo":{"status":"ok","timestamp":1659556717654,"user_tz":420,"elapsed":4,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}}},"id":"AlcBiC0nWtJE","execution_count":2,"outputs":[]},{"cell_type":"code","source":["pip install datasets --quiet"],"metadata":{"id":"s_6vceLhTIBf","executionInfo":{"status":"ok","timestamp":1659556722730,"user_tz":420,"elapsed":5079,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}}},"id":"s_6vceLhTIBf","execution_count":3,"outputs":[]},{"cell_type":"code","source":["pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M-qXH2JkTMdA","executionInfo":{"status":"ok","timestamp":1659556727549,"user_tz":420,"elapsed":4822,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"588755a3-d8b0-4574-ff2c-0b4bf20261d5"},"id":"M-qXH2JkTMdA","execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n"]}]},{"cell_type":"code","execution_count":5,"id":"9b774ad0-b725-4910-9050-423edf160ebd","metadata":{"id":"9b774ad0-b725-4910-9050-423edf160ebd","executionInfo":{"status":"ok","timestamp":1659556732566,"user_tz":420,"elapsed":5020,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}}},"outputs":[],"source":["import os\n","\n","import numpy as np\n","import pandas as pd\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","\n","import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","source":["### Import Dataset and Model"],"metadata":{"id":"gre9lf3p4cwm"},"id":"gre9lf3p4cwm"},{"cell_type":"markdown","source":["Here we are importing the model and the dataset we want to assess. The import is replicating the manner used by the T3-vis implementation, with the removal of a few items such as \"idx\" and \"visualize columns\" as they are unnecessary. "],"metadata":{"id":"Iav1crTg4hs2"},"id":"Iav1crTg4hs2"},{"cell_type":"code","source":["from datasets import load_dataset\n","from transformers import LongformerForSequenceClassification, AutoTokenizer"],"metadata":{"id":"tOTWYLz_OuUe","executionInfo":{"status":"ok","timestamp":1659556733907,"user_tz":420,"elapsed":1351,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}}},"id":"tOTWYLz_OuUe","execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["Here we import the \"papers\" model and dataset."],"metadata":{"id":"NVHYINeq4nZG"},"id":"NVHYINeq4nZG"},{"cell_type":"code","execution_count":7,"id":"66cc97f5-7e3a-476c-9858-5643eeaa6675","metadata":{"id":"66cc97f5-7e3a-476c-9858-5643eeaa6675","executionInfo":{"status":"ok","timestamp":1659556733907,"user_tz":420,"elapsed":3,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}}},"outputs":[],"source":["def longformer_finetuned_notes():\n","    test = torch.load(\"/content/drive/MyDrive/fakeclinicalnotes/models/full_augmented_lr2e-5_dropout3_10_trained_threshold.pt\")\n","    model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', state_dict=test['state_dict'], num_labels = 2)\n","    return model\n","\n","def preprocess_function(tokenizer, example, max_length):\n","    example.update(tokenizer(example['text'], padding='max_length', max_length=max_length, truncation=True))\n","    return example\n","\n","def get_notes_dataset(dataset_type):\n","    max_length = 2048\n","    dataset = load_dataset(\"danielhou13/cogs402datafake\")[dataset_type]\n","\n","    tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n","    # tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","    dataset = dataset.map(lambda x: preprocess_function(tokenizer, x, max_length), batched=True)\n","    setattr(dataset, 'input_columns', ['input_ids', 'attention_mask'])\n","    setattr(dataset, 'target_columns', ['labels'])\n","    setattr(dataset, 'max_length', max_length)\n","    setattr(dataset, 'tokenizer', tokenizer)\n","    return dataset\n","\n","def notes_train_set():\n","    return get_notes_dataset('train')"]},{"cell_type":"markdown","source":["Here we are calling the functions to import the dataset, model and making sure that the dataset is in a pytorch compatible manner."],"metadata":{"id":"tT93uStZ4yv3"},"id":"tT93uStZ4yv3"},{"cell_type":"code","execution_count":8,"id":"24ad54a3-db97-47e3-8cc7-417d4db2c99b","metadata":{"id":"24ad54a3-db97-47e3-8cc7-417d4db2c99b","colab":{"base_uri":"https://localhost:8080/","height":225,"referenced_widgets":["81df873456b24072ac6a5eda36691921","953fd5a80f7148499a9c5ff628920d30","4c10465984fd4efb99b817378325bdc5","11c6cc28cb83401eb72e1beab3d40b5c","edaa746091b54d738857e9fa891da30c","0158ab5534ef4c26b8f6357e6881bf1e","9a21c6da54454fbb8208c3732d48314e","d23adacf918945b49b5efda56f55226d","907e0362b653407db2e35990894e719f","51f6a650e952481c870765115e894d4d","24916ef031f24598b4b932efa1124f82"]},"executionInfo":{"status":"ok","timestamp":1659556755330,"user_tz":420,"elapsed":21425,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"2c03c720-91a9-4306-df24-599634f388a6"},"outputs":[{"output_type":"stream","name":"stderr","text":["Using custom data configuration danielhou13--cogs402datafake-f5349e6cf83e41d8\n","Reusing dataset parquet (/root/.cache/huggingface/datasets/danielhou13___parquet/danielhou13--cogs402datafake-f5349e6cf83e41d8/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81df873456b24072ac6a5eda36691921"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/danielhou13___parquet/danielhou13--cogs402datafake-f5349e6cf83e41d8/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-decb6b766464f86e.arrow\n","Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['longformer_model.embeddings.token_type_embeddings.weight', 'longformer_model.encoder.layer.3.attention.self.key_global.weight', 'longformer_model.encoder.layer.8.output.dense.bias', 'longformer_model.embeddings.LayerNorm.weight', 'longformer_model.encoder.layer.5.attention.self.query.weight', 'longformer_model.encoder.layer.0.attention.self.query_global.bias', 'longformer_model.encoder.layer.3.attention.self.key.weight', 'longformer_model.encoder.layer.4.attention.self.value_global.bias', 'longformer_model.encoder.layer.9.attention.self.key_global.bias', 'longformer_model.encoder.layer.7.attention.self.query.weight', 'longformer_model.encoder.layer.2.attention.self.value_global.bias', 'longformer_model.encoder.layer.6.attention.self.value.bias', 'longformer_model.encoder.layer.7.attention.self.key_global.bias', 'longformer_model.encoder.layer.9.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.10.output.LayerNorm.weight', 'longformer_model.encoder.layer.11.output.dense.weight', 'longformer_model.encoder.layer.9.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.1.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.6.output.LayerNorm.weight', 'longformer_model.encoder.layer.10.attention.self.query_global.bias', 'longformer_model.encoder.layer.5.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.11.attention.self.query_global.weight', 'longformer_model.encoder.layer.3.attention.output.dense.bias', 'longformer_model.encoder.layer.8.attention.self.key_global.bias', 'longformer_model.encoder.layer.5.attention.self.value_global.bias', 'longformer_model.encoder.layer.4.output.dense.weight', 'longformer_model.encoder.layer.6.output.dense.bias', 'longformer_model.encoder.layer.4.output.LayerNorm.weight', 'longformer_model.encoder.layer.7.attention.self.value.bias', 'longformer_model.encoder.layer.5.output.LayerNorm.weight', 'longformer_model.encoder.layer.10.attention.self.query_global.weight', 'longformer_model.encoder.layer.8.attention.self.query.weight', 'longformer_model.encoder.layer.2.attention.self.query.weight', 'longformer_model.encoder.layer.6.attention.self.key.bias', 'dense.weight', 'longformer_model.encoder.layer.5.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.4.output.dense.bias', 'longformer_model.encoder.layer.7.attention.self.value_global.bias', 'longformer_model.encoder.layer.3.output.LayerNorm.bias', 'longformer_model.encoder.layer.0.attention.self.key.weight', 'longformer_model.encoder.layer.3.output.dense.bias', 'longformer_model.encoder.layer.9.output.dense.weight', 'longformer_model.encoder.layer.9.output.LayerNorm.bias', 'longformer_model.encoder.layer.3.attention.self.value.weight', 'longformer_model.encoder.layer.2.output.LayerNorm.weight', 'longformer_model.encoder.layer.0.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.1.attention.output.dense.weight', 'longformer_model.encoder.layer.10.intermediate.dense.weight', 'longformer_model.encoder.layer.11.attention.output.dense.weight', 'longformer_model.encoder.layer.3.attention.output.dense.weight', 'longformer_model.encoder.layer.9.output.dense.bias', 'longformer_model.encoder.layer.3.attention.self.query_global.weight', 'longformer_model.encoder.layer.7.output.dense.bias', 'longformer_model.encoder.layer.8.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.8.output.LayerNorm.weight', 'longformer_model.encoder.layer.3.attention.self.query.bias', 'longformer_model.encoder.layer.3.attention.self.value_global.weight', 'longformer_model.encoder.layer.9.intermediate.dense.bias', 'longformer_model.encoder.layer.2.output.dense.bias', 'longformer_model.encoder.layer.11.attention.self.query.weight', 'longformer_model.encoder.layer.9.attention.self.query_global.weight', 'longformer_model.encoder.layer.10.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.4.intermediate.dense.weight', 'longformer_model.encoder.layer.5.attention.self.value.weight', 'longformer_model.encoder.layer.8.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.0.output.dense.bias', 'longformer_model.encoder.layer.2.attention.self.key_global.bias', 'longformer_model.encoder.layer.3.output.LayerNorm.weight', 'longformer_model.encoder.layer.7.attention.self.value_global.weight', 'longformer_model.encoder.layer.0.intermediate.dense.weight', 'longformer_model.encoder.layer.7.attention.self.key.weight', 'longformer_model.encoder.layer.8.attention.self.value_global.weight', 'longformer_model.encoder.layer.9.attention.self.key_global.weight', 'longformer_model.encoder.layer.0.attention.self.key_global.weight', 'longformer_model.encoder.layer.0.attention.self.query_global.weight', 'longformer_model.encoder.layer.11.attention.self.key_global.weight', 'longformer_model.encoder.layer.3.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.4.output.LayerNorm.bias', 'longformer_model.encoder.layer.0.intermediate.dense.bias', 'longformer_model.encoder.layer.10.attention.self.query.bias', 'longformer_model.encoder.layer.1.attention.self.query_global.weight', 'longformer_model.encoder.layer.2.attention.self.key_global.weight', 'longformer_model.encoder.layer.2.output.dense.weight', 'longformer_model.embeddings.LayerNorm.bias', 'longformer_model.encoder.layer.7.attention.self.query_global.bias', 'longformer_model.encoder.layer.6.attention.self.query_global.bias', 'longformer_model.encoder.layer.11.output.LayerNorm.bias', 'longformer_model.encoder.layer.6.attention.self.key_global.bias', 'longformer_model.encoder.layer.0.attention.self.key.bias', 'longformer_model.encoder.layer.9.attention.self.value.weight', 'longformer_model.encoder.layer.9.attention.self.query_global.bias', 'longformer_model.encoder.layer.6.attention.self.value_global.bias', 'longformer_model.encoder.layer.3.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.6.attention.self.key.weight', 'longformer_model.encoder.layer.11.attention.self.key.bias', 'longformer_model.encoder.layer.10.attention.output.dense.weight', 'longformer_model.encoder.layer.8.intermediate.dense.bias', 'longformer_model.encoder.layer.0.attention.self.query.weight', 'longformer_model.encoder.layer.5.output.dense.weight', 'longformer_model.encoder.layer.0.attention.self.key_global.bias', 'longformer_model.encoder.layer.5.intermediate.dense.bias', 'longformer_model.encoder.layer.0.attention.self.value.bias', 'longformer_model.encoder.layer.0.attention.self.value_global.bias', 'longformer_model.encoder.layer.2.intermediate.dense.weight', 'longformer_model.encoder.layer.7.attention.self.value.weight', 'longformer_model.encoder.layer.7.output.LayerNorm.weight', 'longformer_model.encoder.layer.9.attention.self.query.bias', 'longformer_model.encoder.layer.10.attention.self.value.weight', 'longformer_model.encoder.layer.8.output.dense.weight', 'longformer_model.encoder.layer.6.intermediate.dense.bias', 'longformer_model.encoder.layer.4.attention.self.key_global.weight', 'longformer_model.encoder.layer.5.attention.self.key_global.bias', 'longformer_model.encoder.layer.6.attention.self.query.bias', 'longformer_model.encoder.layer.3.attention.self.key_global.bias', 'longformer_model.encoder.layer.0.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.2.intermediate.dense.bias', 'longformer_model.encoder.layer.4.attention.self.query.bias', 'longformer_model.encoder.layer.4.attention.self.query_global.bias', 'longformer_model.encoder.layer.4.attention.output.dense.weight', 'longformer_model.encoder.layer.7.intermediate.dense.bias', 'longformer_model.encoder.layer.8.attention.self.query.bias', 'longformer_model.encoder.layer.10.attention.output.dense.bias', 'longformer_model.encoder.layer.10.intermediate.dense.bias', 'longformer_model.encoder.layer.7.intermediate.dense.weight', 'longformer_model.embeddings.word_embeddings.weight', 'longformer_model.encoder.layer.2.output.LayerNorm.bias', 'longformer_model.encoder.layer.1.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.6.output.dense.weight', 'longformer_model.encoder.layer.7.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.6.attention.self.query.weight', 'longformer_model.encoder.layer.9.attention.self.key.weight', 'longformer_model.encoder.layer.1.attention.self.key_global.bias', 'longformer_model.encoder.layer.5.attention.self.query.bias', 'longformer_model.encoder.layer.9.attention.output.dense.weight', 'longformer_model.encoder.layer.10.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.0.attention.output.dense.weight', 'longformer_model.encoder.layer.4.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.1.intermediate.dense.bias', 'longformer_model.encoder.layer.9.attention.output.dense.bias', 'longformer_model.encoder.layer.1.attention.self.key.weight', 'longformer_model.encoder.layer.6.intermediate.dense.weight', 'longformer_model.encoder.layer.8.intermediate.dense.weight', 'longformer_model.encoder.layer.0.attention.self.query.bias', 'fc.weight', 'longformer_model.encoder.layer.4.attention.self.key_global.bias', 'longformer_model.encoder.layer.1.output.LayerNorm.bias', 'longformer_model.encoder.layer.11.output.LayerNorm.weight', 'longformer_model.encoder.layer.5.attention.self.key.bias', 'longformer_model.encoder.layer.5.attention.self.value.bias', 'longformer_model.encoder.layer.9.attention.self.query.weight', 'longformer_model.encoder.layer.4.attention.self.value_global.weight', 'longformer_model.encoder.layer.0.attention.self.value.weight', 'longformer_model.encoder.layer.2.attention.output.dense.bias', 'longformer_model.encoder.layer.1.attention.self.query_global.bias', 'longformer_model.encoder.layer.8.attention.output.dense.weight', 'longformer_model.encoder.layer.9.attention.self.key.bias', 'longformer_model.encoder.layer.11.attention.self.value_global.weight', 'longformer_model.encoder.layer.3.attention.self.query.weight', 'longformer_model.encoder.layer.5.attention.self.query_global.bias', 'longformer_model.encoder.layer.11.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.2.attention.self.key.bias', 'longformer_model.encoder.layer.3.output.dense.weight', 'longformer_model.encoder.layer.9.output.LayerNorm.weight', 'longformer_model.encoder.layer.0.output.LayerNorm.weight', 'longformer_model.encoder.layer.4.attention.self.value.weight', 'longformer_model.encoder.layer.7.attention.self.query.bias', 'longformer_model.encoder.layer.10.attention.self.value_global.bias', 'longformer_model.encoder.layer.10.output.dense.weight', 'longformer_model.encoder.layer.11.attention.self.value.weight', 'longformer_model.encoder.layer.3.intermediate.dense.weight', 'longformer_model.encoder.layer.2.attention.self.query_global.bias', 'longformer_model.encoder.layer.8.attention.self.key.weight', 'longformer_model.encoder.layer.10.attention.self.key.weight', 'longformer_model.encoder.layer.2.attention.self.value.weight', 'longformer_model.encoder.layer.4.intermediate.dense.bias', 'longformer_model.encoder.layer.5.attention.self.query_global.weight', 'longformer_model.encoder.layer.8.attention.self.value.weight', 'longformer_model.encoder.layer.9.intermediate.dense.weight', 'longformer_model.encoder.layer.11.attention.output.LayerNorm.weight', 'dense.bias', 'longformer_model.encoder.layer.3.attention.self.value.bias', 'longformer_model.encoder.layer.0.attention.output.dense.bias', 'longformer_model.encoder.layer.6.attention.self.key_global.weight', 'longformer_model.encoder.layer.5.output.dense.bias', 'longformer_model.encoder.layer.7.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.11.intermediate.dense.weight', 'longformer_model.encoder.layer.11.attention.self.query.bias', 'longformer_model.encoder.layer.11.attention.self.value_global.bias', 'longformer_model.encoder.layer.7.output.dense.weight', 'longformer_model.encoder.layer.0.attention.self.value_global.weight', 'longformer_model.encoder.layer.1.attention.output.dense.bias', 'longformer_model.encoder.layer.10.attention.self.key.bias', 'fc.bias', 'longformer_model.encoder.layer.2.attention.self.key.weight', 'longformer_model.encoder.layer.2.attention.self.value_global.weight', 'longformer_model.encoder.layer.1.output.LayerNorm.weight', 'longformer_model.encoder.layer.2.attention.self.value.bias', 'longformer_model.encoder.layer.5.output.LayerNorm.bias', 'longformer_model.encoder.layer.6.output.LayerNorm.bias', 'longformer_model.encoder.layer.1.attention.self.key_global.weight', 'longformer_model.encoder.layer.9.attention.self.value_global.bias', 'longformer_model.encoder.layer.2.attention.self.query.bias', 'longformer_model.encoder.layer.1.attention.self.value_global.weight', 'longformer_model.encoder.layer.6.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.5.attention.self.key_global.weight', 'longformer_model.encoder.layer.8.attention.self.value.bias', 'longformer_model.encoder.layer.8.attention.self.query_global.bias', 'longformer_model.encoder.layer.1.output.dense.weight', 'longformer_model.encoder.layer.8.attention.self.value_global.bias', 'longformer_model.encoder.layer.5.attention.self.value_global.weight', 'longformer_model.encoder.layer.0.output.LayerNorm.bias', 'longformer_model.encoder.layer.1.attention.self.value.bias', 'longformer_model.embeddings.position_embeddings.weight', 'longformer_model.encoder.layer.1.attention.self.query.bias', 'longformer_model.encoder.layer.2.attention.self.query_global.weight', 'longformer_model.encoder.layer.2.attention.output.dense.weight', 'longformer_model.encoder.layer.5.attention.self.key.weight', 'longformer_model.encoder.layer.6.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.7.attention.output.dense.bias', 'longformer_model.encoder.layer.1.attention.self.value.weight', 'longformer_model.encoder.layer.10.output.LayerNorm.bias', 'longformer_model.encoder.layer.11.attention.output.dense.bias', 'longformer_model.encoder.layer.7.attention.self.key.bias', 'longformer_model.encoder.layer.5.attention.output.dense.bias', 'longformer_model.encoder.layer.10.attention.self.key_global.bias', 'longformer_model.encoder.layer.9.attention.self.value_global.weight', 'longformer_model.encoder.layer.8.attention.self.key.bias', 'longformer_model.encoder.layer.11.attention.self.key.weight', 'longformer_model.encoder.layer.6.attention.output.dense.bias', 'longformer_model.encoder.layer.4.attention.output.dense.bias', 'longformer_model.encoder.layer.1.intermediate.dense.weight', 'longformer_model.encoder.layer.7.attention.self.query_global.weight', 'longformer_model.encoder.layer.3.attention.self.value_global.bias', 'longformer_model.encoder.layer.1.attention.self.value_global.bias', 'longformer_model.embeddings.position_ids', 'longformer_model.encoder.layer.2.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.6.attention.self.value_global.weight', 'longformer_model.encoder.layer.11.attention.self.query_global.bias', 'longformer_model.encoder.layer.5.intermediate.dense.weight', 'longformer_model.encoder.layer.3.attention.self.key.bias', 'longformer_model.encoder.layer.10.output.dense.bias', 'longformer_model.encoder.layer.1.attention.self.query.weight', 'longformer_model.encoder.layer.7.attention.output.dense.weight', 'longformer_model.encoder.layer.10.attention.self.key_global.weight', 'longformer_model.encoder.layer.11.attention.self.value.bias', 'longformer_model.encoder.layer.4.attention.self.value.bias', 'longformer_model.encoder.layer.6.attention.self.value.weight', 'longformer_model.encoder.layer.11.attention.self.key_global.bias', 'longformer_model.encoder.layer.4.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.0.output.dense.weight', 'longformer_model.encoder.layer.10.attention.self.value_global.weight', 'longformer_model.encoder.layer.8.attention.self.key_global.weight', 'longformer_model.encoder.layer.4.attention.self.query.weight', 'longformer_model.encoder.layer.2.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.4.attention.self.key.bias', 'longformer_model.encoder.layer.6.attention.self.query_global.weight', 'longformer_model.encoder.layer.1.attention.self.key.bias', 'longformer_model.encoder.layer.7.attention.self.key_global.weight', 'longformer_model.encoder.layer.11.output.dense.bias', 'longformer_model.encoder.layer.10.attention.self.query.weight', 'longformer_model.encoder.layer.5.attention.output.dense.weight', 'longformer_model.encoder.layer.8.attention.self.query_global.weight', 'longformer_model.encoder.layer.11.intermediate.dense.bias', 'longformer_model.encoder.layer.7.output.LayerNorm.bias', 'longformer_model.encoder.layer.3.intermediate.dense.bias', 'longformer_model.encoder.layer.8.attention.output.dense.bias', 'longformer_model.encoder.layer.1.output.dense.bias', 'longformer_model.encoder.layer.6.attention.output.dense.weight', 'longformer_model.encoder.layer.3.attention.self.query_global.bias', 'longformer_model.encoder.layer.9.attention.self.value.bias', 'longformer_model.encoder.layer.4.attention.self.key.weight', 'longformer_model.encoder.layer.8.output.LayerNorm.bias', 'longformer_model.encoder.layer.10.attention.self.value.bias', 'longformer_model.encoder.layer.4.attention.self.query_global.weight']\n","- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['longformer.encoder.layer.6.attention.self.query_global.bias', 'classifier.dense.weight', 'longformer.encoder.layer.1.attention.self.key_global.weight', 'longformer.encoder.layer.1.attention.self.key.bias', 'longformer.encoder.layer.9.attention.self.value_global.bias', 'longformer.encoder.layer.10.attention.self.key_global.weight', 'longformer.encoder.layer.3.attention.output.LayerNorm.bias', 'longformer.encoder.layer.8.attention.self.value.weight', 'longformer.encoder.layer.11.attention.self.query.weight', 'longformer.encoder.layer.9.attention.self.query.weight', 'longformer.encoder.layer.7.output.LayerNorm.weight', 'longformer.embeddings.LayerNorm.bias', 'longformer.encoder.layer.4.attention.output.LayerNorm.weight', 'longformer.encoder.layer.11.intermediate.dense.bias', 'longformer.encoder.layer.7.attention.output.dense.bias', 'longformer.encoder.layer.10.attention.self.value.bias', 'longformer.encoder.layer.0.intermediate.dense.weight', 'longformer.encoder.layer.11.attention.self.query.bias', 'longformer.encoder.layer.8.attention.self.key_global.bias', 'longformer.encoder.layer.6.attention.output.dense.bias', 'longformer.encoder.layer.8.attention.self.key.bias', 'longformer.encoder.layer.3.attention.self.key_global.weight', 'longformer.encoder.layer.2.intermediate.dense.weight', 'longformer.encoder.layer.3.attention.self.value_global.bias', 'longformer.encoder.layer.4.attention.self.query_global.bias', 'longformer.encoder.layer.2.output.LayerNorm.bias', 'longformer.encoder.layer.1.attention.self.key.weight', 'longformer.encoder.layer.3.attention.self.key.bias', 'longformer.encoder.layer.5.attention.self.query.weight', 'longformer.encoder.layer.5.attention.self.key_global.bias', 'longformer.encoder.layer.0.attention.self.value.weight', 'longformer.encoder.layer.5.intermediate.dense.bias', 'longformer.encoder.layer.7.intermediate.dense.weight', 'longformer.encoder.layer.4.attention.self.key_global.weight', 'longformer.encoder.layer.7.attention.self.value.bias', 'longformer.encoder.layer.7.attention.output.dense.weight', 'longformer.encoder.layer.10.output.dense.weight', 'longformer.encoder.layer.7.attention.self.value_global.weight', 'longformer.encoder.layer.2.attention.self.key.bias', 'longformer.encoder.layer.4.attention.self.key_global.bias', 'longformer.encoder.layer.2.attention.self.key_global.weight', 'longformer.encoder.layer.9.attention.self.key.bias', 'longformer.encoder.layer.7.attention.self.key.bias', 'longformer.encoder.layer.5.intermediate.dense.weight', 'longformer.encoder.layer.4.attention.self.value_global.weight', 'longformer.encoder.layer.3.attention.output.LayerNorm.weight', 'longformer.encoder.layer.6.attention.self.key.bias', 'longformer.encoder.layer.2.intermediate.dense.bias', 'longformer.encoder.layer.10.attention.output.dense.weight', 'longformer.encoder.layer.10.attention.self.query.weight', 'longformer.encoder.layer.3.attention.self.query_global.bias', 'longformer.encoder.layer.5.attention.output.LayerNorm.bias', 'longformer.encoder.layer.4.attention.self.query_global.weight', 'longformer.encoder.layer.2.attention.self.value.weight', 'longformer.encoder.layer.7.attention.self.query.weight', 'longformer.encoder.layer.1.attention.self.key_global.bias', 'longformer.encoder.layer.8.attention.output.LayerNorm.weight', 'longformer.encoder.layer.11.attention.self.key_global.bias', 'longformer.encoder.layer.8.attention.self.key_global.weight', 'longformer.encoder.layer.1.attention.self.query.bias', 'longformer.encoder.layer.8.attention.self.query.weight', 'longformer.encoder.layer.2.attention.self.query_global.bias', 'longformer.encoder.layer.7.attention.self.key_global.weight', 'longformer.encoder.layer.6.attention.self.key.weight', 'longformer.encoder.layer.11.attention.self.value_global.weight', 'longformer.encoder.layer.9.output.dense.weight', 'longformer.encoder.layer.11.output.LayerNorm.weight', 'longformer.encoder.layer.1.attention.output.dense.bias', 'longformer.encoder.layer.11.attention.self.value.bias', 'longformer.encoder.layer.10.intermediate.dense.bias', 'longformer.encoder.layer.6.attention.output.LayerNorm.weight', 'longformer.encoder.layer.6.attention.self.key_global.weight', 'longformer.encoder.layer.0.attention.self.key.bias', 'longformer.encoder.layer.9.attention.self.query_global.bias', 'longformer.encoder.layer.11.output.LayerNorm.bias', 'longformer.encoder.layer.5.attention.self.value_global.weight', 'longformer.encoder.layer.6.attention.self.value.weight', 'longformer.encoder.layer.0.attention.output.LayerNorm.weight', 'longformer.encoder.layer.0.attention.self.key.weight', 'longformer.encoder.layer.0.attention.self.key_global.bias', 'longformer.encoder.layer.0.attention.self.value.bias', 'longformer.encoder.layer.2.attention.output.dense.weight', 'longformer.encoder.layer.5.output.dense.weight', 'longformer.encoder.layer.1.attention.self.query_global.weight', 'longformer.encoder.layer.8.output.LayerNorm.weight', 'longformer.encoder.layer.4.intermediate.dense.weight', 'longformer.encoder.layer.7.attention.self.key_global.bias', 'longformer.encoder.layer.6.attention.self.query.weight', 'longformer.encoder.layer.5.output.dense.bias', 'longformer.encoder.layer.9.output.dense.bias', 'longformer.encoder.layer.3.attention.self.query.bias', 'longformer.encoder.layer.4.attention.output.dense.bias', 'longformer.encoder.layer.4.attention.self.query.weight', 'longformer.encoder.layer.4.output.LayerNorm.weight', 'longformer.encoder.layer.8.attention.self.query_global.bias', 'longformer.encoder.layer.8.intermediate.dense.bias', 'longformer.encoder.layer.1.attention.output.LayerNorm.bias', 'longformer.encoder.layer.1.attention.self.value.weight', 'longformer.encoder.layer.6.attention.output.dense.weight', 'longformer.encoder.layer.7.output.dense.weight', 'longformer.encoder.layer.9.attention.output.dense.weight', 'longformer.encoder.layer.3.attention.self.key_global.bias', 'longformer.embeddings.LayerNorm.weight', 'longformer.encoder.layer.5.attention.output.dense.weight', 'longformer.encoder.layer.6.output.LayerNorm.bias', 'longformer.encoder.layer.1.attention.self.value.bias', 'longformer.encoder.layer.4.output.dense.weight', 'longformer.encoder.layer.5.attention.self.query_global.bias', 'longformer.encoder.layer.3.attention.self.query_global.weight', 'longformer.encoder.layer.10.attention.self.value_global.bias', 'longformer.encoder.layer.8.attention.self.key.weight', 'longformer.encoder.layer.5.attention.self.query.bias', 'longformer.encoder.layer.10.attention.output.dense.bias', 'longformer.encoder.layer.7.attention.output.LayerNorm.weight', 'longformer.encoder.layer.0.output.LayerNorm.weight', 'longformer.encoder.layer.8.attention.self.value_global.weight', 'longformer.encoder.layer.1.attention.self.value_global.bias', 'longformer.encoder.layer.11.attention.self.key.weight', 'longformer.encoder.layer.6.output.dense.weight', 'longformer.encoder.layer.8.attention.output.LayerNorm.bias', 'longformer.encoder.layer.5.attention.self.key_global.weight', 'longformer.encoder.layer.3.attention.output.dense.bias', 'longformer.encoder.layer.6.attention.self.query.bias', 'longformer.encoder.layer.2.attention.self.key_global.bias', 'longformer.encoder.layer.4.attention.self.query.bias', 'longformer.encoder.layer.4.attention.output.dense.weight', 'longformer.encoder.layer.2.attention.self.query_global.weight', 'longformer.encoder.layer.6.intermediate.dense.weight', 'longformer.encoder.layer.10.attention.self.key_global.bias', 'longformer.encoder.layer.5.output.LayerNorm.weight', 'longformer.encoder.layer.9.attention.self.query_global.weight', 'longformer.encoder.layer.11.attention.output.LayerNorm.bias', 'longformer.encoder.layer.10.attention.self.value.weight', 'longformer.encoder.layer.11.attention.self.value.weight', 'longformer.encoder.layer.11.attention.self.key_global.weight', 'longformer.encoder.layer.9.attention.self.value.weight', 'longformer.encoder.layer.7.output.dense.bias', 'longformer.encoder.layer.11.intermediate.dense.weight', 'longformer.encoder.layer.4.output.dense.bias', 'longformer.encoder.layer.2.output.dense.bias', 'longformer.encoder.layer.9.output.LayerNorm.weight', 'longformer.encoder.layer.5.attention.self.key.weight', 'longformer.encoder.layer.0.attention.self.query.weight', 'longformer.encoder.layer.0.intermediate.dense.bias', 'longformer.encoder.layer.7.attention.self.query_global.bias', 'longformer.encoder.layer.9.attention.self.key_global.weight', 'longformer.encoder.layer.11.output.dense.weight', 'longformer.encoder.layer.9.output.LayerNorm.bias', 'longformer.encoder.layer.0.attention.output.dense.bias', 'longformer.encoder.layer.4.output.LayerNorm.bias', 'longformer.encoder.layer.3.output.LayerNorm.bias', 'longformer.encoder.layer.0.attention.output.dense.weight', 'longformer.encoder.layer.8.attention.output.dense.weight', 'longformer.encoder.layer.10.attention.output.LayerNorm.bias', 'longformer.encoder.layer.1.attention.self.value_global.weight', 'longformer.encoder.layer.2.attention.output.dense.bias', 'longformer.encoder.layer.3.attention.self.value_global.weight', 'longformer.encoder.layer.4.attention.self.value.bias', 'longformer.encoder.layer.10.attention.self.value_global.weight', 'longformer.encoder.layer.5.attention.output.dense.bias', 'longformer.encoder.layer.3.attention.self.query.weight', 'longformer.encoder.layer.6.attention.self.query_global.weight', 'longformer.encoder.layer.9.attention.self.key.weight', 'longformer.encoder.layer.11.attention.self.query_global.weight', 'longformer.encoder.layer.6.attention.output.LayerNorm.bias', 'longformer.encoder.layer.11.attention.self.query_global.bias', 'longformer.encoder.layer.2.attention.output.LayerNorm.bias', 'longformer.encoder.layer.3.intermediate.dense.bias', 'longformer.encoder.layer.10.attention.self.key.bias', 'longformer.encoder.layer.10.attention.self.key.weight', 'longformer.encoder.layer.4.attention.self.key.weight', 'longformer.encoder.layer.5.attention.self.value_global.bias', 'longformer.encoder.layer.10.output.LayerNorm.weight', 'longformer.encoder.layer.11.attention.output.dense.weight', 'longformer.encoder.layer.1.attention.output.dense.weight', 'longformer.embeddings.position_embeddings.weight', 'longformer.encoder.layer.5.attention.self.query_global.weight', 'longformer.encoder.layer.1.attention.self.query_global.bias', 'longformer.encoder.layer.2.attention.self.value.bias', 'longformer.encoder.layer.8.attention.self.value_global.bias', 'longformer.encoder.layer.10.output.dense.bias', 'longformer.encoder.layer.8.attention.self.query.bias', 'longformer.encoder.layer.9.attention.self.value.bias', 'classifier.out_proj.bias', 'longformer.encoder.layer.4.attention.output.LayerNorm.bias', 'longformer.encoder.layer.0.attention.self.query_global.weight', 'longformer.encoder.layer.0.output.LayerNorm.bias', 'longformer.embeddings.word_embeddings.weight', 'longformer.encoder.layer.8.output.LayerNorm.bias', 'longformer.encoder.layer.5.attention.self.key.bias', 'longformer.encoder.layer.6.output.dense.bias', 'longformer.encoder.layer.2.attention.self.value_global.weight', 'classifier.out_proj.weight', 'longformer.encoder.layer.0.output.dense.bias', 'longformer.encoder.layer.5.attention.output.LayerNorm.weight', 'longformer.encoder.layer.6.attention.self.value_global.bias', 'longformer.encoder.layer.2.output.dense.weight', 'longformer.encoder.layer.2.attention.self.key.weight', 'longformer.encoder.layer.10.attention.self.query_global.bias', 'longformer.encoder.layer.9.attention.self.key_global.bias', 'longformer.encoder.layer.9.attention.output.LayerNorm.bias', 'longformer.encoder.layer.2.attention.self.query.bias', 'longformer.encoder.layer.3.output.dense.weight', 'longformer.encoder.layer.6.attention.self.value_global.weight', 'longformer.encoder.layer.4.intermediate.dense.bias', 'longformer.encoder.layer.8.attention.output.dense.bias', 'longformer.encoder.layer.0.attention.self.value_global.bias', 'longformer.encoder.layer.1.output.dense.weight', 'longformer.encoder.layer.7.attention.self.value_global.bias', 'longformer.encoder.layer.7.intermediate.dense.bias', 'longformer.encoder.layer.6.attention.self.key_global.bias', 'classifier.dense.bias', 'longformer.encoder.layer.3.attention.self.value.bias', 'longformer.encoder.layer.11.attention.output.dense.bias', 'longformer.encoder.layer.3.attention.self.value.weight', 'longformer.encoder.layer.7.attention.self.key.weight', 'longformer.encoder.layer.3.intermediate.dense.weight', 'longformer.encoder.layer.3.output.dense.bias', 'longformer.encoder.layer.1.output.LayerNorm.weight', 'longformer.encoder.layer.5.attention.self.value.bias', 'longformer.encoder.layer.6.output.LayerNorm.weight', 'longformer.encoder.layer.9.intermediate.dense.bias', 'longformer.encoder.layer.6.attention.self.value.bias', 'longformer.encoder.layer.11.output.dense.bias', 'longformer.encoder.layer.5.attention.self.value.weight', 'longformer.encoder.layer.2.attention.output.LayerNorm.weight', 'longformer.encoder.layer.0.attention.self.query.bias', 'longformer.encoder.layer.7.output.LayerNorm.bias', 'longformer.encoder.layer.8.output.dense.bias', 'longformer.encoder.layer.4.attention.self.value.weight', 'longformer.encoder.layer.0.attention.self.query_global.bias', 'longformer.encoder.layer.8.attention.self.query_global.weight', 'longformer.encoder.layer.10.intermediate.dense.weight', 'longformer.encoder.layer.7.attention.self.query_global.weight', 'longformer.encoder.layer.0.attention.self.value_global.weight', 'longformer.encoder.layer.0.attention.output.LayerNorm.bias', 'longformer.encoder.layer.11.attention.self.key.bias', 'longformer.encoder.layer.3.attention.self.key.weight', 'longformer.encoder.layer.3.output.LayerNorm.weight', 'longformer.encoder.layer.2.attention.self.query.weight', 'longformer.encoder.layer.7.attention.self.query.bias', 'longformer.encoder.layer.8.attention.self.value.bias', 'longformer.encoder.layer.9.attention.output.dense.bias', 'longformer.encoder.layer.7.attention.output.LayerNorm.bias', 'longformer.encoder.layer.9.attention.self.query.bias', 'longformer.encoder.layer.1.attention.output.LayerNorm.weight', 'longformer.encoder.layer.10.attention.self.query_global.weight', 'longformer.encoder.layer.10.attention.output.LayerNorm.weight', 'longformer.encoder.layer.2.output.LayerNorm.weight', 'longformer.encoder.layer.6.intermediate.dense.bias', 'longformer.encoder.layer.11.attention.self.value_global.bias', 'longformer.encoder.layer.1.attention.self.query.weight', 'longformer.encoder.layer.4.attention.self.key.bias', 'longformer.encoder.layer.7.attention.self.value.weight', 'longformer.encoder.layer.3.attention.output.dense.weight', 'longformer.encoder.layer.0.attention.self.key_global.weight', 'longformer.encoder.layer.10.attention.self.query.bias', 'longformer.embeddings.token_type_embeddings.weight', 'longformer.encoder.layer.1.intermediate.dense.weight', 'longformer.encoder.layer.1.output.dense.bias', 'longformer.encoder.layer.0.output.dense.weight', 'longformer.encoder.layer.5.output.LayerNorm.bias', 'longformer.encoder.layer.9.attention.output.LayerNorm.weight', 'longformer.encoder.layer.1.intermediate.dense.bias', 'longformer.encoder.layer.2.attention.self.value_global.bias', 'longformer.encoder.layer.1.output.LayerNorm.bias', 'longformer.encoder.layer.10.output.LayerNorm.bias', 'longformer.encoder.layer.9.intermediate.dense.weight', 'longformer.encoder.layer.8.intermediate.dense.weight', 'longformer.encoder.layer.11.attention.output.LayerNorm.weight', 'longformer.encoder.layer.9.attention.self.value_global.weight', 'longformer.encoder.layer.4.attention.self.value_global.bias', 'longformer.encoder.layer.8.output.dense.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["['input_ids', 'attention_mask', 'labels']\n"]}],"source":["cogs402_test = notes_train_set()\n","model = longformer_finetuned_notes()\n","columns = cogs402_test.input_columns + cogs402_test.target_columns\n","print(columns)\n","cogs402_test.set_format(type='torch', columns=columns)\n","cogs402_test=cogs402_test.remove_columns(['text'])"]},{"cell_type":"markdown","source":["Don't forget to use your GPU if you have one for faster performance."],"metadata":{"id":"p9d2r9yM4zs1"},"id":"p9d2r9yM4zs1"},{"cell_type":"code","execution_count":9,"id":"deaabfa2-0855-41fd-870c-7a8b91e32d44","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"deaabfa2-0855-41fd-870c-7a8b91e32d44","executionInfo":{"status":"ok","timestamp":1659556755570,"user_tz":420,"elapsed":242,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"48dcaf38-f57b-412e-b5e9-51b8a9403f71"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["if torch.cuda.is_available():\n","    model = model.cuda()\n","    cuda0 = torch.device('cuda:0')\n","\n","print(model.device)"]},{"cell_type":"markdown","source":["### Functions"],"metadata":{"id":"WjpaH-yn5CoB"},"id":"WjpaH-yn5CoB"},{"cell_type":"markdown","source":["The following block is the normalization code by T3-vis. It operates by converting the values into colour values which we can use for plotting. For each layer and head in the complete attention matrix, we take the (seq_len, seq_len) matrix, normalize all the values, then scale it so each value is between 0-255. Arrays of the same shape representing colour channels are then made, and the colours we do not want are masked and set to 0. Finally we stack our 4 matrices so that each item in our original array now contains 4 colour values (red, blue, green, alpha). We then convert this into a list, keeping the 4 colour values of each item sequential. \n","\n","**The input is an array of shape: (layer, batch, head, seq_len, seq_len)**. \n","\n","**The output shape is a (4 x layer x batch x head x seq_len x seq_len) list**\n","\n","The aggregated attention contains layer x batch x head x seq_len x seq_len values in the attention matrix and there are 4 colour channels: red, blue, green, alpha (controls how opaque the colour is)."],"metadata":{"id":"EYPuk4mG5E1b"},"id":"EYPuk4mG5E1b"},{"cell_type":"code","execution_count":10,"id":"eb707291-bb19-4975-a3cd-8186d00baf32","metadata":{"id":"eb707291-bb19-4975-a3cd-8186d00baf32","executionInfo":{"status":"ok","timestamp":1659556755571,"user_tz":420,"elapsed":2,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}}},"outputs":[],"source":["def format_attention_image(attention):\n","    formatted_attn = []\n","    for layer_idx in range(attention.shape[0]):\n","        for head_idx in range(attention.shape[1]):\n","            formatted_entry = {\n","                'layer': layer_idx,\n","                'head': head_idx\n","            }\n","\n","            # Flatten value of log attention normalize between 255 and 0\n","            if len(attention[layer_idx, head_idx]) == 0:\n","                continue\n","            attn = np.array(attention[layer_idx, head_idx]).flatten()\n","            attn = (attn - attn.min()) / (attn.max() - attn.min())\n","            alpha = np.round(attn * 255)\n","            red = np.ones_like(alpha) * 255\n","            green = np.zeros_like(alpha) * 255\n","            blue = np.zeros_like(alpha) * 255\n","\n","            attn_data = np.dstack([red,green,blue,alpha]).reshape(alpha.shape[0] * 4).astype('uint8')\n","            formatted_entry['attn'] = attn_data.tolist()\n","            formatted_attn.append(formatted_entry)\n","    return formatted_attn"]},{"cell_type":"markdown","source":["This block of code, **not found in T3-vis**, adapts the longformer model's sliding window attention into the traditional attention format of seq_len x seq_len. More information can be found in this notebook [here](https://colab.research.google.com/drive/1Kxx26NtIlUzioRCHpsR8IbSz_DpRFxEZ).\n","\n","**Input: Tensor of shape: (layer, batch, head, seq_len, x + sliding_window_size + 1), where x is the number of tokens with global attention.** \\\\\n","\n","**Output:\n","Tensor of shape: (layer, batch, head, seq_len, seq_len)**\n","\n","By converting the matrix into a seq_len x seq_len matrix, the T3-vis functions will work as intended."],"metadata":{"id":"QMDtrSXh5Q5L"},"id":"QMDtrSXh5Q5L"},{"cell_type":"code","execution_count":11,"id":"0882a3e5-1b0c-4319-92a3-92f5b516d854","metadata":{"id":"0882a3e5-1b0c-4319-92a3-92f5b516d854","executionInfo":{"status":"ok","timestamp":1659556755744,"user_tz":420,"elapsed":3,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}}},"outputs":[],"source":["def create_head_matrix(output_attentions, global_attentions):\n","    new_attention_matrix = torch.zeros((output_attentions.shape[0], \n","                                      output_attentions.shape[0]))\n","    for i in range(output_attentions.shape[0]):\n","        test_non_zeroes = torch.nonzero(output_attentions[i]).squeeze()\n","        test2 = output_attentions[i][test_non_zeroes[1:]]\n","        new_attention_matrix_indices = test_non_zeroes[1:]-257 + i\n","        new_attention_matrix[i][new_attention_matrix_indices] = test2\n","        new_attention_matrix[i][0] = output_attentions[i][0]\n","        new_attention_matrix[0] = global_attentions.squeeze()[:output_attentions.shape[0]]\n","    return new_attention_matrix\n","\n","\n","def attentions_all_heads(output_attentions, global_attentions):\n","    new_matrix = []\n","    for i in range(output_attentions.shape[0]):\n","        matrix = create_head_matrix(output_attentions[i], global_attentions[i])\n","        new_matrix.append(matrix)\n","    return torch.stack(new_matrix)\n","\n","def all_batches(output_attentions, global_attentions):\n","    new_matrix = []\n","    for i in range(output_attentions.shape[0]):\n","        matrix = attentions_all_heads(output_attentions[i], global_attentions[i])\n","        new_matrix.append(matrix)\n","    return torch.stack(new_matrix)\n","\n","def all_layers(output_attentions, global_attentions):\n","    new_matrix = []\n","    for i in range(output_attentions.shape[0]):\n","        matrix = all_batches(output_attentions[i], global_attentions[i])\n","        new_matrix.append(matrix)\n","    return torch.stack(new_matrix)"]},{"cell_type":"markdown","source":["This T3-vis function is used to find collect and aggregate the attentions over the entire validation set. It iterates through the dataset, gets the attention for each example, converts each example's attention into the correct shape, then aggregates the attention. Lastly, it sends the aggregated attention matrix to the normalizer function to create a 4 * layer * batch * head * seq_len * seq_len list."],"metadata":{"id":"FcgFLbnn5ba1"},"id":"FcgFLbnn5ba1"},{"cell_type":"code","execution_count":12,"id":"09a4925c-512b-420a-8978-47fce8bb1fbd","metadata":{"id":"09a4925c-512b-420a-8978-47fce8bb1fbd","executionInfo":{"status":"ok","timestamp":1659556755744,"user_tz":420,"elapsed":2,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}}},"outputs":[],"source":["from tqdm import tqdm\n","def compute_aggregated_attn(model, dataloader, max_input_len):\n","\n","    n_layers = model.longformer.config.num_hidden_layers\n","    n_heads = model.longformer.config.num_attention_heads\n","    # head_size = int(model.longformer.config.hidden_size / n_heads)\n","    # n_examples = len(dataloader.dataset)\n","\n","    # importance_scores = np.zeros((n_layers, n_heads))\n","\n","    device = model.device\n","    # total_loss = 0.\n","    attn = np.zeros((n_layers, n_heads, max_input_len, max_input_len))\n","    print(attn.shape)\n","    model.eval()\n","\n","    attn_normalize_count = torch.zeros(max_input_len, device=device)\n","\n","    for step, inputs in enumerate(tqdm(dataloader, position=0, leave=True)):\n","\n","        # batch_size_ = inputs['input_ids'].__len__()\n","\n","        if torch.cuda.is_available():\n","            for k, v in inputs.items():\n","                if isinstance(v, torch.Tensor):\n","                    inputs[k] = v.cuda()\n","        \n","        inputs['output_attentions']=True\n","        \n","        with torch.no_grad():\n","            output = model(**inputs)\n","        \n","        \n","        attn_normalize_count += inputs['attention_mask'].sum(dim=0)\n","        batch_attn = output[-2]\n","        global_attn = output[-1]\n","        \n","        # print(batch_attn[1].shape)\n","        output_attentions = torch.stack(batch_attn).cpu()\n","\n","        # print(\"output_attention.shape\", output_attentions.shape)\n","        global_attentions = torch.stack(global_attn).cpu()\n","         \n","        # print(output_attentions.device)\n","        # print(global_attentions.device)\n","        \n","        batch_attn2 = all_layers(output_attentions, global_attentions)\n","    \n","        # print(batch_attn2.shape)\n","        batch_attn = torch.cat([l.sum(dim=0).unsqueeze(0) for l in batch_attn2], dim=0).cpu().numpy()\n","        \n","        attn += batch_attn\n","        \n","    max_input_len = len(attn_normalize_count.nonzero(as_tuple=False))\n","    \n","    attn = attn[:, :, :max_input_len, :max_input_len]\n","    attn /= attn_normalize_count.cpu().numpy()[:max_input_len]\n","    print(type(attn))\n","    formatted_attn = format_attention_image(attn)\n","    return formatted_attn"]},{"cell_type":"markdown","source":["The functions operate on a dataloader so we convert our validation set into a dataloader format, using batch_size=1 to minimize our memory usage as longformer uses lots of memory."],"metadata":{"id":"24-O41uK5gfX"},"id":"24-O41uK5gfX"},{"cell_type":"code","execution_count":13,"id":"ced64aa2-5005-4b14-99eb-ad6c97476460","metadata":{"id":"ced64aa2-5005-4b14-99eb-ad6c97476460","executionInfo":{"status":"ok","timestamp":1659556755745,"user_tz":420,"elapsed":3,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}}},"outputs":[],"source":["dataloader = torch.utils.data.DataLoader(cogs402_test, batch_size=1)"]},{"cell_type":"markdown","source":["We run the function here, passing in the model, dataloader and how many tokens your want your attention matrix to visualize."],"metadata":{"id":"ZvkvS3r_5ouV"},"id":"ZvkvS3r_5ouV"},{"cell_type":"code","execution_count":null,"id":"e1177893-14a9-4d3c-9495-25c503ba41c9","metadata":{"id":"e1177893-14a9-4d3c-9495-25c503ba41c9","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3853727c-c5de-4daf-dbde-b10f1f7a6989"},"outputs":[{"output_type":"stream","name":"stdout","text":["(12, 12, 2048, 2048)\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 12/12 [04:59<00:00, 24.99s/it]\n"]},{"output_type":"stream","name":"stdout","text":["<class 'numpy.ndarray'>\n"]}],"source":["test = compute_aggregated_attn(model, dataloader, cogs402_test.max_length)"]},{"cell_type":"code","source":["print(type(test))"],"metadata":{"id":"zILBwKP1pKl_"},"id":"zILBwKP1pKl_","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lastly, we save our new aggregate attention matrix. Remember to change the path to whatever suits your project's needs. The commented-out line of code saves the attention matrix in the current working directory.\n","\n","Warning: This line of code may take a long time as the ndarray can be very large."],"metadata":{"id":"54p2tstg54wF"},"id":"54p2tstg54wF"},{"cell_type":"code","source":["torch.save(test, \"/content/drive/MyDrive/fakeclinicalnotes/t3-visapplication/notes/aggregate_attn.pt\")\n","# torch.save(test, \"aggregate_attn.pt\")"],"metadata":{"id":"wwVXmUXMZ3ZB"},"id":"wwVXmUXMZ3ZB","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"T3-vis_aggregate_attn.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"81df873456b24072ac6a5eda36691921":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_953fd5a80f7148499a9c5ff628920d30","IPY_MODEL_4c10465984fd4efb99b817378325bdc5","IPY_MODEL_11c6cc28cb83401eb72e1beab3d40b5c"],"layout":"IPY_MODEL_edaa746091b54d738857e9fa891da30c"}},"953fd5a80f7148499a9c5ff628920d30":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0158ab5534ef4c26b8f6357e6881bf1e","placeholder":"​","style":"IPY_MODEL_9a21c6da54454fbb8208c3732d48314e","value":"100%"}},"4c10465984fd4efb99b817378325bdc5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d23adacf918945b49b5efda56f55226d","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_907e0362b653407db2e35990894e719f","value":1}},"11c6cc28cb83401eb72e1beab3d40b5c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_51f6a650e952481c870765115e894d4d","placeholder":"​","style":"IPY_MODEL_24916ef031f24598b4b932efa1124f82","value":" 1/1 [00:00&lt;00:00, 10.27it/s]"}},"edaa746091b54d738857e9fa891da30c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0158ab5534ef4c26b8f6357e6881bf1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9a21c6da54454fbb8208c3732d48314e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d23adacf918945b49b5efda56f55226d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"907e0362b653407db2e35990894e719f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"51f6a650e952481c870765115e894d4d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"24916ef031f24598b4b932efa1124f82":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}