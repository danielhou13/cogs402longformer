{"cells":[{"cell_type":"markdown","source":["This notebook is primarily the same as the section on converting to a PDF in [Token_attention_with_head_importance](https://colab.research.google.com/drive/1iVojJQp0CZS484tMZqIizosXPLxgKvRX?usp=sharing); however, this notebook solely focuses on converting the attentions into a PDF visualization. This notebook predicts over the dataset and finds interesting examples one may want to visualize the attentions of such as false negatives, false postiives, and very confident predictions. It of course, will output a PDF of the text with the attentions of the token layered on top of the token."],"metadata":{"id":"t8OGqf7Pl7-h"},"id":"t8OGqf7Pl7-h"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kZTPlUo8Wp1T","executionInfo":{"status":"ok","timestamp":1659555346956,"user_tz":420,"elapsed":15900,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"2ef9241c-e221-45eb-b5ba-8e77150594f3"},"id":"kZTPlUo8Wp1T","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["### Install and Import Dependencies"],"metadata":{"id":"XFt7t0wAERQV"},"id":"XFt7t0wAERQV"},{"cell_type":"code","source":["# import sys\n","# sys.path.append('/content/drive/My Drive/{}'.format(\"cogs402longformer/\"))"],"metadata":{"id":"AlcBiC0nWtJE"},"id":"AlcBiC0nWtJE","execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install datasets --quiet"],"metadata":{"id":"s_6vceLhTIBf"},"id":"s_6vceLhTIBf","execution_count":null,"outputs":[]},{"cell_type":"code","source":["pip install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M-qXH2JkTMdA","executionInfo":{"status":"ok","timestamp":1658960527300,"user_tz":420,"elapsed":4248,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"69d2c3c9-0b31-4a54-8fc4-f4f10a7579a7"},"id":"M-qXH2JkTMdA","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.21.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.8.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.1)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n"]}]},{"cell_type":"markdown","source":["##Import Dataset and Model"],"metadata":{"id":"hSSPoSRn6r9A"},"id":"hSSPoSRn6r9A"},{"cell_type":"code","execution_count":null,"id":"9b774ad0-b725-4910-9050-423edf160ebd","metadata":{"id":"9b774ad0-b725-4910-9050-423edf160ebd"},"outputs":[],"source":["import os\n","\n","import numpy as np\n","import pandas as pd\n","\n","import torch\n","import torch.nn as nn"]},{"cell_type":"markdown","source":["Import the Reserach Papers dataset"],"metadata":{"id":"hfHRqrpw_VlN"},"id":"hfHRqrpw_VlN"},{"cell_type":"code","execution_count":null,"id":"66cc97f5-7e3a-476c-9858-5643eeaa6675","metadata":{"id":"66cc97f5-7e3a-476c-9858-5643eeaa6675"},"outputs":[],"source":["from datasets import load_dataset\n","from transformers import LongformerForSequenceClassification, AutoTokenizer\n","\n","tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n","model_path = 'danielhou13/longformer-finetuned_papers_v2'\n","model_path2 = 'danielhou13/longformer-finetuned-news-cogs402'\n","model_path3 = 'allenai/longformer-base-4096'\n","\n","def longformer_finetuned_papers():\n","    test = torch.load(\"/content/drive/MyDrive/fakeclinicalnotes/models/full_augmented_lr2e-5_dropout3_10_trained_threshold.pt\")\n","    model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', state_dict=test['state_dict'], num_labels = 2)\n","    return model\n","\n","def preprocess_function(tokenizer, example, max_length):\n","    example.update(tokenizer(example['text'], padding='max_length', max_length=max_length, truncation=True))\n","    return example\n","\n","def get_papers_dataset(dataset_type):\n","    max_length = 2048\n","    dataset = load_dataset(\"danielhou13/cogs402datafake\")[dataset_type]\n","\n","    # tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n","    dataset = dataset.map(lambda x: preprocess_function(tokenizer, x, max_length), batched=True)\n","    setattr(dataset, 'input_columns', ['input_ids', 'attention_mask'])\n","    setattr(dataset, 'target_columns', ['labels'])\n","    setattr(dataset, 'max_length', max_length)\n","    setattr(dataset, 'tokenizer', tokenizer)\n","    return dataset\n","\n","def papers_test_set():\n","    return get_papers_dataset('test')\n","\n","def papers_train_set():\n","    return get_papers_dataset('train')"]},{"cell_type":"markdown","source":["Load papers model and dataset and preprocess it"],"metadata":{"id":"srzj_2BeNGOK"},"id":"srzj_2BeNGOK"},{"cell_type":"code","execution_count":null,"id":"24ad54a3-db97-47e3-8cc7-417d4db2c99b","metadata":{"id":"24ad54a3-db97-47e3-8cc7-417d4db2c99b","executionInfo":{"status":"ok","timestamp":1658960550707,"user_tz":420,"elapsed":14241,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"colab":{"base_uri":"https://localhost:8080/","height":233,"referenced_widgets":["d421873c349f401aa4432d85952c5ee4","768f49c6c3d143a9a7378a3449ceb45f","816aac2a8b994e8b9b195294b1037e6b","688a23135e534c58b42a2a93e896be59","0df6a3c497274319a9d994942d4bf2de","b5b0ba6284714cce9c11a1cecbd1a566","900b155be3964b8b8ca7b1409d28fd62","86d7eef078724817ad8de66c20480d24","8d0ae94ed1b14b97a64f83f9a65f1205","f12296c17dc04b799cb8ce6c094200f7","f07c3437feef43ef985afc7b67d0c8b8"]},"outputId":"e377d8c2-cf26-4069-ad5f-e4ed88c6c081"},"outputs":[{"output_type":"stream","name":"stderr","text":["Using custom data configuration danielhou13--cogs402datafake-f5349e6cf83e41d8\n","Reusing dataset parquet (/root/.cache/huggingface/datasets/danielhou13___parquet/danielhou13--cogs402datafake-f5349e6cf83e41d8/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d421873c349f401aa4432d85952c5ee4"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Loading cached processed dataset at /root/.cache/huggingface/datasets/danielhou13___parquet/danielhou13--cogs402datafake-f5349e6cf83e41d8/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-3b0f0b1006ba438f.arrow\n","Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['longformer_model.encoder.layer.8.attention.self.query.bias', 'longformer_model.encoder.layer.0.attention.self.query_global.bias', 'longformer_model.encoder.layer.0.attention.self.query.bias', 'longformer_model.encoder.layer.2.attention.output.dense.weight', 'longformer_model.encoder.layer.4.attention.self.value.weight', 'longformer_model.encoder.layer.4.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.9.attention.output.dense.bias', 'longformer_model.encoder.layer.3.output.LayerNorm.bias', 'longformer_model.encoder.layer.6.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.output.dense.bias', 'longformer_model.encoder.layer.6.attention.self.key.weight', 'longformer_model.encoder.layer.7.attention.self.query.bias', 'longformer_model.encoder.layer.7.attention.self.key_global.bias', 'longformer_model.encoder.layer.8.attention.self.value.weight', 'longformer_model.encoder.layer.10.attention.self.query.weight', 'longformer_model.encoder.layer.11.attention.self.value.bias', 'longformer_model.encoder.layer.1.output.dense.weight', 'dense.bias', 'longformer_model.encoder.layer.4.output.dense.bias', 'longformer_model.encoder.layer.9.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.6.output.dense.weight', 'longformer_model.encoder.layer.2.attention.self.key.weight', 'longformer_model.encoder.layer.11.attention.self.value_global.weight', 'dense.weight', 'longformer_model.encoder.layer.4.attention.output.dense.weight', 'longformer_model.encoder.layer.0.attention.self.query_global.weight', 'longformer_model.encoder.layer.4.intermediate.dense.bias', 'longformer_model.encoder.layer.4.attention.self.key.weight', 'longformer_model.encoder.layer.8.attention.output.dense.bias', 'longformer_model.encoder.layer.5.attention.self.query_global.weight', 'longformer_model.encoder.layer.1.intermediate.dense.bias', 'longformer_model.encoder.layer.10.attention.self.key_global.weight', 'longformer_model.encoder.layer.3.attention.output.dense.bias', 'longformer_model.encoder.layer.3.attention.self.query.weight', 'longformer_model.encoder.layer.3.intermediate.dense.bias', 'longformer_model.encoder.layer.0.attention.self.key_global.bias', 'longformer_model.encoder.layer.7.attention.self.value_global.bias', 'longformer_model.encoder.layer.10.attention.self.value.bias', 'longformer_model.encoder.layer.7.attention.self.key.bias', 'longformer_model.encoder.layer.10.attention.self.key_global.bias', 'longformer_model.encoder.layer.10.intermediate.dense.bias', 'longformer_model.encoder.layer.0.output.LayerNorm.weight', 'longformer_model.encoder.layer.8.attention.self.key.weight', 'longformer_model.embeddings.word_embeddings.weight', 'longformer_model.encoder.layer.6.attention.self.key_global.weight', 'longformer_model.encoder.layer.2.attention.self.key_global.weight', 'longformer_model.encoder.layer.1.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.self.value.bias', 'longformer_model.encoder.layer.7.attention.self.key.weight', 'longformer_model.encoder.layer.4.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.6.attention.self.key.bias', 'longformer_model.encoder.layer.2.attention.self.query.weight', 'longformer_model.encoder.layer.10.intermediate.dense.weight', 'longformer_model.encoder.layer.6.attention.self.value_global.weight', 'longformer_model.encoder.layer.8.attention.self.query.weight', 'longformer_model.encoder.layer.11.attention.self.query_global.weight', 'longformer_model.encoder.layer.3.output.dense.bias', 'longformer_model.encoder.layer.11.output.LayerNorm.weight', 'longformer_model.encoder.layer.0.attention.self.key.weight', 'longformer_model.encoder.layer.9.attention.self.query_global.weight', 'longformer_model.encoder.layer.9.intermediate.dense.bias', 'longformer_model.encoder.layer.4.attention.self.query.weight', 'longformer_model.encoder.layer.11.output.dense.weight', 'longformer_model.encoder.layer.0.output.dense.weight', 'longformer_model.encoder.layer.3.intermediate.dense.weight', 'longformer_model.encoder.layer.11.attention.self.value_global.bias', 'longformer_model.encoder.layer.0.attention.self.value_global.weight', 'longformer_model.encoder.layer.10.attention.self.key.weight', 'longformer_model.encoder.layer.1.attention.self.key_global.weight', 'longformer_model.encoder.layer.9.attention.self.key.bias', 'longformer_model.encoder.layer.9.output.LayerNorm.weight', 'longformer_model.embeddings.LayerNorm.bias', 'longformer_model.encoder.layer.11.intermediate.dense.weight', 'longformer_model.encoder.layer.6.attention.self.query_global.weight', 'longformer_model.encoder.layer.7.intermediate.dense.weight', 'longformer_model.encoder.layer.9.output.LayerNorm.bias', 'longformer_model.encoder.layer.2.attention.self.query.bias', 'longformer_model.encoder.layer.8.attention.self.query_global.weight', 'longformer_model.encoder.layer.4.output.LayerNorm.weight', 'longformer_model.encoder.layer.6.attention.self.value.weight', 'longformer_model.encoder.layer.0.attention.self.key.bias', 'longformer_model.encoder.layer.2.attention.self.query_global.weight', 'longformer_model.encoder.layer.10.attention.output.dense.weight', 'longformer_model.encoder.layer.1.attention.self.value_global.weight', 'longformer_model.encoder.layer.6.attention.self.query.bias', 'longformer_model.encoder.layer.6.output.dense.bias', 'longformer_model.encoder.layer.3.output.LayerNorm.weight', 'longformer_model.encoder.layer.11.attention.self.query_global.bias', 'longformer_model.encoder.layer.2.attention.output.dense.bias', 'longformer_model.encoder.layer.10.attention.output.dense.bias', 'fc.bias', 'longformer_model.encoder.layer.9.attention.self.key_global.weight', 'longformer_model.encoder.layer.4.attention.self.value_global.weight', 'longformer_model.encoder.layer.11.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.1.attention.output.dense.weight', 'longformer_model.encoder.layer.11.attention.self.key.bias', 'longformer_model.encoder.layer.8.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.9.attention.self.value_global.weight', 'longformer_model.encoder.layer.2.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.1.attention.self.value_global.bias', 'longformer_model.encoder.layer.1.attention.self.query_global.bias', 'longformer_model.encoder.layer.3.attention.self.query_global.bias', 'longformer_model.encoder.layer.5.output.LayerNorm.bias', 'longformer_model.encoder.layer.0.attention.self.value.weight', 'longformer_model.encoder.layer.5.attention.self.key_global.weight', 'longformer_model.encoder.layer.5.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.4.attention.self.value_global.bias', 'longformer_model.encoder.layer.1.attention.self.query.weight', 'longformer_model.encoder.layer.7.intermediate.dense.bias', 'longformer_model.encoder.layer.8.intermediate.dense.weight', 'longformer_model.encoder.layer.3.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.10.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.9.attention.self.key.weight', 'longformer_model.encoder.layer.8.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.8.attention.self.query_global.bias', 'longformer_model.encoder.layer.0.intermediate.dense.weight', 'longformer_model.encoder.layer.9.attention.output.dense.weight', 'longformer_model.encoder.layer.9.output.dense.weight', 'longformer_model.encoder.layer.2.attention.self.key_global.bias', 'longformer_model.encoder.layer.6.attention.self.value_global.bias', 'longformer_model.encoder.layer.7.attention.output.dense.bias', 'longformer_model.encoder.layer.3.attention.self.value_global.weight', 'longformer_model.encoder.layer.2.attention.self.value_global.bias', 'longformer_model.encoder.layer.0.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.1.attention.self.key.weight', 'fc.weight', 'longformer_model.encoder.layer.2.attention.self.value.bias', 'longformer_model.encoder.layer.0.attention.output.dense.weight', 'longformer_model.encoder.layer.6.intermediate.dense.bias', 'longformer_model.encoder.layer.9.attention.self.query.bias', 'longformer_model.encoder.layer.8.attention.output.dense.weight', 'longformer_model.encoder.layer.3.attention.output.dense.weight', 'longformer_model.encoder.layer.8.output.dense.weight', 'longformer_model.encoder.layer.1.attention.self.key_global.bias', 'longformer_model.encoder.layer.7.output.LayerNorm.bias', 'longformer_model.encoder.layer.4.attention.self.key_global.weight', 'longformer_model.encoder.layer.1.attention.self.value.bias', 'longformer_model.encoder.layer.10.attention.self.query_global.bias', 'longformer_model.encoder.layer.7.attention.self.query_global.bias', 'longformer_model.encoder.layer.3.attention.self.value.bias', 'longformer_model.encoder.layer.0.attention.self.key_global.weight', 'longformer_model.encoder.layer.0.attention.output.dense.bias', 'longformer_model.encoder.layer.6.output.LayerNorm.weight', 'longformer_model.encoder.layer.8.attention.self.value_global.bias', 'longformer_model.encoder.layer.0.attention.self.query.weight', 'longformer_model.encoder.layer.2.output.LayerNorm.weight', 'longformer_model.embeddings.position_ids', 'longformer_model.encoder.layer.9.attention.self.value.bias', 'longformer_model.encoder.layer.1.output.dense.bias', 'longformer_model.encoder.layer.5.intermediate.dense.weight', 'longformer_model.encoder.layer.6.attention.self.value.bias', 'longformer_model.encoder.layer.3.output.dense.weight', 'longformer_model.encoder.layer.11.attention.self.key.weight', 'longformer_model.encoder.layer.11.attention.self.value.weight', 'longformer_model.encoder.layer.11.attention.output.dense.weight', 'longformer_model.encoder.layer.2.output.LayerNorm.bias', 'longformer_model.encoder.layer.2.output.dense.weight', 'longformer_model.encoder.layer.4.attention.self.query_global.weight', 'longformer_model.embeddings.position_embeddings.weight', 'longformer_model.encoder.layer.9.attention.self.query.weight', 'longformer_model.encoder.layer.7.attention.self.query_global.weight', 'longformer_model.encoder.layer.11.attention.self.key_global.bias', 'longformer_model.encoder.layer.1.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.7.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.1.attention.output.dense.bias', 'longformer_model.encoder.layer.7.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.8.attention.self.key_global.weight', 'longformer_model.encoder.layer.7.attention.self.query.weight', 'longformer_model.encoder.layer.1.attention.self.value.weight', 'longformer_model.encoder.layer.11.attention.self.key_global.weight', 'longformer_model.encoder.layer.1.attention.self.query_global.weight', 'longformer_model.encoder.layer.9.attention.self.value_global.bias', 'longformer_model.encoder.layer.3.attention.self.key_global.bias', 'longformer_model.encoder.layer.1.attention.self.query.bias', 'longformer_model.encoder.layer.10.attention.self.value_global.bias', 'longformer_model.encoder.layer.2.output.dense.bias', 'longformer_model.encoder.layer.2.attention.self.value.weight', 'longformer_model.encoder.layer.8.intermediate.dense.bias', 'longformer_model.encoder.layer.9.attention.self.query_global.bias', 'longformer_model.encoder.layer.10.output.dense.weight', 'longformer_model.encoder.layer.5.attention.self.value_global.weight', 'longformer_model.encoder.layer.5.attention.self.value_global.bias', 'longformer_model.encoder.layer.1.intermediate.dense.weight', 'longformer_model.encoder.layer.4.attention.output.dense.bias', 'longformer_model.encoder.layer.6.intermediate.dense.weight', 'longformer_model.encoder.layer.0.attention.self.value_global.bias', 'longformer_model.encoder.layer.8.attention.self.value_global.weight', 'longformer_model.encoder.layer.10.attention.self.query_global.weight', 'longformer_model.encoder.layer.4.attention.self.key_global.bias', 'longformer_model.encoder.layer.3.attention.self.key.bias', 'longformer_model.encoder.layer.7.output.dense.weight', 'longformer_model.encoder.layer.5.output.dense.weight', 'longformer_model.encoder.layer.0.attention.self.value.bias', 'longformer_model.encoder.layer.4.output.dense.weight', 'longformer_model.encoder.layer.6.output.LayerNorm.bias', 'longformer_model.encoder.layer.10.output.dense.bias', 'longformer_model.encoder.layer.11.attention.self.query.bias', 'longformer_model.encoder.layer.9.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.11.attention.self.query.weight', 'longformer_model.encoder.layer.2.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.8.output.LayerNorm.weight', 'longformer_model.encoder.layer.8.attention.self.value.bias', 'longformer_model.encoder.layer.10.output.LayerNorm.weight', 'longformer_model.encoder.layer.5.attention.self.key_global.bias', 'longformer_model.encoder.layer.8.output.dense.bias', 'longformer_model.encoder.layer.5.attention.self.key.weight', 'longformer_model.encoder.layer.2.intermediate.dense.bias', 'longformer_model.encoder.layer.7.output.LayerNorm.weight', 'longformer_model.encoder.layer.4.attention.self.value.bias', 'longformer_model.encoder.layer.3.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.3.attention.self.query_global.weight', 'longformer_model.encoder.layer.3.attention.self.value.weight', 'longformer_model.encoder.layer.11.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.self.key.bias', 'longformer_model.encoder.layer.8.attention.self.key.bias', 'longformer_model.encoder.layer.0.output.LayerNorm.bias', 'longformer_model.encoder.layer.7.output.dense.bias', 'longformer_model.encoder.layer.6.attention.self.key_global.bias', 'longformer_model.encoder.layer.2.attention.self.query_global.bias', 'longformer_model.encoder.layer.7.attention.self.value_global.weight', 'longformer_model.encoder.layer.9.attention.self.value.weight', 'longformer_model.encoder.layer.9.intermediate.dense.weight', 'longformer_model.encoder.layer.4.intermediate.dense.weight', 'longformer_model.embeddings.LayerNorm.weight', 'longformer_model.encoder.layer.10.output.LayerNorm.bias', 'longformer_model.encoder.layer.10.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.10.attention.self.key.bias', 'longformer_model.encoder.layer.6.attention.output.dense.bias', 'longformer_model.encoder.layer.5.intermediate.dense.bias', 'longformer_model.encoder.layer.6.attention.output.dense.weight', 'longformer_model.encoder.layer.10.attention.self.value.weight', 'longformer_model.encoder.layer.7.attention.self.value.bias', 'longformer_model.encoder.layer.8.attention.self.key_global.bias', 'longformer_model.encoder.layer.5.attention.self.query.bias', 'longformer_model.encoder.layer.1.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.11.intermediate.dense.bias', 'longformer_model.encoder.layer.3.attention.self.value_global.bias', 'longformer_model.encoder.layer.1.output.LayerNorm.weight', 'longformer_model.encoder.layer.3.attention.self.key_global.weight', 'longformer_model.encoder.layer.4.attention.self.key.bias', 'longformer_model.encoder.layer.1.attention.self.key.bias', 'longformer_model.encoder.layer.4.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.output.dense.weight', 'longformer_model.encoder.layer.6.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.7.attention.output.dense.weight', 'longformer_model.encoder.layer.9.attention.self.key_global.bias', 'longformer_model.encoder.layer.3.attention.self.key.weight', 'longformer_model.encoder.layer.2.attention.self.value_global.weight', 'longformer_model.embeddings.token_type_embeddings.weight', 'longformer_model.encoder.layer.5.attention.self.value.weight', 'longformer_model.encoder.layer.2.attention.self.key.bias', 'longformer_model.encoder.layer.7.attention.self.key_global.weight', 'longformer_model.encoder.layer.4.attention.self.query.bias', 'longformer_model.encoder.layer.10.attention.self.value_global.weight', 'longformer_model.encoder.layer.11.output.dense.bias', 'longformer_model.encoder.layer.0.output.dense.bias', 'longformer_model.encoder.layer.8.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.output.dense.bias', 'longformer_model.encoder.layer.10.attention.self.query.bias', 'longformer_model.encoder.layer.0.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.3.attention.self.query.bias', 'longformer_model.encoder.layer.4.attention.self.query_global.bias', 'longformer_model.encoder.layer.5.attention.self.query.weight', 'longformer_model.encoder.layer.5.output.LayerNorm.weight', 'longformer_model.encoder.layer.6.attention.self.query.weight', 'longformer_model.encoder.layer.7.attention.self.value.weight', 'longformer_model.encoder.layer.11.attention.output.dense.bias', 'longformer_model.encoder.layer.6.attention.self.query_global.bias', 'longformer_model.encoder.layer.2.intermediate.dense.weight', 'longformer_model.encoder.layer.9.output.dense.bias', 'longformer_model.encoder.layer.5.attention.self.query_global.bias', 'longformer_model.encoder.layer.11.output.LayerNorm.bias', 'longformer_model.encoder.layer.0.intermediate.dense.bias']\n","- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['longformer.encoder.layer.10.output.LayerNorm.bias', 'longformer.encoder.layer.4.attention.output.LayerNorm.weight', 'longformer.encoder.layer.6.intermediate.dense.weight', 'longformer.encoder.layer.5.attention.self.query_global.weight', 'longformer.encoder.layer.3.output.dense.weight', 'longformer.encoder.layer.1.attention.self.key.bias', 'longformer.encoder.layer.0.output.dense.weight', 'longformer.encoder.layer.3.intermediate.dense.weight', 'longformer.encoder.layer.2.intermediate.dense.bias', 'longformer.encoder.layer.6.intermediate.dense.bias', 'longformer.encoder.layer.10.attention.output.dense.weight', 'longformer.encoder.layer.7.attention.output.LayerNorm.weight', 'longformer.encoder.layer.11.attention.self.value.weight', 'longformer.encoder.layer.4.output.LayerNorm.weight', 'longformer.encoder.layer.10.attention.self.value_global.bias', 'longformer.encoder.layer.9.attention.self.query.bias', 'longformer.encoder.layer.9.output.LayerNorm.bias', 'longformer.encoder.layer.2.output.LayerNorm.bias', 'longformer.encoder.layer.8.attention.self.value_global.weight', 'longformer.encoder.layer.2.output.dense.weight', 'longformer.encoder.layer.11.attention.self.query.weight', 'longformer.encoder.layer.11.attention.self.query.bias', 'longformer.encoder.layer.11.attention.self.value_global.bias', 'longformer.encoder.layer.5.output.LayerNorm.weight', 'longformer.encoder.layer.1.attention.self.key_global.weight', 'longformer.encoder.layer.8.attention.output.LayerNorm.weight', 'longformer.encoder.layer.6.attention.self.value_global.bias', 'longformer.encoder.layer.4.attention.self.value_global.bias', 'longformer.encoder.layer.8.attention.self.value_global.bias', 'longformer.encoder.layer.3.attention.output.LayerNorm.weight', 'longformer.encoder.layer.9.attention.self.key.weight', 'longformer.encoder.layer.9.attention.output.LayerNorm.weight', 'longformer.encoder.layer.9.attention.self.value.weight', 'longformer.encoder.layer.7.attention.self.query_global.weight', 'longformer.encoder.layer.2.attention.self.key_global.weight', 'longformer.encoder.layer.0.attention.self.key_global.weight', 'longformer.encoder.layer.9.attention.self.key_global.bias', 'longformer.encoder.layer.4.attention.self.query_global.weight', 'longformer.encoder.layer.2.attention.self.key.bias', 'longformer.encoder.layer.5.attention.self.key.bias', 'longformer.encoder.layer.5.attention.self.value_global.bias', 'longformer.encoder.layer.3.attention.self.key.weight', 'longformer.encoder.layer.9.attention.self.key_global.weight', 'longformer.encoder.layer.10.attention.self.query.bias', 'longformer.encoder.layer.1.output.LayerNorm.weight', 'longformer.encoder.layer.4.output.dense.bias', 'longformer.encoder.layer.7.output.dense.bias', 'longformer.encoder.layer.7.intermediate.dense.weight', 'longformer.encoder.layer.8.attention.output.dense.bias', 'longformer.encoder.layer.2.attention.self.key.weight', 'longformer.encoder.layer.9.output.LayerNorm.weight', 'longformer.encoder.layer.1.attention.output.LayerNorm.weight', 'longformer.encoder.layer.7.attention.self.key.bias', 'longformer.encoder.layer.1.attention.self.value.weight', 'longformer.encoder.layer.4.attention.self.value.bias', 'longformer.encoder.layer.4.attention.self.value_global.weight', 'longformer.encoder.layer.7.attention.self.key.weight', 'longformer.encoder.layer.6.output.dense.weight', 'longformer.encoder.layer.10.output.dense.bias', 'longformer.encoder.layer.4.attention.self.key_global.bias', 'longformer.encoder.layer.10.attention.output.LayerNorm.weight', 'longformer.encoder.layer.6.attention.self.query.bias', 'longformer.encoder.layer.0.output.dense.bias', 'longformer.encoder.layer.2.attention.output.LayerNorm.weight', 'longformer.embeddings.token_type_embeddings.weight', 'longformer.encoder.layer.6.output.LayerNorm.weight', 'longformer.encoder.layer.9.output.dense.bias', 'longformer.encoder.layer.11.attention.self.key_global.weight', 'longformer.encoder.layer.6.output.LayerNorm.bias', 'longformer.encoder.layer.2.attention.self.query_global.weight', 'longformer.encoder.layer.0.intermediate.dense.weight', 'longformer.embeddings.word_embeddings.weight', 'longformer.encoder.layer.1.attention.self.value.bias', 'longformer.encoder.layer.5.attention.self.query.bias', 'longformer.encoder.layer.0.attention.self.query.bias', 'longformer.encoder.layer.3.attention.self.query_global.bias', 'longformer.encoder.layer.5.attention.self.value.weight', 'longformer.encoder.layer.0.output.LayerNorm.weight', 'longformer.encoder.layer.5.attention.output.dense.weight', 'longformer.encoder.layer.11.attention.self.key.weight', 'longformer.encoder.layer.0.attention.self.value_global.weight', 'classifier.out_proj.bias', 'longformer.encoder.layer.4.intermediate.dense.bias', 'longformer.encoder.layer.11.attention.output.LayerNorm.weight', 'longformer.encoder.layer.2.attention.output.dense.weight', 'longformer.encoder.layer.2.attention.self.value_global.bias', 'longformer.encoder.layer.5.attention.self.value.bias', 'longformer.encoder.layer.10.attention.self.key.bias', 'longformer.encoder.layer.0.attention.self.value.weight', 'longformer.encoder.layer.0.attention.self.query_global.bias', 'longformer.encoder.layer.1.attention.self.query_global.bias', 'longformer.encoder.layer.0.attention.self.value.bias', 'longformer.encoder.layer.8.attention.self.key_global.weight', 'longformer.encoder.layer.0.output.LayerNorm.bias', 'longformer.encoder.layer.9.attention.self.value_global.bias', 'longformer.encoder.layer.10.intermediate.dense.weight', 'longformer.encoder.layer.4.intermediate.dense.weight', 'longformer.encoder.layer.11.attention.self.value_global.weight', 'longformer.encoder.layer.6.output.dense.bias', 'longformer.encoder.layer.1.attention.output.LayerNorm.bias', 'longformer.encoder.layer.10.attention.self.query_global.bias', 'longformer.encoder.layer.2.output.dense.bias', 'longformer.encoder.layer.7.attention.output.dense.weight', 'longformer.encoder.layer.1.intermediate.dense.weight', 'longformer.encoder.layer.5.attention.self.key.weight', 'longformer.encoder.layer.4.attention.self.key.bias', 'longformer.encoder.layer.11.attention.output.dense.bias', 'longformer.encoder.layer.10.attention.self.value_global.weight', 'longformer.encoder.layer.8.intermediate.dense.weight', 'longformer.encoder.layer.9.intermediate.dense.bias', 'longformer.encoder.layer.10.attention.self.value.bias', 'longformer.encoder.layer.6.attention.self.value.bias', 'longformer.encoder.layer.8.attention.self.key_global.bias', 'longformer.encoder.layer.9.attention.output.LayerNorm.bias', 'classifier.out_proj.weight', 'longformer.encoder.layer.0.attention.self.key.weight', 'longformer.encoder.layer.3.output.dense.bias', 'longformer.encoder.layer.3.attention.output.LayerNorm.bias', 'longformer.encoder.layer.9.attention.self.key.bias', 'longformer.encoder.layer.1.output.dense.bias', 'longformer.encoder.layer.10.attention.self.key_global.bias', 'longformer.encoder.layer.3.attention.self.key.bias', 'longformer.encoder.layer.8.output.dense.weight', 'longformer.encoder.layer.8.attention.output.dense.weight', 'longformer.encoder.layer.0.attention.output.dense.bias', 'longformer.encoder.layer.7.attention.self.value_global.weight', 'longformer.encoder.layer.8.attention.self.query_global.weight', 'longformer.encoder.layer.10.attention.self.query_global.weight', 'longformer.encoder.layer.6.attention.output.LayerNorm.bias', 'longformer.encoder.layer.4.attention.self.query_global.bias', 'longformer.encoder.layer.5.intermediate.dense.weight', 'longformer.encoder.layer.0.attention.self.query.weight', 'longformer.encoder.layer.1.attention.self.query.bias', 'longformer.encoder.layer.9.output.dense.weight', 'longformer.encoder.layer.11.attention.output.LayerNorm.bias', 'longformer.encoder.layer.5.attention.self.query_global.bias', 'longformer.encoder.layer.8.attention.self.query_global.bias', 'longformer.encoder.layer.6.attention.self.key_global.bias', 'longformer.encoder.layer.4.output.LayerNorm.bias', 'longformer.encoder.layer.0.intermediate.dense.bias', 'longformer.encoder.layer.11.attention.self.query_global.bias', 'longformer.encoder.layer.7.attention.self.value_global.bias', 'longformer.encoder.layer.0.attention.self.key.bias', 'longformer.encoder.layer.2.attention.self.query_global.bias', 'longformer.encoder.layer.4.attention.self.query.bias', 'longformer.encoder.layer.11.attention.self.key.bias', 'longformer.encoder.layer.5.attention.self.key_global.bias', 'longformer.encoder.layer.7.output.LayerNorm.weight', 'longformer.encoder.layer.1.attention.self.query_global.weight', 'longformer.encoder.layer.1.output.dense.weight', 'longformer.encoder.layer.10.attention.self.key_global.weight', 'longformer.encoder.layer.10.attention.self.value.weight', 'longformer.encoder.layer.5.output.LayerNorm.bias', 'longformer.encoder.layer.7.output.dense.weight', 'longformer.encoder.layer.10.attention.self.key.weight', 'longformer.encoder.layer.2.attention.self.value.bias', 'longformer.encoder.layer.6.attention.output.LayerNorm.weight', 'longformer.encoder.layer.7.attention.self.key_global.weight', 'longformer.encoder.layer.11.output.LayerNorm.weight', 'longformer.encoder.layer.3.attention.output.dense.bias', 'longformer.encoder.layer.1.attention.output.dense.bias', 'longformer.encoder.layer.2.attention.self.query.weight', 'longformer.encoder.layer.3.attention.self.value_global.weight', 'longformer.encoder.layer.9.attention.self.query_global.weight', 'longformer.encoder.layer.1.attention.self.value_global.weight', 'longformer.encoder.layer.7.intermediate.dense.bias', 'longformer.encoder.layer.3.attention.self.query.weight', 'longformer.encoder.layer.4.attention.output.dense.weight', 'longformer.encoder.layer.0.attention.self.value_global.bias', 'longformer.encoder.layer.6.attention.self.key.bias', 'longformer.encoder.layer.4.attention.self.query.weight', 'longformer.encoder.layer.2.attention.output.LayerNorm.bias', 'longformer.encoder.layer.8.output.LayerNorm.weight', 'longformer.encoder.layer.4.attention.self.key_global.weight', 'longformer.encoder.layer.8.attention.self.query.weight', 'longformer.encoder.layer.2.intermediate.dense.weight', 'longformer.encoder.layer.9.attention.self.value.bias', 'longformer.encoder.layer.1.attention.self.query.weight', 'longformer.encoder.layer.6.attention.self.key_global.weight', 'longformer.encoder.layer.9.attention.output.dense.bias', 'longformer.encoder.layer.10.attention.output.dense.bias', 'longformer.encoder.layer.3.attention.self.key_global.bias', 'longformer.encoder.layer.0.attention.self.query_global.weight', 'longformer.encoder.layer.8.attention.self.key.weight', 'longformer.encoder.layer.9.attention.self.value_global.weight', 'longformer.encoder.layer.10.output.LayerNorm.weight', 'longformer.encoder.layer.11.output.dense.weight', 'longformer.encoder.layer.1.attention.self.key_global.bias', 'longformer.encoder.layer.0.attention.output.dense.weight', 'longformer.encoder.layer.9.attention.self.query_global.bias', 'longformer.encoder.layer.3.attention.self.key_global.weight', 'longformer.encoder.layer.0.attention.self.key_global.bias', 'longformer.encoder.layer.8.attention.self.value.bias', 'longformer.embeddings.position_embeddings.weight', 'longformer.encoder.layer.6.attention.self.query_global.weight', 'longformer.encoder.layer.9.attention.output.dense.weight', 'longformer.encoder.layer.1.attention.self.key.weight', 'longformer.encoder.layer.1.intermediate.dense.bias', 'longformer.encoder.layer.6.attention.self.query_global.bias', 'longformer.encoder.layer.3.attention.self.value.bias', 'longformer.encoder.layer.7.attention.self.key_global.bias', 'longformer.encoder.layer.11.attention.self.key_global.bias', 'longformer.encoder.layer.3.attention.self.value.weight', 'longformer.encoder.layer.4.attention.output.dense.bias', 'longformer.encoder.layer.1.attention.self.value_global.bias', 'longformer.encoder.layer.5.intermediate.dense.bias', 'longformer.encoder.layer.8.intermediate.dense.bias', 'longformer.encoder.layer.7.attention.output.dense.bias', 'longformer.encoder.layer.8.attention.self.key.bias', 'longformer.encoder.layer.11.attention.self.value.bias', 'longformer.encoder.layer.7.output.LayerNorm.bias', 'longformer.encoder.layer.2.attention.self.value.weight', 'longformer.encoder.layer.11.output.LayerNorm.bias', 'longformer.encoder.layer.5.attention.self.value_global.weight', 'longformer.encoder.layer.1.attention.output.dense.weight', 'longformer.encoder.layer.5.output.dense.bias', 'longformer.encoder.layer.3.output.LayerNorm.bias', 'longformer.encoder.layer.6.attention.output.dense.weight', 'longformer.encoder.layer.3.attention.output.dense.weight', 'longformer.encoder.layer.7.attention.output.LayerNorm.bias', 'longformer.encoder.layer.5.attention.output.LayerNorm.weight', 'longformer.encoder.layer.8.output.dense.bias', 'longformer.embeddings.LayerNorm.bias', 'longformer.encoder.layer.10.attention.output.LayerNorm.bias', 'longformer.encoder.layer.3.intermediate.dense.bias', 'longformer.encoder.layer.8.attention.self.query.bias', 'longformer.encoder.layer.4.attention.self.key.weight', 'longformer.encoder.layer.4.output.dense.weight', 'longformer.encoder.layer.5.attention.self.key_global.weight', 'longformer.encoder.layer.11.attention.self.query_global.weight', 'longformer.encoder.layer.6.attention.self.value.weight', 'longformer.encoder.layer.7.attention.self.value.weight', 'longformer.encoder.layer.2.attention.self.query.bias', 'longformer.encoder.layer.8.attention.output.LayerNorm.bias', 'longformer.encoder.layer.7.attention.self.query.bias', 'longformer.encoder.layer.10.intermediate.dense.bias', 'longformer.encoder.layer.9.intermediate.dense.weight', 'longformer.encoder.layer.2.attention.self.key_global.bias', 'longformer.encoder.layer.5.attention.output.LayerNorm.bias', 'longformer.encoder.layer.5.output.dense.weight', 'longformer.encoder.layer.7.attention.self.query_global.bias', 'longformer.encoder.layer.6.attention.self.query.weight', 'longformer.encoder.layer.7.attention.self.value.bias', 'longformer.encoder.layer.3.attention.self.query.bias', 'longformer.encoder.layer.6.attention.self.key.weight', 'longformer.encoder.layer.3.attention.self.query_global.weight', 'longformer.encoder.layer.4.attention.output.LayerNorm.bias', 'longformer.encoder.layer.10.attention.self.query.weight', 'longformer.encoder.layer.6.attention.output.dense.bias', 'longformer.encoder.layer.10.output.dense.weight', 'longformer.encoder.layer.7.attention.self.query.weight', 'longformer.encoder.layer.2.output.LayerNorm.weight', 'longformer.encoder.layer.3.attention.self.value_global.bias', 'longformer.encoder.layer.3.output.LayerNorm.weight', 'longformer.embeddings.LayerNorm.weight', 'longformer.encoder.layer.8.output.LayerNorm.bias', 'longformer.encoder.layer.2.attention.output.dense.bias', 'longformer.encoder.layer.5.attention.self.query.weight', 'longformer.encoder.layer.4.attention.self.value.weight', 'longformer.encoder.layer.11.intermediate.dense.weight', 'longformer.encoder.layer.5.attention.output.dense.bias', 'longformer.encoder.layer.0.attention.output.LayerNorm.bias', 'longformer.encoder.layer.9.attention.self.query.weight', 'longformer.encoder.layer.8.attention.self.value.weight', 'longformer.encoder.layer.11.intermediate.dense.bias', 'longformer.encoder.layer.6.attention.self.value_global.weight', 'longformer.encoder.layer.11.attention.output.dense.weight', 'longformer.encoder.layer.11.output.dense.bias', 'longformer.encoder.layer.2.attention.self.value_global.weight', 'longformer.encoder.layer.1.output.LayerNorm.bias', 'longformer.encoder.layer.0.attention.output.LayerNorm.weight', 'classifier.dense.weight', 'classifier.dense.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"stream","name":"stdout","text":["['input_ids', 'attention_mask', 'labels']\n"]}],"source":["cogs402_test = papers_train_set()\n","model = longformer_finetuned_papers()\n","columns = cogs402_test.input_columns + cogs402_test.target_columns\n","print(columns)\n","cogs402_test.set_format(type='torch', columns=columns)\n","cogs402_test=cogs402_test.remove_columns(['text'])"]},{"cell_type":"markdown","source":["Load news model and dataset and preprocess it"],"metadata":{"id":"22BIYrX8NOVC"},"id":"22BIYrX8NOVC"},{"cell_type":"code","source":["# cogs402_test = news_test_set()\n","# model = longformer_finetuned_news(model_path2)\n","# columns = cogs402_test.input_columns + cogs402_test.target_columns\n","# print(columns)\n","# cogs402_test.set_format(type='torch', columns=columns)\n","# cogs402_test=cogs402_test.remove_columns(['text'])"],"metadata":{"id":"xh-YZZyo_eZx"},"id":"xh-YZZyo_eZx","execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"id":"deaabfa2-0855-41fd-870c-7a8b91e32d44","metadata":{"id":"deaabfa2-0855-41fd-870c-7a8b91e32d44","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658960551024,"user_tz":420,"elapsed":331,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"4e5a5d74-a2b8-4536-e093-f71e36df40a8"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["if torch.cuda.is_available():\n","    model = model.cuda()\n","\n","print(model.device)"]},{"cell_type":"markdown","source":["## Predict over the dataset"],"metadata":{"id":"lUXRli8TIIwm"},"id":"lUXRli8TIIwm"},{"cell_type":"markdown","source":["Predict using the model on the selected dataset using the [Huggingface trainer](https://huggingface.co/docs/transformers/main_classes/trainer) API."],"metadata":{"id":"IXjbkX-PAGay"},"id":"IXjbkX-PAGay"},{"cell_type":"code","source":["from transformers import Trainer, TrainingArguments\n","\n","batch_size = 1\n","gradient_acc = 4\n","model_name = f\"longformer-finetuned_papers\"\n","training_args = TrainingArguments(output_dir=f\"models/{model_name}\",\n","                                  num_train_epochs = 2,\n","                                  learning_rate=2e-5,\n","                                  per_device_train_batch_size=batch_size,\n","                                  per_device_eval_batch_size=batch_size,\n","                                  weight_decay=0.01,\n","                                  evaluation_strategy=\"epoch\",\n","                                  disable_tqdm=False,\n","                                  push_to_hub=False,\n","                                  log_level=\"error\",\n","                                  fp16=True,\n","                                  gradient_accumulation_steps=gradient_acc,\n","                                  gradient_checkpointing=True,\n","                                  save_strategy = \"epoch\")"],"metadata":{"id":"7PUTFonI_XLM"},"id":"7PUTFonI_XLM","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["F1 and accuracy are good general metrics for model performance. Recall and precision can be used if desired."],"metadata":{"id":"NSxE_HDPAKaT"},"id":"NSxE_HDPAKaT"},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score, f1_score\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    f1 = f1_score(labels, preds, average=\"weighted\")\n","    acc = accuracy_score(labels, preds)\n","    return {\"accuracy\": acc, \"f1\": f1}"],"metadata":{"id":"65V97tOz_e37"},"id":"65V97tOz_e37","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Place the finishing touches on our trainer, passing in the arguments, model, metrics, and datacollator (which doesn't really matter here as we pass in one item at a time)."],"metadata":{"id":"SmeWbe1hEpjD"},"id":"SmeWbe1hEpjD"},{"cell_type":"code","source":["from transformers import DataCollatorWithPadding\n","data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    tokenizer=tokenizer,\n","    data_collator = data_collator\n",")"],"metadata":{"id":"TpkwUmWN_lu1"},"id":"TpkwUmWN_lu1","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here we predict over the entire validation set."],"metadata":{"id":"J_rg0KRlE2PU"},"id":"J_rg0KRlE2PU"},{"cell_type":"code","source":["preds_output = trainer.predict(cogs402_test)"],"metadata":{"id":"4W9kCODH_wfT","colab":{"base_uri":"https://localhost:8080/","height":17},"executionInfo":{"status":"ok","timestamp":1658960556656,"user_tz":420,"elapsed":2427,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"ce1b78ef-d72a-4730-bca2-5b1d57276ce3"},"id":"4W9kCODH_wfT","execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":[""]},"metadata":{}}]},{"cell_type":"markdown","source":["## Picking Examples"],"metadata":{"id":"wj5dQazdIbxH"},"id":"wj5dQazdIbxH"},{"cell_type":"markdown","source":["False negatives and false postives are usually very interesting examples to analyze so to get the list of all false negatives and positives, we get our model's predictions and the list of true labels."],"metadata":{"id":"wjeyImJmApjd"},"id":"wjeyImJmApjd"},{"cell_type":"code","source":["y_preds = np.argmax(preds_output.predictions, axis=1)\n","y_true = np.array(cogs402_test[\"labels\"])"],"metadata":{"id":"IyRQ0z-QAzAU"},"id":"IyRQ0z-QAzAU","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can get the list of false negatives and false positives by subtracting the list of labels. \n","\n","\n","If, after subtracting, the list is 0, then we have a correct prediction as the two labels are the same. We can then filter by the value of the labels to get the positive and negative class. \n","\n","\n","If, after subtracting the false label from the true label, we have a negative, then we know that the actual label is 0 while the predicted label is 1 (as 0-1 is -1). Therefore, we get a false positive in that case. \n","\n","\n","On the other hand, if after subtracting the false label from the true label, we get a positive, then we know that the actual label is 1 while the predicted label is 0 (as 1-0 is 1). Therefore, we have a false negative."],"metadata":{"id":"T9iF_01WFrZL"},"id":"T9iF_01WFrZL"},{"cell_type":"code","source":["diff = y_true-y_preds\n","correct = np.where(diff == 0)[0]\n","\n","pos = np.where((y_true-y_preds == 0) & (y_true==1))[0]\n","neg = np.where((y_true-y_preds == 0) & (y_true==0))[0]\n","\n","false_pos = np.where(diff == -1)[0]\n","false_neg = np.where(diff == 1)[0]\n","\n","print('Correctly classified: ', correct)\n","\n","print('cor pos: ', pos)\n","print('cor neg: ', neg)\n","\n","print('False positives: ', false_pos)\n","print('False negatives: ', false_neg)"],"metadata":{"id":"ZoIa9_KIA2rt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658960556657,"user_tz":420,"elapsed":10,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"dd6fc9f6-2685-4dbd-8c29-044a983553d9"},"id":"ZoIa9_KIA2rt","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Correctly classified:  [ 9 10 11]\n","cor pos:  []\n","cor neg:  [ 9 10 11]\n","False positives:  []\n","False negatives:  [0 1 2 3 4 5 6 7 8]\n"]}]},{"cell_type":"markdown","source":["Take example for evaluation based on random pick"],"metadata":{"id":"d4DdhW2T6wHE"},"id":"d4DdhW2T6wHE"},{"cell_type":"code","execution_count":null,"id":"a35e74a3-bd67-4ee2-8e5b-2da01503f27b","metadata":{"id":"a35e74a3-bd67-4ee2-8e5b-2da01503f27b"},"outputs":[],"source":["# rand_pos = np.random.choice(pos, size=1)\n","# rand_neg = np.random.choice(neg, size=1)\n","# rand_fp = np.random.choice(false_pos, size=1)\n","# rand_fn = np.random.choice(false_neg, size=1)"]},{"cell_type":"markdown","source":["Some other interesting examples include the examples that are the most confidently predicted to be positive or negative. (i.e. the examples with the highest predicted probability)"],"metadata":{"id":"kyVEhUfidwRV"},"id":"kyVEhUfidwRV"},{"cell_type":"code","source":["highest_pos = [np.argmax(preds_output.predictions[:,1])]\n","highest_neg = [np.argmax(preds_output.predictions[:,0])]\n","\n","# for news dataset\n","# highest_neg = [np.argmax(np.delete(preds_output.predictions, 1933, 0)[:,0])]\n","\n","print(highest_pos)\n","print(highest_neg)"],"metadata":{"id":"6WWpe52Cd3wk","executionInfo":{"status":"ok","timestamp":1658960570326,"user_tz":420,"elapsed":281,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"3aeda4ff-a133-42fb-c9b9-ba22db9b545d"},"id":"6WWpe52Cd3wk","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[10]\n","[7]\n"]}]},{"cell_type":"markdown","source":["## Getting the attention"],"metadata":{"id":"RIeTsarXI7I9"},"id":"RIeTsarXI7I9"},{"cell_type":"markdown","source":["Now that we have the example we want to visualize the attentions for, we pass it into the model again in order to obtain the attention output. We stack the attentions to get an output attention tensor of shape: (layer, batch, head, seq_len, x + attention_window + 1) and a global attention tensor of shape (layer, batch, head, seq_len, x) where x is the number of global attention tokens."],"metadata":{"id":"xctV7QJdJEDe"},"id":"xctV7QJdJEDe"},{"cell_type":"code","source":["test_val = [7]\n","print(test_val)\n","testexam = cogs402_test[test_val]"],"metadata":{"id":"FrX3LUUxeORB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658960588424,"user_tz":420,"elapsed":297,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"e1c6efb0-bbc9-4897-8b9e-124dcbdc6311"},"id":"FrX3LUUxeORB","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[7]\n"]}]},{"cell_type":"code","execution_count":null,"id":"04c5bcaf-5fe3-4813-8444-5cfb2c63a92b","metadata":{"id":"04c5bcaf-5fe3-4813-8444-5cfb2c63a92b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658960589183,"user_tz":420,"elapsed":421,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"e1c56bd9-1c52-417b-d742-f754f48d7522"},"outputs":[{"output_type":"stream","name":"stdout","text":["output_attention.shape torch.Size([12, 1, 12, 2046, 514])\n","gl_output_attention.shape torch.Size([12, 1, 12, 2048, 1])\n"]}],"source":["output = model(testexam[\"input_ids\"].cuda(), attention_mask=testexam['attention_mask'].cuda(), labels=testexam['labels'].cuda(), output_attentions = True)\n","batch_attn = output[-2]\n","output_attentions = torch.stack(batch_attn).cpu()\n","global_attention = output[-1]\n","output_global_attentions = torch.stack(global_attention).cpu()\n","print(\"output_attention.shape\", output_attentions.shape)\n","print(\"gl_output_attention.shape\", output_global_attentions.shape)"]},{"cell_type":"code","source":["print(testexam['labels'][0])\n","print(output[1].argmax())"],"metadata":{"id":"SO4yNy98t_UP","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658960589183,"user_tz":420,"elapsed":4,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"e55036a3-893f-45b0-ec01-7886817d2c8a"},"id":"SO4yNy98t_UP","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1)\n","tensor(0, device='cuda:0')\n"]}]},{"cell_type":"code","execution_count":null,"id":"2467c3d1-fe58-4e6d-aa98-534a6df72fcc","metadata":{"id":"2467c3d1-fe58-4e6d-aa98-534a6df72fcc"},"outputs":[],"source":["# print(os.getcwd())\n","# yes = torch.load(\"resources/longformer_test2/epoch_3/aggregate_attn.pt\")"]},{"cell_type":"markdown","source":["A unique property of the longformer model is that the matrix output for the attention is not a seq_len x seq_len output. Each token can only attend to the preceeding w/2 tokens and the succeeding w/2 tokens, dictated by whatever you choose the model's attention window w to be. Another name for this is called the sliding window attention. Therefore, we need to convert sliding attention matrix to correct seq_len x seq_len matrix to remain consistent with other types of Transformer Neural Networks.\n","\n","To do so, we run the following 4 functions. Our attentions will change from an output attention tensor of shape (layer, batch, head, seq_len, x + attention_window + 1) and a global attention tensor of shape (layer, batch, head, seq_len, x) to a single tensor of shape (layer, batch, head, seq_len, seq_len). More information about the functions can be found here. More information about the functions can be found [here](https://colab.research.google.com/drive/1Kxx26NtIlUzioRCHpsR8IbSz_DpRFxEZ#scrollTo=liVhkxiH9Le0)."],"metadata":{"id":"OevnNprR67LK"},"id":"OevnNprR67LK"},{"cell_type":"code","execution_count":null,"id":"0882a3e5-1b0c-4319-92a3-92f5b516d854","metadata":{"id":"0882a3e5-1b0c-4319-92a3-92f5b516d854"},"outputs":[],"source":["def create_head_matrix(output_attentions, global_attentions):\n","    new_attention_matrix = torch.zeros((output_attentions.shape[0], \n","                                      output_attentions.shape[0]))\n","    for i in range(output_attentions.shape[0]):\n","        test_non_zeroes = torch.nonzero(output_attentions[i]).squeeze()\n","        test2 = output_attentions[i][test_non_zeroes[1:]]\n","        new_attention_matrix_indices = test_non_zeroes[1:]-257 + i\n","        new_attention_matrix[i][new_attention_matrix_indices] = test2\n","        new_attention_matrix[i][0] = output_attentions[i][0]\n","        new_attention_matrix[0] = global_attentions.squeeze()[:output_attentions.shape[0]]\n","    return new_attention_matrix.detach().cpu().numpy()\n","\n","\n","def attentions_all_heads(output_attentions, global_attentions):\n","    new_matrix = []\n","    for i in range(output_attentions.shape[0]):\n","        matrix = create_head_matrix(output_attentions[i], global_attentions[i])\n","        new_matrix.append(matrix)\n","    return np.stack(new_matrix)\n","\n","\n","def all_batches(output_attentions, global_attentions):\n","    new_matrix = []\n","    for i in range(output_attentions.shape[0]):\n","        matrix = attentions_all_heads(output_attentions[i], global_attentions[i])\n","        new_matrix.append(matrix)\n","    return np.stack(new_matrix)\n","\n","def all_layers(output_attentions, global_attentions):\n","    new_matrix = []\n","    for i in range(output_attentions.shape[0]):\n","        matrix = all_batches(output_attentions[i], global_attentions[i])\n","        new_matrix.append(matrix)\n","    return np.stack(new_matrix)"]},{"cell_type":"code","source":["converted_mat = all_layers(output_attentions, output_global_attentions)\n","print(converted_mat.shape)"],"metadata":{"id":"IpdfMEMAuvyR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658960623787,"user_tz":420,"elapsed":34308,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"f1692c3f-7961-4334-e7d4-481e61c3f2b1"},"id":"IpdfMEMAuvyR","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(12, 1, 12, 2046, 2046)\n"]}]},{"cell_type":"markdown","source":["## Formatting the attentions"],"metadata":{"id":"OA32VVs6KF-M"},"id":"OA32VVs6KF-M"},{"cell_type":"markdown","source":["Our end goal is to overlay the attentions onto the tokens and produce a PDF of the results, so we need to grab the original tokens from the text. We cant grab the original text as it is one large string, but using the tokenizer function, we can change our input ids back to a list of tokens."],"metadata":{"id":"Sl4xm-JGKKvt"},"id":"Sl4xm-JGKKvt"},{"cell_type":"code","source":["all_tokens = tokenizer.convert_ids_to_tokens(testexam[\"input_ids\"][0])"],"metadata":{"id":"N-Wr_p4LsTv-"},"id":"N-Wr_p4LsTv-","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Some heads may be more important than others so we scale each attention matrix by their respective head and layer. The notebook used to get head importance is [here](https://colab.research.google.com/drive/1O4QCi8ewBp7asegKqySRflTQZ9HeH8mQ?usp=sharing)."],"metadata":{"id":"P-nS_AHa7Hv6"},"id":"P-nS_AHa7Hv6"},{"cell_type":"code","source":["# head_importance = torch.load(\"/content/drive/MyDrive/fakeclinicalnotes/t3-visapplication/notes/head_importance.pt\")\n","head_importance = torch.load(\"/content/drive/MyDrive/cogs402longformer/t3-visapplication/resources/notes/head_importance.pt\") "],"metadata":{"id":"UsAznmcDyBgR"},"id":"UsAznmcDyBgR","execution_count":null,"outputs":[]},{"cell_type":"code","source":["def scale_by_importance(attention_matrix, head_importance):\n","  new_matrix = np.zeros_like(attention_matrix)\n","  for i in range(attention_matrix.shape[0]):\n","    head_importance_layer = head_importance[i]\n","    for j in range(attention_matrix.shape[1]):\n","      new_matrix[i,j] = attention_matrix[i,j] * np.expand_dims(head_importance_layer, axis=(1,2))\n","  return new_matrix"],"metadata":{"id":"_HFs_vLw0230"},"id":"_HFs_vLw0230","execution_count":null,"outputs":[]},{"cell_type":"code","source":["converted_mat_importance = scale_by_importance(converted_mat, head_importance)"],"metadata":{"id":"XUcjPfxeHbqT"},"id":"XUcjPfxeHbqT","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Get the sum of the attentions for all the tokens (column-wise). In other words, find out how much every word is attended to"],"metadata":{"id":"u0ViKJAn7Ap0"},"id":"u0ViKJAn7Ap0"},{"cell_type":"code","source":["attention_matrix_importance = converted_mat_importance.sum(axis=3)\n","print(attention_matrix_importance.shape)"],"metadata":{"id":"EhMNdupbxFrx","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658960628539,"user_tz":420,"elapsed":341,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"83cb7808-3534-4ed1-953e-2b2251d25c61"},"id":"EhMNdupbxFrx","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(12, 1, 12, 2046)\n"]}]},{"cell_type":"markdown","source":["## Visualizing the Attention"],"metadata":{"id":"BvnR9GJi13oP"},"id":"BvnR9GJi13oP"},{"cell_type":"markdown","source":["A dataframe is good for picking out information from the example, but it isn't the best being a easy to read visualization. Its easier to see how much each word is attended to in an example if we have the actual example, with the words highlighted based on the magnitude of attention.\n","\n","We use https://github.com/jiesutd/Text-Attention-Heatmap-Visualization to show how much each token in the example is attended to, up to the max number of tokens we specified earlier.\n","\n","In short, these functions iterate over the list of attentions and tokens, cleans the tokens to remove special characters, and normalizes the data if you wish for it to."],"metadata":{"id":"DjTu0_guLI1T"},"id":"DjTu0_guLI1T"},{"cell_type":"code","source":["## convert the text/attention list to latex code, which will further generates the text heatmap based on attention weights.\n","import numpy as np\n","\n","latex_special_token = [\"!@#$%^&*(){}\"]\n","\n","def generate(text_list, attention_list, latex_file, color='red', rescale_value = True):\n","\tassert(len(text_list) == len(attention_list))\n","\tif rescale_value:\n","\t\tattention_list = rescale(attention_list)\n","\tword_num = len(text_list)\n","\ttext_list = clean_word(text_list)\n","\twith open(latex_file,'w') as f:\n","\t\tf.write(r'''\\documentclass[varwidth]{standalone}\n","\\special{papersize=210mm,297mm}\n","\\usepackage{color}\n","\\usepackage{tcolorbox}\n","\\usepackage{CJK}\n","\\usepackage{adjustbox}\n","\\tcbset{width=0.9\\textwidth,boxrule=0pt,colback=red,arc=0pt,auto outer arc,left=0pt,right=0pt,boxsep=5pt}\n","\\begin{document}\n","\\begin{CJK*}{UTF8}{gbsn}'''+'\\n')\n","\t\tstring = r'''{\\setlength{\\fboxsep}{0pt}\\colorbox{white!0}{\\parbox{0.9\\textwidth}{'''+\"\\n\"\n","\t\tfor idx in range(word_num):\n","\t\t\tstring += \"\\\\colorbox{%s!%s}{\"%(color, attention_list[idx])+\"\\\\strut \" + text_list[idx]+\"} \"\n","\t\tstring += \"\\n}}}\"\n","\t\tf.write(string+'\\n')\n","\t\tf.write(r'''\\end{CJK*}\n","\\end{document}''')\n","\n","def rescale(input_list):\n","\tthe_array = np.asarray(input_list)\n","\tthe_max = np.max(the_array)\n","\tthe_min = np.min(the_array)\n","\trescale = ((the_array - the_min)/(the_max-the_min))*100\n","\treturn rescale.tolist()\n","\n","\n","def clean_word(word_list):\n","\tnew_word_list = []\n","\tfor word in word_list:\n","\t\tfor special_sensitive in [\"\\\\\", \"^\"]:\n","\t\t\tif special_sensitive in word:\n","\t\t\t\tword = word.replace(special_sensitive, '')\n","\t\tfor latex_sensitive in [\"%\", \"&\", \"#\", \"_\",  \"{\", \"}\"]:\n","\t\t\tif latex_sensitive in word:\n","\t\t\t\tword = word.replace(latex_sensitive, '\\\\' +latex_sensitive)\n","\t\tnew_word_list.append(word)\n","\treturn new_word_list"],"metadata":{"id":"FCsAm5i3jmKA"},"id":"FCsAm5i3jmKA","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Here, we sum get the attentions over all layers and heads."],"metadata":{"id":"chtfdhR_LmE2"},"id":"chtfdhR_LmE2"},{"cell_type":"code","source":["average_attention = attention_matrix_importance.squeeze().sum(axis=1)\n","average_attention = average_attention.sum(axis=0)\n","print(average_attention)"],"metadata":{"id":"goaO9arYjopz","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658960628540,"user_tz":420,"elapsed":6,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"65ee7fc9-cb8a-4abf-f6dd-2d6eab5cd01e"},"id":"goaO9arYjopz","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[266.9439    43.682247  43.542274 ...  44.49121   43.936462  44.018005]\n"]}]},{"cell_type":"markdown","source":["We call the main function above. It takes in a list of tokens, a list of attentions, a title, and a colour. Please change \"papers\" to whatever your project requires."],"metadata":{"id":"Hzic3AuULynV"},"id":"Hzic3AuULynV"},{"cell_type":"code","source":["title_all = f\"notes_{test_val[0]}.tex\"\n","generate(all_tokens, average_attention, title_all, 'red')"],"metadata":{"id":"htPtu7kErWfN"},"id":"htPtu7kErWfN","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Lets suppose you don't want to find out the attentions over all layers, but just one layer. You can do that by doing one less summation and instead picking out the layer you want immediately. Here we are picking out the last layer."],"metadata":{"id":"TCKn9M3ILsEh"},"id":"TCKn9M3ILsEh"},{"cell_type":"code","source":["print(attention_matrix_importance[11].squeeze().shape)\n","average_attention_final_layer = attention_matrix_importance[11].squeeze().sum(axis=0)\n","print(average_attention_final_layer)\n","\n","# mean_12 = np.median(average_attention_final_layer)\n","# average_attention_final_layer[average_attention_final_layer < mean_12] = 0\n","# print(average_attention_final_layer)"],"metadata":{"id":"GaM018NVunZH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1658960628540,"user_tz":420,"elapsed":4,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"eca3f19c-0c87-4c48-f5ec-a159bcccb804"},"id":"GaM018NVunZH","execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(12, 2046)\n","[8.128963  1.2755948 1.462493  ... 1.4194398 1.2911068 1.3994944]\n"]}]},{"cell_type":"markdown","source":["We call the main function above. Please change \"papers\" to whatever your project requires."],"metadata":{"id":"Cq8g9U_RML8Y"},"id":"Cq8g9U_RML8Y"},{"cell_type":"code","source":["title_last_layer = f\"notes_{test_val[0]}_layer_12_only.tex\"\n","generate(all_tokens, average_attention_final_layer, title_last_layer, 'red')"],"metadata":{"id":"fSl21NvVyW-n"},"id":"fSl21NvVyW-n","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Of course, you can experiment with which layers, or heads you want to visualize the attentions for based on what you desire from your own project."],"metadata":{"id":"46NQc1FGMTf6"},"id":"46NQc1FGMTf6"}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"colab":{"name":"Token_attention_with_head_importance_pdf_notes.ipynb","provenance":[{"file_id":"1Gyxj9rP2KnnCzit9zN3h3MeTTiQ13R7B","timestamp":1658867796010},{"file_id":"1iVojJQp0CZS484tMZqIizosXPLxgKvRX","timestamp":1656448374773},{"file_id":"1QaArRBpiPUWB-xqhdHQF0RuYpOmElf0x","timestamp":1655855819552}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU","gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"d421873c349f401aa4432d85952c5ee4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_768f49c6c3d143a9a7378a3449ceb45f","IPY_MODEL_816aac2a8b994e8b9b195294b1037e6b","IPY_MODEL_688a23135e534c58b42a2a93e896be59"],"layout":"IPY_MODEL_0df6a3c497274319a9d994942d4bf2de"}},"768f49c6c3d143a9a7378a3449ceb45f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b5b0ba6284714cce9c11a1cecbd1a566","placeholder":"​","style":"IPY_MODEL_900b155be3964b8b8ca7b1409d28fd62","value":"100%"}},"816aac2a8b994e8b9b195294b1037e6b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_86d7eef078724817ad8de66c20480d24","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8d0ae94ed1b14b97a64f83f9a65f1205","value":1}},"688a23135e534c58b42a2a93e896be59":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f12296c17dc04b799cb8ce6c094200f7","placeholder":"​","style":"IPY_MODEL_f07c3437feef43ef985afc7b67d0c8b8","value":" 1/1 [00:00&lt;00:00, 25.24it/s]"}},"0df6a3c497274319a9d994942d4bf2de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b5b0ba6284714cce9c11a1cecbd1a566":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"900b155be3964b8b8ca7b1409d28fd62":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86d7eef078724817ad8de66c20480d24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d0ae94ed1b14b97a64f83f9a65f1205":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f12296c17dc04b799cb8ce6c094200f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f07c3437feef43ef985afc7b67d0c8b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":5}