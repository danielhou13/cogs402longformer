{"cells":[{"cell_type":"markdown","source":["This notebook explores the relation between the model's attributions and attentions for a given example. Historically, we found that attentions are not a feasible method of explanation whereas attributions are, but attributions are also not part of a model's traditional outputs. Therefore it may be interesting to see if we can find anything with attentions by comparing them to a feasible and plausible method of explanation. We also apply masking to various scenarios to examine their effects on similarity. \n","\n","This notebook is very similar to this [notebook](https://colab.research.google.com/drive/14a1tOimrRbLlXd0rbtY-UTcizR5GDd4F) and serves to present a priminary findings of what examples are interesting by iterating through the entire dataset using much few steps when computing [Integrated Gradients](https://arxiv.org/abs/1703.01365). The output of this notebook is a .csv file containing the similarities of different attention layers."],"metadata":{"id":"mlqrlFd9SzDV"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"4G07aiA37RC9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659512412428,"user_tz":420,"elapsed":163256,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"229a5046-20d2-401b-c117-6ef5a03fde45"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Import dependencies"],"metadata":{"id":"HkZ5bD2_z-0C"}},{"cell_type":"code","source":["pip install transformers --quiet"],"metadata":{"id":"zMjzQIFJ2P_T","executionInfo":{"status":"ok","timestamp":1659512421932,"user_tz":420,"elapsed":9508,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ccff006a-f5ed-45a0-89df-388cbb08a9f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 4.7 MB 5.1 MB/s \n","\u001b[K     |████████████████████████████████| 6.6 MB 39.6 MB/s \n","\u001b[K     |████████████████████████████████| 101 kB 11.1 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 80.1 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","source":["pip install captum --quiet"],"metadata":{"id":"Uno0qwr12UTd","executionInfo":{"status":"ok","timestamp":1659512425523,"user_tz":420,"elapsed":3608,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a56811c3-e604-4b87-b192-b9cff6b0276e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l\r\u001b[K     |▎                               | 10 kB 29.0 MB/s eta 0:00:01\r\u001b[K     |▌                               | 20 kB 34.1 MB/s eta 0:00:01\r\u001b[K     |▊                               | 30 kB 12.4 MB/s eta 0:00:01\r\u001b[K     |█                               | 40 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |█▏                              | 51 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█▍                              | 61 kB 5.5 MB/s eta 0:00:01\r\u001b[K     |█▋                              | 71 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |█▉                              | 81 kB 5.6 MB/s eta 0:00:01\r\u001b[K     |██                              | 92 kB 6.2 MB/s eta 0:00:01\r\u001b[K     |██▎                             | 102 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██▌                             | 112 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██▊                             | 122 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███                             | 133 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▏                            | 143 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▌                            | 153 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███▊                            | 163 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████                            | 174 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 184 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▍                           | 194 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▋                           | 204 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████▉                           | 215 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████                           | 225 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 235 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▌                          | 245 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 256 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████                          | 266 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▏                         | 276 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▍                         | 286 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 296 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████                         | 307 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▏                        | 317 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▍                        | 327 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 337 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████▉                        | 348 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████                        | 358 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 368 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 378 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████▊                       | 389 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████                       | 399 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▏                      | 409 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▍                      | 419 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 430 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████▉                      | 440 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 450 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 460 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 471 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████▉                     | 481 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████                     | 491 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 501 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 512 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████▊                    | 522 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████                    | 532 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▏                   | 542 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▍                   | 552 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 563 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████▉                   | 573 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 583 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 593 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 604 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████▉                  | 614 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 624 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 634 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▌                 | 645 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████▊                 | 655 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 665 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▏                | 675 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▍                | 686 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 696 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 706 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████                | 716 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 727 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▌               | 737 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 747 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 757 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 768 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▌              | 778 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████▊              | 788 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 798 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▏             | 808 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▍             | 819 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 829 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 839 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 849 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 860 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▌            | 870 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████▊            | 880 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 890 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 901 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▌           | 911 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 921 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 931 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 942 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▍          | 952 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 962 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▉          | 972 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 983 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 993 kB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▊         | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▏        | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▍        | 1.0 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▊        | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▍       | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▊      | 1.1 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▍     | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▋     | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▍    | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▋    | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████    | 1.2 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▎   | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▊   | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████   | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▍  | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████  | 1.3 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▋ | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████ | 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4 MB 5.2 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 1.4 MB 5.2 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","source":["pip install datasets --quiet"],"metadata":{"id":"OaOSPMJE3ONc","executionInfo":{"status":"ok","timestamp":1659512431898,"user_tz":420,"elapsed":6379,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"65c870e6-e825-449d-9b8a-3700c9dda388"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 365 kB 5.1 MB/s \n","\u001b[K     |████████████████████████████████| 141 kB 69.6 MB/s \n","\u001b[K     |████████████████████████████████| 115 kB 80.4 MB/s \n","\u001b[K     |████████████████████████████████| 212 kB 75.9 MB/s \n","\u001b[K     |████████████████████████████████| 127 kB 86.1 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","source":["pip install rbo"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yTTL5d_d9KDH","executionInfo":{"status":"ok","timestamp":1659512435624,"user_tz":420,"elapsed":3730,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"e7af29e0-f4d4-4174-d6aa-b8a9d7d54b7f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting rbo\n","  Downloading rbo-0.1.2-py3-none-any.whl (7.5 kB)\n","Requirement already satisfied: numpy<2.0,>=1.18 in /usr/local/lib/python3.7/dist-packages (from rbo) (1.21.6)\n","Installing collected packages: rbo\n","Successfully installed rbo-0.1.2\n"]}]},{"cell_type":"code","source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\""],"metadata":{"id":"hRSNYTrRIPkr"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wrRRZJ6-0_Il"},"outputs":[],"source":["from captum.attr import visualization as viz\n","from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\n","from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n","\n","import torch\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b5V31lsc0_Il"},"outputs":[],"source":["device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","source":["## Import model"],"metadata":{"id":"USHRv2j70Fb4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"9nzBfB-v0_Im","executionInfo":{"status":"ok","timestamp":1659512470459,"user_tz":420,"elapsed":31532,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"colab":{"base_uri":"https://localhost:8080/","height":252,"referenced_widgets":["f81393c0b5f548658cbc98c7376df1f9","8fd1f56d02b54c279e86a26d04df4822","1ab3064b12084945a62ed2de52603d2c","d7caccd3386647dfa9cda51e56584b4a","5727b24cc76841b0a028a80a181b2dbc","26fc9791f8974270b863e1a53085c7de","76ec00781ffc443fb49d366e63819bd5","cd360623b92f4f6d86595793e09f8391","144692894fa54c67af9847fb84a72e14","cb468d72122e4f37b371b9cae878fa01","20ff3d450578481fb9eb83f0a1d48eb6","d751757f96184a86972067e7c9ea7804","b5753f8c6f2b4cbaaf0b03ba193609d7","9afd502da29a4bd5a4cfc4af851eacc7","178b39c93bbd48f89a14684318ce7910","602e4579b8c14b949dadd8ca93b30540","d66481b8b3884d148b73cfbff0e22bcd","ecfd0de10182442db0c1d0e7510247b3","3f57a9e446fc4e3da09d766c8090a2a3","131950c0fbe64eaa871b961f4e3cd70a","085155aac4fd4b11a61538a5648e18f9","447ad5eedaf1417982002270ee5d9630","91215adec2ca4693b5e9eddd27de534c","8aa82c7580dc4cfe906f1599706a8828","3ae768799d644c368b7b413557daae28","d53bb5f1945f4648876b2c77531c678c","f85445971ba14e8187bed25bad6addde","05e757971af440ad99526d66396b09bc","4616231b58a04cd2a163ad3b0daa8caa","b1136f129a484cd687f5d922dca1c4d4","72b10f1f9f8540d69244edbb4458bd9f","ef5251e54de6405da874f39d52f55138","95effa98de274be698732ab4f65d3b24","cfd04175e20049c1b24b98f5ae230be6","ff5ce5c1b9d14720881c688296ada688","d978db119a2c448c953d2c979b52152b","1ece88ee359442c18aafe5cfb585fdec","97824c5ca330429aace237f626492537","1717fde6ea1140339abcf1f75a4ac65d","22f58760aa5046dd8d0eeefc0ead5bff","74354be1fe8e4cb69f63fe52098cb4ba","54d4abfb8c494ab1a506eb488a9c8064","7d2329db8c1b4585a59512ee00a5a7e8","6c2d159dbed44797b133e45bbf97d2e2"]},"outputId":"e8b4fb28-36ef-4b7a-ea8b-7200c8b0e9bb"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading config.json:   0%|          | 0.00/694 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f81393c0b5f548658cbc98c7376df1f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/570M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d751757f96184a86972067e7c9ea7804"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at allenai/longformer-base-4096 were not used when initializing LongformerForSequenceClassification: ['longformer_model.encoder.layer.6.intermediate.dense.weight', 'longformer_model.encoder.layer.8.output.dense.bias', 'longformer_model.encoder.layer.4.attention.self.query_global.bias', 'longformer_model.encoder.layer.2.output.LayerNorm.bias', 'longformer_model.encoder.layer.2.attention.self.value_global.weight', 'longformer_model.encoder.layer.2.attention.self.query.bias', 'longformer_model.encoder.layer.4.output.dense.bias', 'longformer_model.encoder.layer.5.attention.self.key_global.weight', 'longformer_model.encoder.layer.3.output.LayerNorm.bias', 'longformer_model.encoder.layer.6.attention.output.dense.weight', 'longformer_model.encoder.layer.9.attention.self.query.bias', 'longformer_model.encoder.layer.7.attention.self.key_global.bias', 'longformer_model.encoder.layer.10.intermediate.dense.weight', 'longformer_model.encoder.layer.3.attention.output.dense.bias', 'longformer_model.encoder.layer.4.output.dense.weight', 'longformer_model.encoder.layer.2.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.9.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.4.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.8.intermediate.dense.bias', 'longformer_model.encoder.layer.9.output.dense.bias', 'longformer_model.encoder.layer.8.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.2.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.9.attention.self.value_global.bias', 'longformer_model.encoder.layer.3.attention.self.query_global.bias', 'longformer_model.encoder.layer.10.attention.self.key.bias', 'longformer_model.encoder.layer.10.attention.output.dense.weight', 'longformer_model.embeddings.LayerNorm.weight', 'longformer_model.encoder.layer.2.output.dense.weight', 'longformer_model.encoder.layer.2.output.LayerNorm.weight', 'longformer_model.encoder.layer.0.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.intermediate.dense.weight', 'longformer_model.encoder.layer.9.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.9.output.LayerNorm.weight', 'longformer_model.encoder.layer.4.attention.self.value.weight', 'longformer_model.encoder.layer.2.intermediate.dense.bias', 'longformer_model.encoder.layer.8.output.LayerNorm.weight', 'longformer_model.encoder.layer.10.attention.self.query_global.bias', 'longformer_model.encoder.layer.5.intermediate.dense.bias', 'longformer_model.encoder.layer.8.attention.self.key.weight', 'longformer_model.encoder.layer.10.output.dense.weight', 'longformer_model.encoder.layer.3.output.LayerNorm.weight', 'longformer_model.encoder.layer.4.attention.output.dense.bias', 'longformer_model.encoder.layer.10.attention.self.value.bias', 'longformer_model.encoder.layer.11.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.4.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.0.attention.self.value.bias', 'longformer_model.encoder.layer.0.intermediate.dense.weight', 'longformer_model.encoder.layer.10.attention.self.key_global.weight', 'longformer_model.encoder.layer.10.intermediate.dense.bias', 'longformer_model.encoder.layer.11.attention.self.key.bias', 'longformer_model.embeddings.position_ids', 'longformer_model.encoder.layer.1.attention.self.query.weight', 'longformer_model.encoder.layer.8.attention.self.key.bias', 'longformer_model.encoder.layer.2.attention.self.key_global.weight', 'longformer_model.encoder.layer.10.attention.self.key.weight', 'longformer_model.encoder.layer.5.attention.self.value_global.weight', 'dense.weight', 'longformer_model.encoder.layer.4.attention.self.key.weight', 'longformer_model.encoder.layer.11.attention.self.value.weight', 'fc.weight', 'longformer_model.encoder.layer.0.attention.self.query.weight', 'longformer_model.encoder.layer.0.attention.self.value.weight', 'longformer_model.encoder.layer.4.attention.self.key.bias', 'longformer_model.encoder.layer.7.attention.self.value.weight', 'longformer_model.encoder.layer.8.attention.self.value_global.weight', 'longformer_model.encoder.layer.3.attention.self.key.bias', 'longformer_model.encoder.layer.1.output.LayerNorm.weight', 'longformer_model.encoder.layer.1.attention.output.dense.weight', 'longformer_model.encoder.layer.8.attention.self.query_global.weight', 'longformer_model.encoder.layer.0.attention.output.dense.bias', 'longformer_model.encoder.layer.5.output.dense.weight', 'longformer_model.encoder.layer.7.output.dense.bias', 'longformer_model.encoder.layer.3.attention.self.key_global.bias', 'longformer_model.encoder.layer.7.attention.self.key.bias', 'longformer_model.encoder.layer.1.attention.self.value.weight', 'longformer_model.encoder.layer.3.attention.self.value_global.weight', 'longformer_model.encoder.layer.6.intermediate.dense.bias', 'longformer_model.encoder.layer.0.output.dense.weight', 'longformer_model.encoder.layer.6.attention.self.value.bias', 'longformer_model.embeddings.token_type_embeddings.weight', 'longformer_model.encoder.layer.7.attention.self.value_global.bias', 'longformer_model.embeddings.LayerNorm.bias', 'longformer_model.encoder.layer.10.attention.self.query_global.weight', 'longformer_model.encoder.layer.1.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.11.output.dense.weight', 'longformer_model.encoder.layer.7.attention.self.query.weight', 'longformer_model.encoder.layer.7.output.LayerNorm.bias', 'longformer_model.encoder.layer.7.attention.self.query.bias', 'longformer_model.encoder.layer.9.attention.output.dense.weight', 'longformer_model.encoder.layer.4.attention.self.value_global.bias', 'longformer_model.encoder.layer.9.output.LayerNorm.bias', 'longformer_model.encoder.layer.2.attention.self.key.bias', 'longformer_model.encoder.layer.2.attention.self.value.weight', 'longformer_model.encoder.layer.4.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.output.dense.bias', 'longformer_model.encoder.layer.11.attention.self.value.bias', 'longformer_model.encoder.layer.4.output.LayerNorm.weight', 'longformer_model.encoder.layer.11.output.dense.bias', 'longformer_model.encoder.layer.2.attention.self.key.weight', 'longformer_model.encoder.layer.8.attention.self.value.bias', 'longformer_model.encoder.layer.6.output.LayerNorm.weight', 'longformer_model.encoder.layer.9.intermediate.dense.weight', 'longformer_model.encoder.layer.5.attention.self.query.bias', 'longformer_model.encoder.layer.1.attention.self.key.bias', 'longformer_model.encoder.layer.11.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.0.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.11.output.LayerNorm.bias', 'longformer_model.encoder.layer.7.attention.output.dense.weight', 'longformer_model.encoder.layer.0.attention.self.value_global.weight', 'longformer_model.encoder.layer.2.intermediate.dense.weight', 'longformer_model.encoder.layer.4.intermediate.dense.weight', 'longformer_model.encoder.layer.3.attention.self.key_global.weight', 'longformer_model.encoder.layer.6.attention.self.key_global.weight', 'longformer_model.encoder.layer.4.attention.self.query.bias', 'longformer_model.encoder.layer.3.attention.self.query.bias', 'longformer_model.encoder.layer.7.intermediate.dense.bias', 'longformer_model.encoder.layer.6.attention.self.query_global.weight', 'longformer_model.encoder.layer.11.output.LayerNorm.weight', 'longformer_model.encoder.layer.9.attention.self.query.weight', 'longformer_model.encoder.layer.10.output.dense.bias', 'longformer_model.encoder.layer.4.attention.self.key_global.bias', 'longformer_model.encoder.layer.0.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.5.attention.self.key_global.bias', 'longformer_model.encoder.layer.8.output.LayerNorm.bias', 'longformer_model.encoder.layer.3.output.dense.bias', 'longformer_model.encoder.layer.1.attention.self.value_global.weight', 'longformer_model.encoder.layer.6.attention.self.key_global.bias', 'longformer_model.encoder.layer.5.attention.self.value_global.bias', 'longformer_model.encoder.layer.3.intermediate.dense.weight', 'longformer_model.encoder.layer.11.attention.self.query.weight', 'longformer_model.encoder.layer.9.attention.self.query_global.weight', 'longformer_model.encoder.layer.11.attention.self.value_global.weight', 'longformer_model.encoder.layer.2.attention.self.value.bias', 'longformer_model.encoder.layer.6.attention.self.query.bias', 'longformer_model.encoder.layer.1.attention.output.dense.bias', 'longformer_model.encoder.layer.9.attention.self.value.bias', 'longformer_model.encoder.layer.11.attention.self.key.weight', 'longformer_model.encoder.layer.1.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.3.attention.self.value.bias', 'longformer_model.encoder.layer.6.attention.self.query.weight', 'longformer_model.encoder.layer.3.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.6.output.dense.bias', 'longformer_model.encoder.layer.1.intermediate.dense.weight', 'longformer_model.encoder.layer.3.attention.self.query_global.weight', 'longformer_model.encoder.layer.10.attention.self.key_global.bias', 'longformer_model.encoder.layer.10.attention.self.value.weight', 'longformer_model.encoder.layer.0.intermediate.dense.bias', 'longformer_model.encoder.layer.0.attention.self.key.bias', 'longformer_model.encoder.layer.6.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.6.output.dense.weight', 'longformer_model.encoder.layer.7.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.3.attention.self.value_global.bias', 'longformer_model.encoder.layer.5.attention.self.query_global.weight', 'longformer_model.encoder.layer.1.output.dense.weight', 'longformer_model.encoder.layer.2.attention.self.query_global.bias', 'longformer_model.encoder.layer.1.attention.self.value.bias', 'longformer_model.encoder.layer.4.attention.self.query_global.weight', 'longformer_model.encoder.layer.6.attention.output.dense.bias', 'longformer_model.encoder.layer.8.attention.self.key_global.weight', 'longformer_model.encoder.layer.9.attention.self.query_global.bias', 'longformer_model.encoder.layer.11.attention.output.dense.bias', 'longformer_model.encoder.layer.7.attention.self.key_global.weight', 'longformer_model.encoder.layer.9.attention.self.value.weight', 'longformer_model.encoder.layer.3.attention.output.dense.weight', 'longformer_model.encoder.layer.9.attention.output.dense.bias', 'longformer_model.encoder.layer.0.attention.self.key_global.weight', 'longformer_model.encoder.layer.4.intermediate.dense.bias', 'longformer_model.encoder.layer.7.output.dense.weight', 'longformer_model.encoder.layer.10.output.LayerNorm.weight', 'longformer_model.encoder.layer.2.attention.output.dense.bias', 'longformer_model.encoder.layer.0.attention.self.query_global.weight', 'longformer_model.encoder.layer.3.attention.self.key.weight', 'longformer_model.embeddings.word_embeddings.weight', 'longformer_model.encoder.layer.10.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.5.attention.self.value.weight', 'longformer_model.encoder.layer.7.attention.self.value_global.weight', 'dense.bias', 'fc.bias', 'longformer_model.encoder.layer.6.attention.self.query_global.bias', 'longformer_model.encoder.layer.2.output.dense.bias', 'longformer_model.encoder.layer.9.attention.self.key_global.bias', 'longformer_model.encoder.layer.1.attention.self.key_global.bias', 'longformer_model.encoder.layer.5.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.11.attention.self.query.bias', 'longformer_model.encoder.layer.7.output.LayerNorm.weight', 'longformer_model.encoder.layer.5.attention.output.dense.weight', 'longformer_model.encoder.layer.8.attention.self.key_global.bias', 'longformer_model.encoder.layer.11.attention.self.key_global.bias', 'longformer_model.encoder.layer.6.attention.self.value_global.weight', 'longformer_model.encoder.layer.4.attention.self.value_global.weight', 'longformer_model.encoder.layer.4.attention.output.dense.weight', 'longformer_model.encoder.layer.5.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.0.attention.self.key_global.bias', 'longformer_model.encoder.layer.1.intermediate.dense.bias', 'longformer_model.encoder.layer.3.attention.self.value.weight', 'longformer_model.encoder.layer.6.attention.self.value.weight', 'longformer_model.encoder.layer.3.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.3.intermediate.dense.bias', 'longformer_model.encoder.layer.5.attention.self.query.weight', 'longformer_model.encoder.layer.6.attention.self.key.weight', 'longformer_model.encoder.layer.3.output.dense.weight', 'longformer_model.encoder.layer.5.attention.self.key.weight', 'longformer_model.encoder.layer.5.attention.self.key.bias', 'longformer_model.encoder.layer.8.attention.output.dense.weight', 'longformer_model.encoder.layer.8.intermediate.dense.weight', 'longformer_model.encoder.layer.4.attention.self.key_global.weight', 'longformer_model.encoder.layer.8.attention.output.dense.bias', 'longformer_model.encoder.layer.0.attention.self.value_global.bias', 'longformer_model.encoder.layer.2.attention.self.value_global.bias', 'longformer_model.encoder.layer.0.attention.self.query.bias', 'longformer_model.encoder.layer.8.output.dense.weight', 'longformer_model.encoder.layer.8.attention.self.query_global.bias', 'longformer_model.encoder.layer.6.output.LayerNorm.bias', 'longformer_model.encoder.layer.7.intermediate.dense.weight', 'longformer_model.encoder.layer.8.attention.self.value.weight', 'longformer_model.encoder.layer.9.attention.self.key.weight', 'longformer_model.encoder.layer.9.attention.self.key_global.weight', 'longformer_model.encoder.layer.10.attention.self.query.bias', 'longformer_model.encoder.layer.2.attention.self.key_global.bias', 'longformer_model.encoder.layer.7.attention.output.LayerNorm.weight', 'longformer_model.encoder.layer.6.attention.self.key.bias', 'longformer_model.encoder.layer.1.attention.self.query_global.weight', 'longformer_model.encoder.layer.0.output.LayerNorm.weight', 'longformer_model.encoder.layer.1.attention.self.key_global.weight', 'longformer_model.encoder.layer.8.attention.self.query.weight', 'longformer_model.encoder.layer.8.attention.self.value_global.bias', 'longformer_model.encoder.layer.0.attention.self.key.weight', 'longformer_model.encoder.layer.11.attention.self.value_global.bias', 'longformer_model.encoder.layer.9.intermediate.dense.bias', 'longformer_model.encoder.layer.2.attention.self.query.weight', 'longformer_model.encoder.layer.5.output.LayerNorm.weight', 'longformer_model.encoder.layer.4.attention.self.query.weight', 'longformer_model.encoder.layer.1.attention.self.value_global.bias', 'longformer_model.encoder.layer.7.attention.output.dense.bias', 'longformer_model.encoder.layer.11.intermediate.dense.bias', 'longformer_model.encoder.layer.2.attention.output.dense.weight', 'longformer_model.encoder.layer.1.output.dense.bias', 'longformer_model.encoder.layer.1.attention.self.query_global.bias', 'longformer_model.encoder.layer.2.attention.self.query_global.weight', 'longformer_model.encoder.layer.8.attention.self.query.bias', 'longformer_model.encoder.layer.5.output.dense.bias', 'longformer_model.encoder.layer.4.attention.self.value.bias', 'longformer_model.encoder.layer.10.output.LayerNorm.bias', 'longformer_model.encoder.layer.6.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.10.attention.output.dense.bias', 'longformer_model.encoder.layer.9.attention.self.key.bias', 'longformer_model.encoder.layer.5.attention.self.value.bias', 'longformer_model.encoder.layer.0.attention.self.query_global.bias', 'longformer_model.encoder.layer.9.attention.self.value_global.weight', 'longformer_model.encoder.layer.0.attention.output.dense.weight', 'longformer_model.encoder.layer.7.attention.self.query_global.bias', 'longformer_model.encoder.layer.7.attention.self.key.weight', 'longformer_model.encoder.layer.11.attention.self.key_global.weight', 'longformer_model.encoder.layer.10.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.10.attention.self.query.weight', 'longformer_model.encoder.layer.1.attention.self.query.bias', 'longformer_model.encoder.layer.6.attention.self.value_global.bias', 'longformer_model.encoder.layer.8.attention.output.LayerNorm.bias', 'longformer_model.encoder.layer.5.attention.self.query_global.bias', 'longformer_model.encoder.layer.5.output.LayerNorm.bias', 'longformer_model.encoder.layer.10.attention.self.value_global.weight', 'longformer_model.encoder.layer.7.attention.self.value.bias', 'longformer_model.encoder.layer.7.attention.self.query_global.weight', 'longformer_model.encoder.layer.10.attention.self.value_global.bias', 'longformer_model.encoder.layer.9.output.dense.weight', 'longformer_model.encoder.layer.11.attention.self.query_global.bias', 'longformer_model.encoder.layer.11.intermediate.dense.weight', 'longformer_model.encoder.layer.1.attention.self.key.weight', 'longformer_model.encoder.layer.1.output.LayerNorm.bias', 'longformer_model.encoder.layer.11.attention.output.dense.weight', 'longformer_model.encoder.layer.3.attention.self.query.weight', 'longformer_model.embeddings.position_embeddings.weight', 'longformer_model.encoder.layer.11.attention.self.query_global.weight', 'longformer_model.encoder.layer.0.output.dense.bias']\n","- This IS expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing LongformerForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of LongformerForSequenceClassification were not initialized from the model checkpoint at allenai/longformer-base-4096 and are newly initialized: ['longformer.encoder.layer.0.attention.self.value.weight', 'longformer.encoder.layer.7.attention.self.value.weight', 'longformer.encoder.layer.7.attention.output.dense.bias', 'longformer.encoder.layer.7.attention.self.value_global.weight', 'longformer.encoder.layer.7.attention.self.query_global.bias', 'longformer.encoder.layer.4.output.LayerNorm.weight', 'longformer.encoder.layer.4.attention.self.key_global.bias', 'longformer.encoder.layer.5.attention.self.value.weight', 'longformer.encoder.layer.7.attention.self.key.bias', 'longformer.encoder.layer.7.attention.self.query.bias', 'longformer.encoder.layer.11.intermediate.dense.bias', 'longformer.encoder.layer.3.attention.self.key_global.bias', 'longformer.encoder.layer.7.output.LayerNorm.bias', 'longformer.encoder.layer.9.attention.output.dense.bias', 'longformer.encoder.layer.6.attention.self.value_global.bias', 'longformer.encoder.layer.11.output.dense.bias', 'longformer.encoder.layer.3.output.LayerNorm.bias', 'longformer.encoder.layer.0.intermediate.dense.bias', 'longformer.encoder.layer.10.output.dense.bias', 'longformer.encoder.layer.8.attention.self.query_global.bias', 'longformer.encoder.layer.2.intermediate.dense.bias', 'longformer.encoder.layer.9.attention.self.query_global.bias', 'longformer.encoder.layer.7.attention.output.LayerNorm.bias', 'longformer.encoder.layer.1.output.dense.bias', 'longformer.encoder.layer.3.output.LayerNorm.weight', 'longformer.encoder.layer.8.attention.self.value_global.bias', 'longformer.encoder.layer.11.attention.self.query_global.weight', 'longformer.encoder.layer.0.attention.output.dense.weight', 'longformer.encoder.layer.8.attention.self.key.weight', 'longformer.encoder.layer.3.intermediate.dense.bias', 'longformer.encoder.layer.6.attention.self.query.bias', 'longformer.encoder.layer.2.attention.self.key.weight', 'longformer.encoder.layer.0.attention.self.value_global.bias', 'longformer.encoder.layer.10.attention.self.key_global.weight', 'longformer.encoder.layer.5.attention.self.query.weight', 'longformer.encoder.layer.6.attention.output.LayerNorm.bias', 'longformer.encoder.layer.6.attention.output.dense.bias', 'longformer.encoder.layer.7.output.dense.bias', 'longformer.encoder.layer.6.output.LayerNorm.weight', 'longformer.encoder.layer.9.output.LayerNorm.bias', 'longformer.encoder.layer.1.intermediate.dense.weight', 'longformer.encoder.layer.2.attention.self.value_global.weight', 'longformer.encoder.layer.2.attention.output.LayerNorm.bias', 'longformer.encoder.layer.2.output.dense.bias', 'longformer.encoder.layer.4.attention.self.query_global.bias', 'longformer.encoder.layer.9.attention.self.key_global.bias', 'longformer.encoder.layer.6.output.dense.weight', 'longformer.encoder.layer.2.attention.self.query_global.bias', 'longformer.encoder.layer.3.attention.self.value_global.bias', 'longformer.encoder.layer.4.output.dense.weight', 'longformer.encoder.layer.5.attention.output.dense.weight', 'longformer.encoder.layer.10.attention.self.value_global.bias', 'longformer.encoder.layer.1.attention.self.key_global.bias', 'longformer.encoder.layer.7.attention.self.key.weight', 'longformer.encoder.layer.4.attention.self.value_global.weight', 'longformer.encoder.layer.2.attention.output.LayerNorm.weight', 'longformer.embeddings.token_type_embeddings.weight', 'longformer.encoder.layer.8.output.LayerNorm.bias', 'longformer.encoder.layer.6.attention.self.key.bias', 'longformer.encoder.layer.2.attention.self.value.weight', 'longformer.encoder.layer.3.attention.output.dense.weight', 'longformer.encoder.layer.9.attention.self.query.weight', 'longformer.encoder.layer.0.attention.self.value.bias', 'longformer.encoder.layer.4.attention.self.query.bias', 'longformer.encoder.layer.6.attention.self.value.bias', 'longformer.encoder.layer.11.intermediate.dense.weight', 'longformer.encoder.layer.10.attention.self.key.bias', 'longformer.encoder.layer.9.attention.self.value_global.bias', 'longformer.encoder.layer.0.attention.self.key.bias', 'longformer.encoder.layer.3.output.dense.weight', 'longformer.encoder.layer.1.attention.self.key.weight', 'longformer.encoder.layer.7.attention.self.key_global.bias', 'longformer.encoder.layer.2.attention.self.value.bias', 'longformer.encoder.layer.11.attention.self.query_global.bias', 'longformer.encoder.layer.6.attention.self.query_global.bias', 'longformer.encoder.layer.2.output.LayerNorm.bias', 'longformer.encoder.layer.0.intermediate.dense.weight', 'longformer.encoder.layer.4.attention.self.value_global.bias', 'longformer.encoder.layer.6.attention.output.dense.weight', 'longformer.encoder.layer.9.output.dense.weight', 'longformer.encoder.layer.0.output.LayerNorm.weight', 'classifier.out_proj.weight', 'longformer.encoder.layer.11.attention.self.query.bias', 'longformer.encoder.layer.6.output.LayerNorm.bias', 'longformer.encoder.layer.4.intermediate.dense.bias', 'longformer.encoder.layer.9.output.LayerNorm.weight', 'longformer.encoder.layer.8.intermediate.dense.bias', 'longformer.encoder.layer.1.attention.self.query_global.weight', 'longformer.encoder.layer.2.attention.self.query.weight', 'longformer.encoder.layer.5.attention.self.query.bias', 'longformer.encoder.layer.0.attention.self.query_global.bias', 'longformer.encoder.layer.3.attention.self.value_global.weight', 'longformer.encoder.layer.4.output.dense.bias', 'longformer.encoder.layer.6.attention.self.value.weight', 'longformer.encoder.layer.8.attention.self.query.bias', 'longformer.encoder.layer.5.output.LayerNorm.bias', 'longformer.encoder.layer.0.attention.self.query.weight', 'longformer.encoder.layer.7.attention.self.key_global.weight', 'longformer.encoder.layer.11.output.dense.weight', 'longformer.encoder.layer.8.attention.self.value.weight', 'longformer.encoder.layer.10.attention.output.dense.weight', 'longformer.encoder.layer.11.attention.self.key_global.bias', 'longformer.encoder.layer.2.attention.self.query.bias', 'longformer.encoder.layer.1.attention.output.dense.weight', 'longformer.encoder.layer.3.attention.self.key.bias', 'longformer.encoder.layer.1.attention.self.value_global.bias', 'longformer.encoder.layer.11.attention.self.key.weight', 'longformer.encoder.layer.3.attention.self.query.weight', 'longformer.encoder.layer.2.output.dense.weight', 'longformer.encoder.layer.8.attention.output.dense.bias', 'longformer.encoder.layer.8.attention.output.dense.weight', 'longformer.encoder.layer.9.attention.self.key.weight', 'longformer.encoder.layer.9.attention.self.value_global.weight', 'longformer.encoder.layer.11.attention.self.value.bias', 'longformer.encoder.layer.8.attention.self.value.bias', 'classifier.dense.bias', 'classifier.dense.weight', 'longformer.encoder.layer.3.attention.self.query_global.weight', 'longformer.encoder.layer.9.attention.self.value.weight', 'longformer.encoder.layer.5.intermediate.dense.weight', 'longformer.encoder.layer.7.attention.self.query_global.weight', 'longformer.encoder.layer.10.attention.self.query.weight', 'longformer.encoder.layer.11.attention.output.dense.weight', 'longformer.encoder.layer.11.attention.output.LayerNorm.bias', 'longformer.encoder.layer.2.attention.self.key.bias', 'longformer.encoder.layer.6.attention.self.query.weight', 'longformer.encoder.layer.4.attention.self.key.bias', 'longformer.encoder.layer.6.attention.self.query_global.weight', 'longformer.encoder.layer.10.output.LayerNorm.bias', 'longformer.encoder.layer.7.intermediate.dense.bias', 'longformer.encoder.layer.4.attention.output.LayerNorm.weight', 'longformer.encoder.layer.10.attention.self.value_global.weight', 'longformer.encoder.layer.0.output.dense.bias', 'longformer.encoder.layer.10.attention.self.query_global.weight', 'longformer.encoder.layer.7.attention.self.value_global.bias', 'longformer.encoder.layer.7.attention.output.LayerNorm.weight', 'longformer.encoder.layer.1.attention.self.query_global.bias', 'longformer.encoder.layer.10.attention.self.query.bias', 'longformer.encoder.layer.8.attention.output.LayerNorm.bias', 'longformer.encoder.layer.4.attention.self.key.weight', 'longformer.encoder.layer.10.attention.self.value.weight', 'longformer.encoder.layer.10.intermediate.dense.weight', 'classifier.out_proj.bias', 'longformer.encoder.layer.10.output.LayerNorm.weight', 'longformer.embeddings.LayerNorm.bias', 'longformer.encoder.layer.2.attention.self.key_global.bias', 'longformer.encoder.layer.4.attention.output.dense.bias', 'longformer.encoder.layer.8.attention.self.key.bias', 'longformer.encoder.layer.9.attention.self.query.bias', 'longformer.encoder.layer.0.attention.output.dense.bias', 'longformer.encoder.layer.3.attention.self.key_global.weight', 'longformer.encoder.layer.10.attention.output.LayerNorm.weight', 'longformer.embeddings.position_embeddings.weight', 'longformer.encoder.layer.4.attention.self.value.bias', 'longformer.encoder.layer.1.attention.self.value_global.weight', 'longformer.encoder.layer.6.attention.self.key_global.bias', 'longformer.encoder.layer.9.attention.output.dense.weight', 'longformer.encoder.layer.5.attention.self.key_global.weight', 'longformer.encoder.layer.5.attention.self.key.bias', 'longformer.encoder.layer.1.attention.output.LayerNorm.bias', 'longformer.encoder.layer.11.attention.output.LayerNorm.weight', 'longformer.encoder.layer.2.attention.output.dense.weight', 'longformer.encoder.layer.0.output.LayerNorm.bias', 'longformer.encoder.layer.3.attention.output.dense.bias', 'longformer.encoder.layer.8.output.LayerNorm.weight', 'longformer.encoder.layer.3.output.dense.bias', 'longformer.encoder.layer.9.attention.output.LayerNorm.bias', 'longformer.encoder.layer.5.attention.output.dense.bias', 'longformer.encoder.layer.9.attention.self.value.bias', 'longformer.encoder.layer.4.attention.self.key_global.weight', 'longformer.encoder.layer.6.attention.self.key.weight', 'longformer.encoder.layer.5.intermediate.dense.bias', 'longformer.encoder.layer.11.attention.self.key.bias', 'longformer.encoder.layer.1.attention.self.query.bias', 'longformer.encoder.layer.5.attention.self.value_global.bias', 'longformer.encoder.layer.1.attention.self.query.weight', 'longformer.encoder.layer.8.attention.self.query.weight', 'longformer.encoder.layer.3.attention.output.LayerNorm.weight', 'longformer.encoder.layer.5.attention.self.key.weight', 'longformer.encoder.layer.1.output.LayerNorm.weight', 'longformer.encoder.layer.7.attention.self.value.bias', 'longformer.encoder.layer.9.attention.self.query_global.weight', 'longformer.encoder.layer.10.output.dense.weight', 'longformer.encoder.layer.8.attention.output.LayerNorm.weight', 'longformer.encoder.layer.2.attention.self.query_global.weight', 'longformer.encoder.layer.10.attention.self.key.weight', 'longformer.encoder.layer.3.attention.self.key.weight', 'longformer.encoder.layer.9.output.dense.bias', 'longformer.encoder.layer.6.intermediate.dense.bias', 'longformer.encoder.layer.4.attention.output.LayerNorm.bias', 'longformer.encoder.layer.5.output.dense.bias', 'longformer.encoder.layer.10.intermediate.dense.bias', 'longformer.encoder.layer.0.attention.self.key.weight', 'longformer.encoder.layer.6.intermediate.dense.weight', 'longformer.encoder.layer.9.intermediate.dense.weight', 'longformer.encoder.layer.4.attention.output.dense.weight', 'longformer.encoder.layer.4.attention.self.query_global.weight', 'longformer.encoder.layer.8.output.dense.weight', 'longformer.encoder.layer.6.attention.self.value_global.weight', 'longformer.encoder.layer.11.output.LayerNorm.bias', 'longformer.encoder.layer.11.attention.self.value_global.bias', 'longformer.encoder.layer.5.output.LayerNorm.weight', 'longformer.encoder.layer.2.attention.output.dense.bias', 'longformer.encoder.layer.0.output.dense.weight', 'longformer.encoder.layer.7.output.LayerNorm.weight', 'longformer.encoder.layer.6.attention.self.key_global.weight', 'longformer.encoder.layer.0.attention.output.LayerNorm.bias', 'longformer.encoder.layer.1.intermediate.dense.bias', 'longformer.encoder.layer.3.attention.self.query.bias', 'longformer.encoder.layer.8.attention.self.key_global.weight', 'longformer.encoder.layer.3.intermediate.dense.weight', 'longformer.encoder.layer.1.attention.self.key.bias', 'longformer.encoder.layer.0.attention.output.LayerNorm.weight', 'longformer.encoder.layer.1.output.dense.weight', 'longformer.encoder.layer.1.attention.output.LayerNorm.weight', 'longformer.encoder.layer.1.attention.output.dense.bias', 'longformer.encoder.layer.1.output.LayerNorm.bias', 'longformer.encoder.layer.2.output.LayerNorm.weight', 'longformer.encoder.layer.11.output.LayerNorm.weight', 'longformer.encoder.layer.4.attention.self.value.weight', 'longformer.encoder.layer.11.attention.output.dense.bias', 'longformer.encoder.layer.4.intermediate.dense.weight', 'longformer.encoder.layer.5.output.dense.weight', 'longformer.encoder.layer.1.attention.self.key_global.weight', 'longformer.encoder.layer.11.attention.self.value_global.weight', 'longformer.encoder.layer.0.attention.self.query.bias', 'longformer.encoder.layer.5.attention.self.query_global.weight', 'longformer.encoder.layer.7.attention.output.dense.weight', 'longformer.encoder.layer.8.attention.self.key_global.bias', 'longformer.encoder.layer.9.intermediate.dense.bias', 'longformer.encoder.layer.10.attention.self.key_global.bias', 'longformer.embeddings.LayerNorm.weight', 'longformer.encoder.layer.7.intermediate.dense.weight', 'longformer.encoder.layer.0.attention.self.value_global.weight', 'longformer.encoder.layer.9.attention.output.LayerNorm.weight', 'longformer.encoder.layer.6.output.dense.bias', 'longformer.encoder.layer.3.attention.self.value.weight', 'longformer.encoder.layer.9.attention.self.key_global.weight', 'longformer.encoder.layer.10.attention.output.dense.bias', 'longformer.encoder.layer.0.attention.self.key_global.bias', 'longformer.encoder.layer.8.output.dense.bias', 'longformer.embeddings.word_embeddings.weight', 'longformer.encoder.layer.2.attention.self.key_global.weight', 'longformer.encoder.layer.0.attention.self.query_global.weight', 'longformer.encoder.layer.2.intermediate.dense.weight', 'longformer.encoder.layer.11.attention.self.key_global.weight', 'longformer.encoder.layer.5.attention.self.query_global.bias', 'longformer.encoder.layer.3.attention.self.query_global.bias', 'longformer.encoder.layer.1.attention.self.value.bias', 'longformer.encoder.layer.10.attention.self.query_global.bias', 'longformer.encoder.layer.5.attention.output.LayerNorm.weight', 'longformer.encoder.layer.7.output.dense.weight', 'longformer.encoder.layer.10.attention.self.value.bias', 'longformer.encoder.layer.9.attention.self.key.bias', 'longformer.encoder.layer.5.attention.self.value.bias', 'longformer.encoder.layer.10.attention.output.LayerNorm.bias', 'longformer.encoder.layer.11.attention.self.query.weight', 'longformer.encoder.layer.11.attention.self.value.weight', 'longformer.encoder.layer.3.attention.self.value.bias', 'longformer.encoder.layer.6.attention.output.LayerNorm.weight', 'longformer.encoder.layer.2.attention.self.value_global.bias', 'longformer.encoder.layer.3.attention.output.LayerNorm.bias', 'longformer.encoder.layer.4.output.LayerNorm.bias', 'longformer.encoder.layer.8.attention.self.value_global.weight', 'longformer.encoder.layer.1.attention.self.value.weight', 'longformer.encoder.layer.5.attention.output.LayerNorm.bias', 'longformer.encoder.layer.8.intermediate.dense.weight', 'longformer.encoder.layer.7.attention.self.query.weight', 'longformer.encoder.layer.8.attention.self.query_global.weight', 'longformer.encoder.layer.5.attention.self.value_global.weight', 'longformer.encoder.layer.5.attention.self.key_global.bias', 'longformer.encoder.layer.4.attention.self.query.weight', 'longformer.encoder.layer.0.attention.self.key_global.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading vocab.json:   0%|          | 0.00/878k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"91215adec2ca4693b5e9eddd27de534c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading merges.txt:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cfd04175e20049c1b24b98f5ae230be6"}},"metadata":{}}],"source":["from transformers import LongformerForSequenceClassification, LongformerTokenizer, LongformerConfig\n","# replace <PATH-TO-SAVED-MODEL> with the real path of the saved model\n","model_path = 'danielhou13/longformer-finetuned_papers_v2'\n","#model_path = 'danielhou13/longformer-finetuned-news-cogs402'\n","\n","# load model\n","test = torch.load(\"/content/drive/MyDrive/fakeclinicalnotes/models/full_augmented_lr2e-5_dropout3_10_trained_threshold.pt\")\n","model = LongformerForSequenceClassification.from_pretrained('allenai/longformer-base-4096', state_dict=test['state_dict'], num_labels = 2)\n","model.to(device)\n","model.eval()\n","model.zero_grad()\n","\n","# load tokenizer\n","tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vAAjmDRl0_In"},"outputs":[],"source":["ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n","sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n","cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence"]},{"cell_type":"markdown","source":["## Import dataset"],"metadata":{"id":"BdY6GsO00RG_"}},{"cell_type":"markdown","source":["Here we import the papers dataset"],"metadata":{"id":"e_gGIPpwkmbH"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6aQM9nM0_Ip","colab":{"base_uri":"https://localhost:8080/","height":249,"referenced_widgets":["f15c3e7cfbbb4e88bcde85271986d1fc","86fc06064ba74b59930f798334830f98","8eceb2f11f36496dba471c1312fb8a38","8bf8d14e4cf74955a54fe413450ce9bd","91201ff82b7648eea59b6453ae14dbac","8413354afbd24d218b932aa153d4de3a","4e3be7912a384fc3a4a380de8df83114","4ad309a5c75f497490639d5dbb4460dc","3ec7106896fe4ccaa077c7017205eace","e2c739093a97434eba565cebb2df3c55","916f4ba96c344a82a038c0a2c6debeb6","e8c12bf138d447c2a0b2060be17560d7","c69a55d4dbb74a599ea34917f294135d","813d23b62d9f474493cc534141730ee1","6dfca0d9543740ef91ef412902d20a5d","695aece1a68947a7a51d0d82a6fdf0a7","507c62435bd140588368995d853ae861","08aac8d4ecf645bf9a8ae59d3dd0980a","5e3482fcee18454182c49f4bfc7f293e","ded0eb5deb7b4807b3aedc87e4f65465","e80ef233f14f40fb9d47381194bbffa0","b1944977de324ae59073e8bde89d95be","a080e52e02f448e7b1a79cacd3de4212","ec91e884a5d240179736fc569bd7ca37","3f45cd198ec8486d9911d0407a3eda31","6a9a232be3664866aad321d2d9582408","67cfa6834bf34413b0fd03903fbc9751","5ba0f9fd74b54f6ea3439b0feab5e56d","0c2efdd33d8f438997349f3d1181d0ca","31d6461ada8c4802adc32b740dc7f053","6fe2f07343544061b2f4d3ecaf8f0b83","40cbd50f9442444184aaa90e1463cd46","8452d41757dc444daf63f3e3b35200e7","0469deaf91044c13b230c5de4381f680","aeea89a950504058afb73c8422e91f28","ec2d7d104b9248f080a660e601fc0d35","cb92133c6337469cb26cd4ee3bb291cf","61449827a7294377a979f46f84508447","bd08290fa5834a31890607dfde98322f","91253fbdba9843b5aa938eb0fce96282","2d782def8a224deb92d321cb61e23a4b","39861043e76d420ebd035bb550cef1e0","eb36464f6ca7410482ebe4955e7406a1","2d645fa0475a479382baa38ca02153c3","8e2c845a98994b7cbc835b993c6bfff7","1d9a63c62bf2451db05f43a106593cad","071cb0593ebc492c8c03ecc1257ec207","d2423f00be6049d8871ed3879c99df61","a5127cd43edb4c37b6f8b651723336ba","f1bb553c3a7f4ac3bdc72310d1df29ce","4112ffb75125421c9f3325a3d1d3faef","7f64b9eaf1f9431a89ec96f95dd3a13f","07c8b12e05e34035b4732bcde64ddad4","cedd0fb40f674e73b4b62dde1a829ece","56b5756c48bb483b8c854211a7fdcff2","ac50bb965247424c87f4d3d74e0ecb7f","c7c3f79c46634d77bbacb9f27bbe4660","a208e8d8b9944955a3d0eee6e8890fb5","901a6540128d4204b098bde09843b822","6858411de99340eabdddf3e6709b4e4b","e53116a2750340969b19215a02bfe748","532063b3607f46b2aad4ce8dbce068de","7fb4b698ef2d4e398c8742ab8a75de02","cd3c904a0a4e4085bc6d7f0bc4d3ecb6","cd5aa0b926f641b7ae310e35d10db58e","dea9355b39154005aefcc52caea58dd2"]},"executionInfo":{"status":"ok","timestamp":1659512473528,"user_tz":420,"elapsed":3073,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"24492567-8d56-4f1b-a393-a5b608d5626e"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/613 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f15c3e7cfbbb4e88bcde85271986d1fc"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Using custom data configuration danielhou13--cogs402datafake-f5349e6cf83e41d8\n"]},{"output_type":"stream","name":"stdout","text":["Downloading and preparing dataset None/None (download: 59.78 KiB, generated: 114.45 KiB, post-processed: Unknown size, total: 174.22 KiB) to /root/.cache/huggingface/datasets/danielhou13___parquet/danielhou13--cogs402datafake-f5349e6cf83e41d8/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e8c12bf138d447c2a0b2060be17560d7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading data:   0%|          | 0.00/61.2k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a080e52e02f448e7b1a79cacd3de4212"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0469deaf91044c13b230c5de4381f680"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["0 tables [00:00, ? tables/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e2c845a98994b7cbc835b993c6bfff7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Dataset parquet downloaded and prepared to /root/.cache/huggingface/datasets/danielhou13___parquet/danielhou13--cogs402datafake-f5349e6cf83e41d8/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec. Subsequent calls will reuse this data.\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/1 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac50bb965247424c87f4d3d74e0ecb7f"}},"metadata":{}}],"source":["from datasets import load_dataset\n","import numpy as np\n","cogs402_ds = load_dataset(\"danielhou13/cogs402datafake\")[\"train\"]"]},{"cell_type":"markdown","source":["Here we import the news dataset"],"metadata":{"id":"bHUUw096r-EL"}},{"cell_type":"code","source":["# cogs402_ds2 = load_dataset('hyperpartisan_news_detection', 'bypublisher')['validation']\n","# val_size = 5000\n","# val_indices = np.random.randint(0, len(cogs402_ds2), val_size)\n","# val_ds = cogs402_ds2.select(val_indices)\n","# labels2 = map(int, val_ds['hyperpartisan'])\n","# labels2 = list(labels2)\n","# val_ds = val_ds.add_column(\"labels\", labels2)"],"metadata":{"id":"58zzpZYRWRDj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Get Attributions"],"metadata":{"id":"sqGG8ke1Zd1N"}},{"cell_type":"markdown","source":["We need to create a custom forward function for use in our [Integrated Gradients](https://arxiv.org/abs/1703.01365) functions. Specifially the output we want from the forward pass of the model is the softmaxed logits, which indicate the probabilities of predicting each class for the given example."],"metadata":{"id":"1QIudhnqA7pQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"y5-heo2y0_Im"},"outputs":[],"source":["def predict(inputs, position_ids=None, attention_mask=None):\n","    output = model(inputs,\n","                   position_ids=position_ids,\n","                   attention_mask=attention_mask)\n","    return output.logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"axwpHq-y0_Io"},"outputs":[],"source":["#set 1 if we are dealing with a positive class, and 0 if dealing with negative class\n","def custom_forward(inputs, position_ids=None, attention_mask=None):\n","    preds = predict(inputs,\n","                   position_ids=position_ids,\n","                   attention_mask=attention_mask\n","                   )\n","    return torch.softmax(preds, dim = 1)"]},{"cell_type":"markdown","source":["To get the attributions, we perform Integrated Gradients using the model's embeddings and pass in our custom forward function."],"metadata":{"id":"bQaYaSDf0buh"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"4uDuDrip0_Ip"},"outputs":[],"source":["lig = LayerIntegratedGradients(custom_forward, model.longformer.embeddings)"]},{"cell_type":"markdown","source":["Create functions that give us the input ids and the position ids for the text we want to examine. Furthermore, it also returns the baselines we want for integrated gradients. In this case, every token in our baseline, is a padding token."],"metadata":{"id":"bqbvfvXv0VTp"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"UgBSBpz-0_In"},"outputs":[],"source":["max_length = 2046\n","def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id):\n","\n","    text_ids = tokenizer.encode(text, truncation = True, add_special_tokens=False, max_length = max_length)\n","    # construct input token ids\n","    input_ids = [cls_token_id] + text_ids + [sep_token_id]\n","    # construct reference token ids \n","    ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n","\n","    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(text_ids)\n","\n","def construct_input_ref_pos_id_pair(input_ids):\n","    seq_length = input_ids.size(1)\n","\n","    #taken from the longformer implementation\n","    mask = input_ids.ne(ref_token_id).int()\n","    incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask\n","    position_ids = incremental_indices.long().squeeze() + ref_token_id\n","\n","    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n","    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n","\n","    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n","    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n","    return position_ids, ref_position_ids\n","    \n","def construct_attention_mask(input_ids):\n","    return torch.ones_like(input_ids)"]},{"cell_type":"markdown","source":["The attributions returned has very high dimensionality and we just want a single number for every token in our example, so we sum over the last dimension and squeeze the result to get an array of shape (seq_len). You may notice that we are not normalizing the attributions here. It's okay because we will normalize it later."],"metadata":{"id":"PRXe-mIcBGFo"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"lIeE9P7b0_Ir"},"outputs":[],"source":["def summarize_attributions(attributions):\n","    attributions = attributions.sum(dim=-1).squeeze(0)\n","    return attributions"]},{"cell_type":"markdown","source":["For use in later functions, we want to store the attributions we find in a dictionary where the key is the example number."],"metadata":{"id":"8-wA6Nd4BIeb"}},{"cell_type":"code","source":["all_attributions = {}"],"metadata":{"id":"wf1SNgF73l1K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["On the other hand, if you have a dictionary of attributions already saved, you can import it as follows. Replace the path with a path to your own dictionary."],"metadata":{"id":"aall1F3lCpaf"}},{"cell_type":"code","source":["# all_attributions = torch.load('/content/drive/MyDrive/cogs402longformer/results/papers/papers_attributions/example_attrib_dict_all.pt')"],"metadata":{"id":"g4nc7qSvUedJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this block of code, we iterate over the entire dataset, obtain the input_ids, position_ids, attention_mask and the baseline for integrated gradients, perform integrated gradients, and store the result in the dictionary. If you have already loaded your attributions, you can skip this step. Increase the number of steps if you desire, but it may take a very long time to run. At the end of an iteration, we also save the dictionary.\n","\n","Note: the attributions will be with respect to the positive class, meaning positive attributions have more influence in the model predicting positive and negative attributions will be more influential in predicting negative."],"metadata":{"id":"Ap4wQhI7CqJ2"}},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","for i in tqdm(range(len(cogs402_ds))):\n","  if str(i) not in all_attributions:\n","    #get input ids, position ids and attention mask for integrated gradients\n","    text = cogs402_ds['text'][i]\n","    label = cogs402_ds['labels'][i]\n","\n","    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n","    position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n","    attention_mask = construct_attention_mask(input_ids)\n","\n","    attributions = lig.attribute(inputs=input_ids,\n","                                      baselines=ref_input_ids,\n","                                      additional_forward_args=(position_ids, attention_mask),\n","                                      target=1,\n","                                      n_steps=50,\n","                                      internal_batch_size = 2)\n","\n","    attributions_sum = summarize_attributions(attributions)\n","\n","    all_attributions[str(i)] = attributions_sum.detach().cpu().numpy()\n","\n","    # torch.save(all_attributions, '/content/drive/MyDrive/cogs402longformer/results/papers/papers_attributions/example_attrib_dict_all.pt')"],"metadata":{"id":"ItEOhzhNTdxF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659512782148,"user_tz":420,"elapsed":308626,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"e2c49ce4-167a-4687-c7c0-519d8b74b13b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 12/12 [05:08<00:00, 25.71s/it]\n"]}]},{"cell_type":"markdown","source":["## Get Attentions"],"metadata":{"id":"fxugVBIAZksG"}},{"cell_type":"markdown","source":["We then get the attentions and global attentions so we can compare with the attributions."],"metadata":{"id":"BGDWlUa3GvNS"}},{"cell_type":"markdown","source":["A unique property of the longformer model is that the matrix output for the attention is not a seq_len x seq_len output. Each token can only attend to the preceeding w/2 tokens and the succeeding w/2 tokens, dictated by whatever you choose the model's attention window w to be. Another name for this is called the sliding window attention. Therefore, we need to convert sliding attention matrix to correct seq_len x seq_len matrix to remain consistent with other types of Transformer Neural Networks.\n","\n","To do so, we run the following 3 functions. Our attentions will change from a output attention tensor of shape (layer, head, seq_len, x + attention_window + 1) and a global attention tensor of shape (layer, head, seq_len, x) to a single tensor of shape (layer, batch, head, seq_len, seq_len). More information about the functions can be found [here](https://colab.research.google.com/drive/1Kxx26NtIlUzioRCHpsR8IbSz_DpRFxEZ#scrollTo=t_XCoyTsQKAU)."],"metadata":{"id":"dISC7-FaG2Lh"}},{"cell_type":"code","source":["def create_head_matrix(output_attentions, global_attentions):\n","    new_attention_matrix = torch.zeros((output_attentions.shape[0], \n","                                      output_attentions.shape[0]))\n","    for i in range(output_attentions.shape[0]):\n","        test_non_zeroes = torch.nonzero(output_attentions[i]).squeeze()\n","        test2 = output_attentions[i][test_non_zeroes[1:]]\n","        new_attention_matrix_indices = test_non_zeroes[1:]-257 + i\n","        new_attention_matrix[i][new_attention_matrix_indices] = test2\n","        new_attention_matrix[i][0] = output_attentions[i][0]\n","        new_attention_matrix[0] = global_attentions.squeeze()[:output_attentions.shape[0]]\n","    return new_attention_matrix\n","\n","\n","def attentions_all_heads(output_attentions, global_attentions):\n","    new_matrix = []\n","    for i in range(output_attentions.shape[0]):\n","        matrix = create_head_matrix(output_attentions[i], global_attentions[i])\n","        new_matrix.append(matrix)\n","    return torch.stack(new_matrix)\n","\n","def all_layers(output_attentions, global_attentions):\n","    new_matrix = []\n","    for i in range(output_attentions.shape[0]):\n","        matrix = attentions_all_heads(output_attentions[i], global_attentions[i])\n","        new_matrix.append(matrix)\n","    return torch.stack(new_matrix)"],"metadata":{"id":"AFyZpNST1JRr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Some heads may be more important than others so we scale each attention matrix by their respective head and layer. The notebook used to get head importance is [here](https://colab.research.google.com/drive/1O4QCi8ewBp7asegKqySRflTQZ9HeH8mQ?usp=sharing). However, its possible that you might not want to scale the attentions, in which case you can ignore this section."],"metadata":{"id":"WJynBkquG9cv"}},{"cell_type":"code","source":["def scale_by_importance(attention_matrix, head_importance):\n","  new_matrix = np.zeros_like(attention_matrix)\n","  for i in range(attention_matrix.shape[0]):\n","    head_importance_layer = head_importance[i]\n","    new_matrix[i] = attention_matrix[i] * np.expand_dims(head_importance_layer, axis=(1))\n","  return new_matrix"],"metadata":{"id":"nDuqZlIZ1NSr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["head_importance = torch.load(\"/content/drive/MyDrive/fakeclinicalnotes/t3-visapplication/notes/head_importance.pt\")"],"metadata":{"id":"Bt_2TsKb1ML5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following block of code creates a new dictionary of attention matrices. Each key corresponds to their respective example in the dataset (range 0 - dataset length) and stores a layer x head x seq_len matrix of attentions for each key"],"metadata":{"id":"fCZJ4gIKO8ys"}},{"cell_type":"code","source":["all_attentions = {}"],"metadata":{"id":"F8QC9aqjtiZ6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["On the other hand, if you have a dictionary of attentions already saved, you can import it as follows. Replace the path with a path to your own dictionary."],"metadata":{"id":"m-20QqqlPNs2"}},{"cell_type":"code","source":["# all_attentions = torch.load('/content/drive/MyDrive/cogs402longformer/results/papers/full_attention_matrices/papers_atten_summed_dict.pt')"],"metadata":{"id":"JeKFfGfueDPZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In this block of code, we iterate over the entire dataset, obtain the input_ids, position_ids, attention_mask, pass the inputs into the model, obtain the output, convert the attention matrix and store the result in the dictionary. If you have already loaded your attentions, you can skip this step. Increase the number of steps if you desire, but it may take a very long time to run. At the end of every 10 iterations, we also save the dictionary. We do every 10 as saving dictionaries of attentions takes a bit of time."],"metadata":{"id":"F4_6027PElWG"}},{"cell_type":"code","source":["from tqdm import tqdm\n","\n","for i in tqdm(range(len(cogs402_ds))):\n","  if str(i) not in all_attentions:\n","    #get input ids, position ids and attention mask for integrated gradients\n","    text = cogs402_ds['text'][i]\n","    label = cogs402_ds['labels'][i]\n","\n","    input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n","    position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n","    attention_mask = construct_attention_mask(input_ids)\n","\n","    output = model(input_ids.cuda(), attention_mask=attention_mask.cuda(), labels=torch.tensor(label).cuda(), output_attentions = True)\n","\n","    batch_attn = output[-2]\n","\n","    # We are working with one item at a time, so we squeeze the tensor to remove the batch axis in both tensors\n","    # shape: (layer, head, seq_len, x+attention_window+1)\n","    output_attentions = torch.stack(batch_attn).cpu().squeeze()\n","    global_attention = output[-1]\n","\n","    # shape: (layer, head, seq_len, x)\n","    output_global_attentions = torch.stack(global_attention).cpu().squeeze()\n","\n","    converted_mat = all_layers(output_attentions, output_global_attentions).detach().cpu().numpy()\n","    \n","    attention_matrix_summed = converted_mat.sum(axis=2)\n","    all_attentions[str(i)] = attention_matrix_summed\n","\n","#     if i%10 == 9:\n","#       torch.save(all_attentions, '/content/drive/MyDrive/cogs402longformer/results/papers/full_attention_matrices/papers_atten_summed_dict.pt')"],"metadata":{"id":"mx69KUz20KmC","colab":{"base_uri":"https://localhost:8080/"},"outputId":"49fd06b6-9508-42af-f73f-fe05ad050729","executionInfo":{"status":"ok","timestamp":1659513102281,"user_tz":420,"elapsed":319205,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 12/12 [05:19<00:00, 26.61s/it]\n"]}]},{"cell_type":"markdown","source":["### Scaling the Attention"],"metadata":{"id":"7PEQxg77LLvw"}},{"cell_type":"markdown","source":["We will then scale the summed attention by head importance. If you do not wish to scale the attentions, there is a section later in the notebook that does not perform scaling. However, we will do so here."],"metadata":{"id":"UZAi6NYZK-n0"}},{"cell_type":"markdown","source":["The following are two dictionaries of attention weights for each token (how much each token is attended to), weighted by head importance, for layer 12 and over all layers. The number of keys in the dictionary is 1070 (number of items in the validation set) and each key contains an array of shape (seq_len)."],"metadata":{"id":"DyGjnOpaM8dZ"}},{"cell_type":"code","source":["# all_attentions_final = torch.load('/content/drive/MyDrive/cogs402longformer/results/papers/full_attention_matrices/example_atten_dict_12.pt')\n","# all_attentions_all = torch.load('/content/drive/MyDrive/cogs402longformer/results/papers/full_attention_matrices/example_atten_dict_all.pt')"],"metadata":{"id":"eJuGcLQpS-Ky"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Another example of importing dictionaries of attentions. These two dictionaries store the summed attentions for the layers 1-6 and 7-12 respectively. "],"metadata":{"id":"TUGT5wEXrs4c"}},{"cell_type":"code","source":["# all_attentions_lower = torch.load('/content/drive/MyDrive/cogs402longformer/results/papers/full_attention_matrices/example_atten_dict_lower.pt')\n","# all_attentions_upper = torch.load('/content/drive/MyDrive/cogs402longformer/results/papers/full_attention_matrices/example_atten_dict_upper.pt')"],"metadata":{"id":"tckSMU4yqBhU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This block of code iterates through the entire dataset, scales the attention matrix for each example by head importance (optional), and sums up the attention. For this notebook, we sum the attention over 6 layers and the last 6 layers separately.\n","\n","In this case, when taking a specific layer, you pick the layer you want (replace 11 with whatever layer you wish) and then we sum over all of the heads.\n","\n","When taking a range of layers, you either want to specify a range (e.g. attention_matrix_summed[0:6]) or leave as it is to sum over all layers. Then we sum up the layers and the heads.\n","\n","Lastly, we save the dictionary of summed attentions for easy access on repeat runs or other notebooks.\n","\n","The two dictionaries of attentions are currently labeled _lower and _upper for the use case, but should be changed to fit the task at hand."],"metadata":{"id":"YDqk5Y1rr7eU"}},{"cell_type":"code","source":["all_attentions_one = {}\n","all_attentions_two = {}\n","for i in tqdm(range(len(cogs402_ds))):\n","  if str(i) not in all_attentions_one and str(i) not in all_attentions_two:\n","\n","    att_mat = all_attentions[str(i)]\n","\n","    att_mat = scale_by_importance(att_mat, head_importance)\n","\n","    # how to pick a range of layers\n","    att_mat_one = att_mat\n","\n","    # Sum the attentions for a group of layers\n","    attention_one = att_mat_one.sum(axis=1)\n","    attention_one = attention_one.sum(axis=0)\n","    all_attentions_one[str(i)] = attention_one\n","\n","    # attention_two = att_mat_two.sum(axis=1)\n","    # attention_two = attention_two.sum(axis=0)\n","    # all_attentions_upper[str(i)] = attention_two\n","\n","    #template for single layer\n","    attention_two = att_mat[11].sum(axis=0)\n","    all_attentions_two[str(i)] = attention_two\n","    \n","#     torch.save(all_attentions_lower, '/content/drive/MyDrive/cogs402longformer/results/papers/full_attention_matrices/example_atten_dict_lower.pt')\n","#     torch.save(all_attentions_upper, '/content/drive/MyDrive/cogs402longformer/results/papers/full_attention_matrices/example_atten_dict_upper.pt')"],"metadata":{"id":"ZWeFQNW8MmJL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659513102495,"user_tz":420,"elapsed":221,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"63a29b0b-2602-4fa9-d843-ca5086ee68b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 12/12 [00:00<00:00, 69.30it/s]\n"]}]},{"cell_type":"markdown","source":["## Finding the Similarities "],"metadata":{"id":"Vp9Vj8sqZoj3"}},{"cell_type":"markdown","source":["By using the attention and attribution dictionaries we created, we can now do a series of comparisons for each example in our dataset."],"metadata":{"id":"P9NCgV58Fyy6"}},{"cell_type":"markdown","source":["Short helper function to normalize the attentions and attributions. "],"metadata":{"id":"6cwcV4jXNWe-"}},{"cell_type":"code","source":["def normalize(data):\n","    return (data - np.min(data)) / (np. max(data) - np.min(data))"],"metadata":{"id":"ukncMseTj1-K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following block of code iterates through the entire dataset, grabs the appropriate attributions and attentions from their respective dictionaries, and computes the cosine similarities, [Kendalltau](https://www.jstor.org/stable/2332226), and [Rank-biased Overlap (RBO)](https://dl.acm.org/doi/10.1145/1852102.1852106). Note that the array of attributions will be absolute valued for all cases other than the raw similarities. We absolute value the attributions because negative attributions would not necessarily mean that they have the lowest attention, rather they might have really high attention as they are more likely to help the model predict the negative class, and might be something the attentions picked up by the model. The output is a dataframe containing the computed similarities.\n","\n","For the cosine similarites, we are comparing the raw un-normalized attributions and attentions, the normalized arrays, arrays with values below the median masked, arrays with values below the mean masked, and the ranks of the tokens\n","\n","More information about the similarities can be found [here](https://colab.research.google.com/drive/14a1tOimrRbLlXd0rbtY-UTcizR5GDd4F#scrollTo=gB7w8fsMHFT2)"],"metadata":{"id":"bw08J8acuEY4"}},{"cell_type":"code","source":["from numpy.linalg import norm\n","import scipy.stats as stats\n","import rbo\n","from tqdm import tqdm"],"metadata":{"id":"t4ppKBXJC3AB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def jaccard_similarity(set1, set2):\n","    intersection = len(list(set1.intersection(set2)))\n","    union = (len(set1) + len(set2)) - intersection\n","    return float(intersection) / union"],"metadata":{"id":"H6SJ3JwefFqM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def get_sim_dataframe(cogs402_ds, all_attentions, all_attributions):\n","\n","  dataframe = []\n","\n","  for i in tqdm(range(len(cogs402_ds))):\n","    #raw sims\n","    exam_attrib = all_attributions[str(i)]\n","    attention_final_layer = all_attentions[str(i)]\n","    \n","    exam_attrib = exam_attrib[:len(all_attentions[str(i)])]\n","\n","    \n","    cosine_raw = np.dot(exam_attrib, attention_final_layer) / (norm(exam_attrib)*norm(attention_final_layer))\n","\n","    #normalized sims\n","    attention_final_layer2 = normalize(attention_final_layer)\n","\n","    exam_attrib2 = np.abs(exam_attrib)\n","    exam_attrib2 = normalize(exam_attrib2)\n","  \n","    cosine = np.dot(exam_attrib2, attention_final_layer2) / (norm(exam_attrib2)*norm(attention_final_layer2))\n","\n","    #Jaccard Sim for 95th percentile\n","    exam_attrib3 = np.copy(exam_attrib2)\n","    median_exam = np.percentile(exam_attrib3, 95)\n","    exam_attrib3[exam_attrib3 < median_exam] = 0\n","\n","    exam_attrib_top = np.flatnonzero(exam_attrib3)\n","    exam_attrib_top = set(exam_attrib_top)\n","\n","    attention_final_layer3 = np.copy(attention_final_layer2)\n","    median_12 = np.percentile(attention_final_layer3, 95)\n","    attention_final_layer3[attention_final_layer3 < median_12] = 0\n","\n","    attention_final_layer_top = np.flatnonzero(attention_final_layer3)\n","    attention_final_layer_top = set(attention_final_layer_top)\n","\n","    jaccard_sim = jaccard_similarity(exam_attrib_top, attention_final_layer_top)\n","    \n","    cosine_med = np.dot(exam_attrib3, attention_final_layer3) / (norm(exam_attrib3)*norm(attention_final_layer3))\n","\n","    #sim using the ranks of the tokens\n","    exam_attrib_rank = np.copy(exam_attrib2)\n","    order_attrib = exam_attrib_rank.argsort()[::-1]\n","    ranks_attrib = order_attrib.argsort()\n","\n","    attention_final_layer_rank = np.copy(attention_final_layer2)\n","    order = attention_final_layer_rank.argsort()[::-1]\n","    ranks = order.argsort()\n","\n","    cosine_rank = np.dot(ranks_attrib, ranks) / (norm(ranks_attrib)*norm(ranks))\n","\n","    # other similarity metrics like RBO and Kendalltau\n","    tau, p_value = stats.kendalltau(ranks_attrib, ranks)\n","\n","    rbo_sim = rbo.RankingSimilarity(order_attrib, order).rbo()\n","\n","    d = {'example': i, 'similarity normalized': cosine, 'similarity raw': cosine_raw, \"sim w/ ranks\":cosine_rank, 'Jaccard_sim':cosine_med,\n","        \"kendalltau\": tau, \"rbo\":rbo_sim, 'jaccard sim 95th': jaccard_sim}\n","    dataframe.append(d)\n","\n","  return pd.DataFrame(dataframe)"],"metadata":{"id":"dmKF8EBP2Nqz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Apply the above function for our two cases, upper layers, and lower layers."],"metadata":{"id":"Kpy-tAPEHfiQ"}},{"cell_type":"code","source":["df = get_sim_dataframe(cogs402_ds, all_attentions_two, all_attributions)"],"metadata":{"id":"gQIFZTQzlAEI","executionInfo":{"status":"ok","timestamp":1659513102909,"user_tz":420,"elapsed":258,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ecea8d59-361f-4f77-f08d-dca113a2e62a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 12/12 [00:00<00:00, 146.25it/s]\n"]}]},{"cell_type":"code","source":["df2 = get_sim_dataframe(cogs402_ds, all_attentions_one, all_attributions)"],"metadata":{"id":"gtCEZaQFOgwh","executionInfo":{"status":"ok","timestamp":1659513102909,"user_tz":420,"elapsed":20,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"49a10f9e-9353-4668-ac52-9ecee30cb888"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 12/12 [00:00<00:00, 182.38it/s]\n"]}]},{"cell_type":"code","source":["df"],"metadata":{"id":"Yn97-oeTC_qt","colab":{"base_uri":"https://localhost:8080/","height":426},"executionInfo":{"status":"ok","timestamp":1659513102909,"user_tz":420,"elapsed":12,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"a35cb752-6c07-4bb1-c6bf-1f7df9ef207d"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["    example  similarity normalized  similarity raw  sim w/ ranks  Jaccard_sim  \\\n","0         0               0.788554       -0.193915      0.756329     0.024793   \n","1         1               0.768917       -0.235910      0.753975     0.056477   \n","2         2               0.781920       -0.221427      0.763064     0.046645   \n","3         3               0.774407       -0.227337      0.766804     0.096124   \n","4         4               0.780943       -0.278108      0.760556     0.048976   \n","5         5               0.780936       -0.178520      0.763249     0.057046   \n","6         6               0.789521       -0.247086      0.766655     0.081115   \n","7         7               0.752455       -0.229324      0.751315     0.016496   \n","8         8               0.768489       -0.190230      0.754346     0.044790   \n","9         9               0.778412       -0.231626      0.756542     0.047974   \n","10       10               0.768090       -0.201108      0.755917     0.076264   \n","11       11               0.770515       -0.203179      0.759775     0.069456   \n","\n","    kendalltau       rbo  jaccard sim 95th  \n","0     0.017554  0.502961          0.014778  \n","1     0.011294  0.498206          0.035176  \n","2     0.035312  0.509722          0.030000  \n","3     0.045452  0.520142          0.061856  \n","4     0.029136  0.508702          0.026549  \n","5     0.034823  0.508402          0.035176  \n","6     0.045125  0.510215          0.045161  \n","7     0.004231  0.494442          0.009804  \n","8     0.011975  0.500024          0.030000  \n","9     0.017818  0.501214          0.022989  \n","10    0.016005  0.505391          0.045685  \n","11    0.026501  0.507409          0.040404  "],"text/html":["\n","  <div id=\"df-daca8968-0ec4-4bad-af2b-2e3dcc130d09\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>example</th>\n","      <th>similarity normalized</th>\n","      <th>similarity raw</th>\n","      <th>sim w/ ranks</th>\n","      <th>Jaccard_sim</th>\n","      <th>kendalltau</th>\n","      <th>rbo</th>\n","      <th>jaccard sim 95th</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.788554</td>\n","      <td>-0.193915</td>\n","      <td>0.756329</td>\n","      <td>0.024793</td>\n","      <td>0.017554</td>\n","      <td>0.502961</td>\n","      <td>0.014778</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.768917</td>\n","      <td>-0.235910</td>\n","      <td>0.753975</td>\n","      <td>0.056477</td>\n","      <td>0.011294</td>\n","      <td>0.498206</td>\n","      <td>0.035176</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.781920</td>\n","      <td>-0.221427</td>\n","      <td>0.763064</td>\n","      <td>0.046645</td>\n","      <td>0.035312</td>\n","      <td>0.509722</td>\n","      <td>0.030000</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.774407</td>\n","      <td>-0.227337</td>\n","      <td>0.766804</td>\n","      <td>0.096124</td>\n","      <td>0.045452</td>\n","      <td>0.520142</td>\n","      <td>0.061856</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>0.780943</td>\n","      <td>-0.278108</td>\n","      <td>0.760556</td>\n","      <td>0.048976</td>\n","      <td>0.029136</td>\n","      <td>0.508702</td>\n","      <td>0.026549</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>5</td>\n","      <td>0.780936</td>\n","      <td>-0.178520</td>\n","      <td>0.763249</td>\n","      <td>0.057046</td>\n","      <td>0.034823</td>\n","      <td>0.508402</td>\n","      <td>0.035176</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>6</td>\n","      <td>0.789521</td>\n","      <td>-0.247086</td>\n","      <td>0.766655</td>\n","      <td>0.081115</td>\n","      <td>0.045125</td>\n","      <td>0.510215</td>\n","      <td>0.045161</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>7</td>\n","      <td>0.752455</td>\n","      <td>-0.229324</td>\n","      <td>0.751315</td>\n","      <td>0.016496</td>\n","      <td>0.004231</td>\n","      <td>0.494442</td>\n","      <td>0.009804</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>8</td>\n","      <td>0.768489</td>\n","      <td>-0.190230</td>\n","      <td>0.754346</td>\n","      <td>0.044790</td>\n","      <td>0.011975</td>\n","      <td>0.500024</td>\n","      <td>0.030000</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>9</td>\n","      <td>0.778412</td>\n","      <td>-0.231626</td>\n","      <td>0.756542</td>\n","      <td>0.047974</td>\n","      <td>0.017818</td>\n","      <td>0.501214</td>\n","      <td>0.022989</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>10</td>\n","      <td>0.768090</td>\n","      <td>-0.201108</td>\n","      <td>0.755917</td>\n","      <td>0.076264</td>\n","      <td>0.016005</td>\n","      <td>0.505391</td>\n","      <td>0.045685</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>11</td>\n","      <td>0.770515</td>\n","      <td>-0.203179</td>\n","      <td>0.759775</td>\n","      <td>0.069456</td>\n","      <td>0.026501</td>\n","      <td>0.507409</td>\n","      <td>0.040404</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-daca8968-0ec4-4bad-af2b-2e3dcc130d09')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-daca8968-0ec4-4bad-af2b-2e3dcc130d09 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-daca8968-0ec4-4bad-af2b-2e3dcc130d09');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":36}]},{"cell_type":"markdown","source":["Since we have converted our similarities into a dataframe, we can easily find things like the mean and the max of the similarities, which we can use to pick out an example if needed."],"metadata":{"id":"rBcXyOuGHlJN"}},{"cell_type":"code","source":["df.max()"],"metadata":{"id":"SY9blyjLDeUt","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659513102910,"user_tz":420,"elapsed":12,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"08d1fd60-ef4e-4bbe-8bfc-f72646f4652e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["example                  11.000000\n","similarity normalized     0.789521\n","similarity raw           -0.178520\n","sim w/ ranks              0.766804\n","Jaccard_sim               0.096124\n","kendalltau                0.045452\n","rbo                       0.520142\n","jaccard sim 95th          0.061856\n","dtype: float64"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["df.min()"],"metadata":{"id":"U8Egh8vVEL3w","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659513102910,"user_tz":420,"elapsed":9,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"c27eaa4c-1523-4d2e-c824-9f54af4554a0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["example                  0.000000\n","similarity normalized    0.752455\n","similarity raw          -0.278108\n","sim w/ ranks             0.751315\n","Jaccard_sim              0.016496\n","kendalltau               0.004231\n","rbo                      0.494442\n","jaccard sim 95th         0.009804\n","dtype: float64"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["df2.max()"],"metadata":{"id":"VIDkyg0RDqTZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659513103351,"user_tz":420,"elapsed":446,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"c462686c-28f5-4864-b7f7-7f200fc4ae0f"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["example                  11.000000\n","similarity normalized     0.788029\n","similarity raw           -0.178138\n","sim w/ ranks              0.770644\n","Jaccard_sim               0.114131\n","kendalltau                0.056028\n","rbo                       0.531626\n","jaccard sim 95th          0.072848\n","dtype: float64"]},"metadata":{},"execution_count":39}]},{"cell_type":"code","source":["df.mean()"],"metadata":{"id":"2HPzjI4i4Oz0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659513103352,"user_tz":420,"elapsed":10,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"a760b418-b60e-412c-fa5b-40e0a6778ef1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["example                  5.500000\n","similarity normalized    0.775263\n","similarity raw          -0.219814\n","sim w/ ranks             0.759044\n","Jaccard_sim              0.055513\n","kendalltau               0.024602\n","rbo                      0.505569\n","jaccard sim 95th         0.033131\n","dtype: float64"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","source":["### Storing our Results"],"metadata":{"id":"LduQ8w_yIaW5"}},{"cell_type":"markdown","source":["Given that this is meant to be preliminary findings about the comparison of attributions and attentions, you likely want to run through this multiple times with multiple configurations to see which configuration gives the most interesting results. As such, we want to store the results found above in a manner that we can easily access but also easy to view. Below we demonstrate how we stored the results of the means in a .csv file that we export."],"metadata":{"id":"XQrU4q-SI-Q-"}},{"cell_type":"markdown","source":["We convert our results into a dictionary so we can later create a dataframe and store this information. Notably, we want the attention this example operates on in the form of text (e.g. attrib_vs_layer_12), which means **you will have to change the text depending on your context**, the mean cosine similarities, the mean cosine similarities for the ranks, the kendall tau coefficient, the RBO, as well as whether they are scaled, and/or alphanumeric tokens only."],"metadata":{"id":"ZKZKg9mcY6XV"}},{"cell_type":"code","source":["result_dataframe = []\n","d = {'example': \"attrib_vs_layer12_attn\", 'mean_cosine_sim': df.mean()[\"similarity normalized\"].round(6), \"mean_cosine_sim_ranks\":df.mean()[\"sim w/ ranks\"].round(6),\n","        \"mean_kendall_tau\": df.mean()[\"kendalltau\"].round(6), \"mean_RBO\":df.mean()[\"rbo\"].round(6), 'mean_Jaccard_95th':df.mean()['jaccard sim 95th'].round(6), \"scaled\":True, \"alpha_only\":False}\n","result_dataframe.append(d)"],"metadata":{"id":"8I3H3SR2Nj5w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df2.mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kdhfJcyEYxpZ","executionInfo":{"status":"ok","timestamp":1659513103352,"user_tz":420,"elapsed":7,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"00e54f99-2984-4a9a-8617-a13188004741"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["example                  5.500000\n","similarity normalized    0.770697\n","similarity raw          -0.220619\n","sim w/ ranks             0.763060\n","Jaccard_sim              0.054570\n","kendalltau               0.035511\n","rbo                      0.516411\n","jaccard sim 95th         0.034858\n","dtype: float64"]},"metadata":{},"execution_count":42}]},{"cell_type":"markdown","source":["We do the same, changing the example text to ...upper_attn rather than lower attention because df2 stores the values comparing attribution against upper attention layers."],"metadata":{"id":"WSMXkDj2ZyV5"}},{"cell_type":"code","source":["d2 = {'example': \"attrib_vs_all_layer_attn\", 'mean_cosine_sim': df2.mean()[\"similarity normalized\"].round(6), \"mean_cosine_sim_ranks\":df2.mean()[\"sim w/ ranks\"].round(6),\n","        \"mean_kendall_tau\": df2.mean()[\"kendalltau\"].round(6), \"mean_RBO\":df2.mean()[\"rbo\"].round(6), 'mean_Jaccard_95th':df2.mean()['jaccard sim 95th'].round(6), \"scaled\":True, \"alpha_only\":False}\n","result_dataframe.append(d2)"],"metadata":{"id":"OVHDWIwgO9Se"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Alphanumeric Tokens only"],"metadata":{"id":"o0z0xod8JnLi"}},{"cell_type":"markdown","source":["The following block of code iterates through the entire dataset in the same manner as the above version, grabs the appropriate attributions and attentions from their respective dictionaries, and computes the cosine similarities, kendall tau coefficients, and the RBO. The output is a dataframe containing the computed similarities. The difference is that this block of code masks the values of tokens that are non-alphanumeric (e.g. \".,][?\") before calculating similarities.\n","\n","More information about the similarities can be found [here](https://colab.research.google.com/drive/14a1tOimrRbLlXd0rbtY-UTcizR5GDd4F#scrollTo=gB7w8fsMHFT2)"],"metadata":{"id":"HYT4iQynt6-m"}},{"cell_type":"code","source":["import nltk\n","from transformers import AutoTokenizer\n","nltk.download('stopwords')\n","tokenizer2 = AutoTokenizer.from_pretrained('allenai/longformer-base-4096', add_prefix_space=True)"],"metadata":{"id":"SoudPRwHldKZ","colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["03ea1d3264a64819bc3d4527a74fa793","26712c36d67840b38f8f0c82a56cb799","26a99ad2d866437792f6b6bea4847b64","69bff586a4634d12be227f1c365ab975","8fffb6dac2fe44d1963a9a5fcd624b51","3008bae0fc4340149dc036d10a97d9fb","5e06e1b5282a4658bef3d10c3c84a6bf","fb789ded969945efb39ea360dca78990","f2d81834a22e487eb165f6499deadadf","9181632a84894f67b11f7b748e94d2ca","e074bfc0fa2d46db948671057606e490"]},"executionInfo":{"status":"ok","timestamp":1659513483219,"user_tz":420,"elapsed":2549,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"d5377e7a-9347-4e90-8a81-20d459d919f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading tokenizer.json:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03ea1d3264a64819bc3d4527a74fa793"}},"metadata":{}}]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","all_stopwords = stopwords.words('english')\n","all_stopwords.append(\" \")\n","stopwords = set(tokenizer2.tokenize(all_stopwords, is_split_into_words =True))\n","stopwords.update(all_stopwords)\n","print(stopwords)"],"metadata":{"id":"r0ahJeMAleBg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659513483220,"user_tz":420,"elapsed":5,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"09a5dca6-2186-4223-af71-5236d39b1195"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["{'Ġour', 'Ġhers', \"hadn't\", 'Ġbeen', 'o', \"won't\", 'Ġmight', 'Ġboth', 'Ġbut', 'were', 's', 'yours', 'Ġyours', 'about', 'Ġhim', 'too', 'Ġisn', 'Ġnor', 'haven', 'Ġher', 'if', \"she's\", 'Ġmore', 'Ġm', 'couldn', 'Ġby', 'Ġhasn', 'until', 'more', 'by', 've', 'these', 'just', \"mightn't\", 'below', 'Ġabove', 'Ġdidn', 'Ġhave', 'will', 'Ġis', 'Ġdoing', 'through', 'very', 'Ġll', 'in', 'this', 'Ġdoesn', 'an', 'Ġthem', 'Ġonce', 'Ġyour', 'wasn', 'Ġmust', 'Ġsuch', 'Ġs', 'Ġof', 'Ġor', 'there', 'be', 'with', 'those', 'Ġwhile', \"'s\", 'Ġnot', 'me', 'themselves', \"wasn't\", 'Ġhis', 'your', \"wouldn't\", 'once', 'Ġthere', 'Ġagain', 'Ġyourselves', 'Ġam', 'Ġto', 'then', 'Ġneed', 'aren', 'Ġhadn', 'won', 'Ġunder', 'weren', 'Ġall', 'after', 'each', 'Ġwho', 'Ġany', 'Ġshe', 'Ġo', 'n', 'Ġi', 'Ġare', 'theirs', 'yourselves', \"it's\", 'Ġan', 'Ġother', 'Ġy', 'Ġno', \"you'd\", 'can', 'Ġagainst', 'Ġat', \"don't\", 'you', 'Ġand', 'Ġwhy', 'Ġwere', 'further', 'all', 'how', 'Ġown', 'to', 'Ġnow', 'Ġso', \"weren't\", 'Ġwhen', 'Ġtheirs', 'Ġits', 'both', \"'d\", 'Ġve', 'up', 'have', 'himself', 'Ġvery', 'yourself', 'it', 'he', 'Ġyou', 'she', \"'ll\", 'most', 'but', 'Ġaren', 'was', 'my', 'had', 'Ġwas', 'didn', 'Ġthemselves', 'from', 'm', 'Ġover', 'Ġre', \"'re\", 'than', 'Ġuntil', 'being', 'Ġme', 'as', 'Ġa', 'ma', 'shan', 'Ġsome', 'Ġif', 'Ġbe', 'the', 'wouldn', 'Ġafter', 'for', 'of', 'myself', 'her', 'Ġwill', 'Ġdo', \"shan't\", 'Ġbetween', 'ain', 'doesn', 'few', 'doing', 'Ġabout', \"'ve\", 'Ġthe', 'Ġon', 'does', 'over', 'a', \"should've\", 'hadn', 'are', 'Ġhere', \"doesn't\", 'Ġwe', 'Ġshould', 'against', 'above', 'him', 'Ġthose', 'should', \"that'll\", 'Ġdon', 'them', 'here', 'some', 'Ġfor', 'Ġd', 'been', 'on', 'while', 'out', 'Ġthat', 'Ġonly', 'Ġma', 'having', 'between', 'Ġeach', 'own', 'their', 'Ġbelow', 'Ġwasn', 'Ġit', 'Ġbeing', 'because', 'Ġwhat', 'where', 'Ġthan', \"you've\", \"you'll\", 'Ġbefore', 'ours', 'Ġhow', \"needn't\", 'mightn', 'ourselves', 'Ġhaven', 'Ġherself', 'during', 'Ġyourself', 'Ġhimself', 'nor', 'into', 'Ġinto', 'Ġdown', 'Ġcouldn', 'Ġourselves', 'our', 'Ġhe', 'under', 'has', \"shouldn't\", 'only', 'Ġours', 'Ġin', 'shouldn', 'Ġthese', 'that', 'hasn', 'hers', 'its', 'which', 'what', 'Ġwith', 'Ġfew', 'll', 'same', 'Ġtheir', 'other', 'now', 'Ġthis', 'isn', 'Ġweren', 'Ġhad', 'Ġwhere', 'Ġcan', 'Ġout', \"mustn't\", \"couldn't\", 'Ġwouldn', 'Ġain', 'when', 'such', ' ', 'off', 'Ġt', \"'t\", 'Ġjust', 'Ġthey', 'not', 'am', 'his', 'Ġhas', \"aren't\", 'Ġtoo', 'Ġsame', 'whom', 'at', 'Ġitself', 'Ġsh', 're', 'y', 'Ġduring', 'we', 'herself', 'Ġup', 'Ġmyself', 'don', 'Ġshouldn', 'Ġdoes', \"haven't\", 'down', 'so', \"you're\", 'before', 'again', 'needn', 'Ġwhich', 'Ġwon', 'Ġmost', 'do', 'Ġoff', 'mustn', 'any', 'Ġfrom', 'itself', 'and', \"hasn't\", 'no', 'Ġthen', 'Ġfurther', 'they', 'd', 'Ġhaving', 'Ġbecause', 'who', 'Ġwhom', 'Ġthrough', \"didn't\", 't', 'or', 'did', 'why', 'i', 'is', \"isn't\", 'Ġdid', 'Ġas', 'Ġ', 'Ġmy'}\n"]}]},{"cell_type":"code","source":["def get_sim_dataframe_alpha(cogs402_ds, all_attentions, all_attributions):\n","\n","  dataframe = []\n","\n","  for i in tqdm(range(len(cogs402_ds))):\n","    exam_attrib = all_attributions[str(i)]\n","    attention_final_layer = all_attentions[str(i)]\n","    \n","    exam_attrib = exam_attrib[:len(all_attentions[str(i)])]\n","\n","    #input_ids\n","    text = cogs402_ds['text'][i]\n","    input_ids, _, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n","    indices = input_ids[0].detach().tolist()\n","    all_tokens_curr = tokenizer.convert_ids_to_tokens(indices)\n","\n","    exam_tokens = all_tokens_curr\n","    alpha_numeric_nums = [idx for idx, element in enumerate(exam_tokens) if element.isalpha() if element not in stopwords]\n","    mask = np.ones(attention_final_layer.shape,dtype=bool) \n","    mask[alpha_numeric_nums] = False\n","\n","    #raw attributions\n","    cosine_raw = np.dot(exam_attrib, attention_final_layer) / (norm(exam_attrib)*norm(attention_final_layer))\n","\n","    #normalized attributions\n","    attention_final_layer2 = normalize(attention_final_layer)\n","\n","    exam_attrib2 = np.abs(exam_attrib)\n","    exam_attrib2 = normalize(exam_attrib2)\n","    \n","    attention_final_layer2[mask] = 0\n","    exam_attrib2[mask] = 0\n","\n","    cosine = np.dot(exam_attrib2, attention_final_layer2) / (norm(exam_attrib2)*norm(attention_final_layer2))\n","\n","    #Jaccard Sim for 95th percentile\n","    exam_attrib3 = np.copy(exam_attrib2)\n","    median_exam = np.percentile(exam_attrib3, 95)\n","    exam_attrib3[exam_attrib3 < median_exam] = 0\n","\n","    exam_attrib_top = np.flatnonzero(exam_attrib3)\n","    exam_attrib_top = set(exam_attrib_top)\n","\n","    attention_final_layer3 = np.copy(attention_final_layer2)\n","    median_12 = np.percentile(attention_final_layer3, 95)\n","    attention_final_layer3[attention_final_layer3 < median_12] = 0\n","\n","    attention_final_layer_top = np.flatnonzero(attention_final_layer3)\n","    attention_final_layer_top = set(attention_final_layer_top)\n","    \n","    jaccard_sim = jaccard_similarity(exam_attrib_top, attention_final_layer_top)\n","\n","    #sim using the ranks of the tokens\n","    exam_attrib_rank = np.copy(exam_attrib2)\n","    order_attrib = exam_attrib_rank.argsort()[::-1]\n","    ranks_attrib = order_attrib.argsort()\n","\n","    attention_final_layer_rank = np.copy(attention_final_layer2)\n","    order = attention_final_layer_rank.argsort()[::-1]\n","    ranks = order.argsort()\n","    \n","    cosine_rank = np.dot(ranks_attrib, ranks) / (norm(ranks_attrib)*norm(ranks))\n","\n","    tau, p_value = stats.kendalltau(ranks_attrib, ranks)\n","\n","    rbo_sim = rbo.RankingSimilarity(order_attrib, order).rbo()\n","\n","    d = {'example': i, 'similarity normalized': cosine, 'similarity raw': cosine_raw, \"sim w/ ranks\":cosine_rank,\n","        \"kendalltau\": tau, \"rbo\":rbo_sim, 'Jaccard_sim_95th':jaccard_sim}\n","    dataframe.append(d)\n","\n","  return pd.DataFrame(dataframe)"],"metadata":{"id":"sXoj61dc6sQ0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Apply the above function for our two cases, upper layers, and lower layers."],"metadata":{"id":"52VIfl7DJuMi"}},{"cell_type":"code","source":["df_alpha = get_sim_dataframe_alpha(cogs402_ds, all_attentions_two, all_attributions)"],"metadata":{"id":"AC49DcgfH-6L","executionInfo":{"status":"ok","timestamp":1659513683902,"user_tz":420,"elapsed":407,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"a6d2888f-7bb5-493a-f955-b093af8e8b50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 12/12 [00:00<00:00, 51.22it/s]\n"]}]},{"cell_type":"code","source":["df_alpha2 = get_sim_dataframe_alpha(cogs402_ds, all_attentions_one, all_attributions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q0WW-X0dofyb","executionInfo":{"status":"ok","timestamp":1659513688123,"user_tz":420,"elapsed":438,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"56f4d85f-7f7f-4778-822f-a7b5d065a4c6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 12/12 [00:00<00:00, 51.16it/s]\n"]}]},{"cell_type":"code","source":["df_alpha.mean()"],"metadata":{"id":"2lNm1B5FJIbn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659513694408,"user_tz":420,"elapsed":237,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"5e934303-231a-48be-c5b2-775f5bc0b983"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["example                  5.500000\n","similarity normalized    0.775128\n","similarity raw          -0.219814\n","sim w/ ranks             0.974078\n","kendalltau               0.762864\n","rbo                      0.761922\n","Jaccard_sim_95th         0.060641\n","dtype: float64"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["Similar to the previous time we saved our results to a dictionary, but this time we change alnum_only to True because df_alpha masks all the non-alphanumeric tokens."],"metadata":{"id":"QG_6e4jUaZqI"}},{"cell_type":"code","source":["d_alpha = {'example': \"attrib_vs_layer12_attn\", 'mean_cosine_sim': df_alpha.mean()[\"similarity normalized\"].round(6), \"mean_cosine_sim_ranks\":df_alpha.mean()[\"sim w/ ranks\"].round(6),\n","        \"mean_kendall_tau\": df_alpha.mean()[\"kendalltau\"].round(6), \"mean_RBO\":df_alpha.mean()[\"rbo\"].round(6), 'mean_Jaccard_95th':df_alpha.mean()['Jaccard_sim_95th'].round(6), \"scaled\":True, \"alpha_only\":True}\n","result_dataframe.append(d_alpha)"],"metadata":{"id":"DYVy_3HlPTek"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_alpha2.mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u4akOqvPZdoa","executionInfo":{"status":"ok","timestamp":1659513728785,"user_tz":420,"elapsed":3,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"fde8468c-956c-4d88-e112-44e060a71601"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["example                  5.500000\n","similarity normalized    0.777440\n","similarity raw          -0.220619\n","sim w/ ranks             0.974820\n","kendalltau               0.768707\n","rbo                      0.767221\n","Jaccard_sim_95th         0.072178\n","dtype: float64"]},"metadata":{},"execution_count":53}]},{"cell_type":"markdown","source":["The same principle for d_alpha is applied here for the upper layers."],"metadata":{"id":"RjUz53bqajFb"}},{"cell_type":"code","source":["d_alpha2 = {'example': \"attrib_vs_all_layer_attn\", 'mean_cosine_sim': df_alpha2.mean()[\"similarity normalized\"].round(6), \"mean_cosine_sim_ranks\":df_alpha2.mean()[\"sim w/ ranks\"].round(6),\n","        \"mean_kendall_tau\": df_alpha2.mean()[\"kendalltau\"].round(6), \"mean_RBO\":df_alpha2.mean()[\"rbo\"].round(6), 'mean_Jaccard_95th':df_alpha2.mean()['Jaccard_sim_95th'].round(6), \"scaled\":True, \"alpha_only\":True}\n","result_dataframe.append(d_alpha2)"],"metadata":{"id":"XBikudz4Pd3o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Unscaled Versions"],"metadata":{"id":"in_N4CznKzZF"}},{"cell_type":"markdown","source":["These two dictionaries store the unscaled summed attentions for layer 12 and all layers respectively. You can change the name and the path to suit your project."],"metadata":{"id":"fMrIo9ophBUt"}},{"cell_type":"code","source":["# all_attentions_unscale_12 = torch.load('/content/drive/MyDrive/cogs402longformer/results/papers/full_attention_matrices/example_atten_dict_12_unscale.pt')\n","# all_attentions_unscale_all = torch.load('/content/drive/MyDrive/cogs402longformer/results/papers/full_attention_matrices/example_atten_dict_all_unscale.pt')"],"metadata":{"id":"uruAVE0XhNOk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["These two dictionaries store the unscaled summed attentions for the layers 1-6 and 7-12 respectively. You can change the name and the path to suit your project"],"metadata":{"id":"Wrri3dZoajIh"}},{"cell_type":"code","source":["# all_attentions_unscale_lower = torch.load('/content/drive/MyDrive/cogs402longformer/results/papers/full_attention_matrices/example_atten_dict_lower_unscale.pt')\n","# all_attentions_unscale_upper = torch.load('/content/drive/MyDrive/cogs402longformer/results/papers/full_attention_matrices/example_atten_dict_upper_unscale.pt')"],"metadata":{"id":"4lAQkA7wk27O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The following block of code sums the attention in the same manner as the above [version](https://colab.research.google.com/drive/1-FZYN7yBm2jVsZds8Q4lydasdxJRe9zk#scrollTo=ZWeFQNW8MmJL&line=1&uniqifier=1), producing arrays of attention with shape (seq_len) and storing it in a dictionary.\n","\n","Feel free to skip this block of code if you have already imported your attentions."],"metadata":{"id":"HO3nWkS1u71q"}},{"cell_type":"code","source":["all_attentions_unscale_one = {}\n","all_attentions_unscale_two = {}\n","for i in tqdm(range(len(cogs402_ds))):\n","  if str(i) not in all_attentions_unscale_one and str(i) not in all_attentions_unscale_two:\n","\n","    att_mat = all_attentions[str(i)]\n","\n","    # Sum the attentions for the last layer and over all layers\n","    att_mat_one = att_mat.sum(axis=1)\n","    att_mat_one = att_mat_one.sum(axis=0)\n","    all_attentions_unscale_one[str(i)] = att_mat_one\n","\n","    #template for single layer\n","    attention_single_layer = att_mat[11].sum(axis=0)\n","    all_attentions_unscale_two[str(i)] = attention_single_layer\n","    \n","#     torch.save(all_attentions_unscale_lower, '/content/drive/MyDrive/cogs402longformer/results/papers/papers_attributions/full_attention_matrices/example_atten_dict_lower_unscale.pt')\n","#     torch.save(all_attentions_unscale_upper, '/content/drive/MyDrive/cogs402longformer/results/papers/papers_attributions/full_attention_matrices/example_atten_dict_upper_unscale.pt')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nMpNSHvMZpy0","executionInfo":{"status":"ok","timestamp":1659514229424,"user_tz":420,"elapsed":162,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"b962f559-2a7c-489f-b39a-86335a39b1b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 12/12 [00:00<00:00, 3282.57it/s]\n"]}]},{"cell_type":"markdown","source":["We use the same functions we create previously [above](https://colab.research.google.com/drive/1-FZYN7yBm2jVsZds8Q4lydasdxJRe9zk#scrollTo=dmKF8EBP2Nqz&line=11&uniqifier=1) to get dataframes which we can use to get the mean or the max."],"metadata":{"id":"Ntic7Vv4MTHY"}},{"cell_type":"code","source":["df_unscale = get_sim_dataframe(cogs402_ds, all_attentions_unscale_two, all_attributions)\n","df_unscale2 = get_sim_dataframe(cogs402_ds, all_attentions_unscale_one, all_attributions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Zzxb8qDXkt-S","executionInfo":{"status":"ok","timestamp":1659514232479,"user_tz":420,"elapsed":390,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"1551b39a-8af7-48c0-bd87-4ff36a50b731"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 12/12 [00:00<00:00, 167.83it/s]\n","100%|██████████| 12/12 [00:00<00:00, 182.74it/s]\n"]}]},{"cell_type":"code","source":["df_unscale.mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5R6CNQz5npSQ","executionInfo":{"status":"ok","timestamp":1659514232479,"user_tz":420,"elapsed":6,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"2e4761df-d606-4b03-a63f-28841e025684"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["example                  5.500000\n","similarity normalized    0.773959\n","similarity raw          -0.220792\n","sim w/ ranks             0.760612\n","Jaccard_sim              0.059665\n","kendalltau               0.028889\n","rbo                      0.509123\n","jaccard sim 95th         0.036035\n","dtype: float64"]},"metadata":{},"execution_count":58}]},{"cell_type":"markdown","source":["We set the value of scaled to False and alnum to False given that we did not scale the attentions when getting the similarities in df_unscale nor did we mask the non-alphanumeric tokens."],"metadata":{"id":"64cIeyEzapTs"}},{"cell_type":"code","source":["d_unscale = {'example': \"attrib_vs_layer12_attn\", 'mean_cosine_sim': df_unscale.mean()[\"similarity normalized\"].round(6), \"mean_cosine_sim_ranks\":df_unscale.mean()[\"sim w/ ranks\"].round(6),\n","        \"mean_kendall_tau\": df_unscale.mean()[\"kendalltau\"].round(6), \"mean_RBO\":df_unscale.mean()[\"rbo\"].round(6), 'mean_Jaccard_95th':df_unscale.mean()['jaccard sim 95th'].round(6), \"scaled\":False, \"alpha_only\":False}\n","result_dataframe.append(d_unscale)"],"metadata":{"id":"SeiwUePGPjnL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The same principle for d_unscale is applied here, but for the other example (in our case the upper layers)."],"metadata":{"id":"KM9CvxinbLXU"}},{"cell_type":"code","source":["df_unscale2.mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yJKEhyEonuNI","executionInfo":{"status":"ok","timestamp":1659514235849,"user_tz":420,"elapsed":2,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"735b57af-ecd3-4434-ab1f-815d29e9f725"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["example                  5.500000\n","similarity normalized    0.769475\n","similarity raw          -0.220300\n","sim w/ ranks             0.762385\n","Jaccard_sim              0.053278\n","kendalltau               0.033868\n","rbo                      0.516537\n","jaccard sim 95th         0.033999\n","dtype: float64"]},"metadata":{},"execution_count":60}]},{"cell_type":"code","source":["d_unscale2 = {'example': \"attrib_vs_all_layer_attn\", 'mean_cosine_sim': df_unscale2.mean()[\"similarity normalized\"].round(6), \"mean_cosine_sim_ranks\":df_unscale2.mean()[\"sim w/ ranks\"].round(6),\n","        \"mean_kendall_tau\": df_unscale2.mean()[\"kendalltau\"].round(6), \"mean_RBO\":df_unscale2.mean()[\"rbo\"].round(6), 'mean_Jaccard_95th':df_unscale2.mean()['jaccard sim 95th'].round(6), \"scaled\":False, \"alpha_only\":False}\n","result_dataframe.append(d_unscale2)"],"metadata":{"id":"GeHomiB0PrFd"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Alphanumeric Tokens only"],"metadata":{"id":"o67UKUOvMrxs"}},{"cell_type":"markdown","source":["We use the function we created earlier [above] in order to get the dataframe with our similarities using alphanumeric inputs only."],"metadata":{"id":"zL8GKBCEMxU9"}},{"cell_type":"code","source":["df_unscale_alpha = get_sim_dataframe_alpha(cogs402_ds, all_attentions_unscale_two, all_attributions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9kTlwzd1sve4","executionInfo":{"status":"ok","timestamp":1659514252190,"user_tz":420,"elapsed":376,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"2d619ca7-0f05-4f17-ea5d-e9e2a91a063d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 12/12 [00:00<00:00, 46.41it/s]\n"]}]},{"cell_type":"code","source":["df_unscale_alpha2 = get_sim_dataframe_alpha(cogs402_ds, all_attentions_unscale_one, all_attributions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"VCqPMGqgs4Cg","executionInfo":{"status":"ok","timestamp":1659514253375,"user_tz":420,"elapsed":446,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"7ee89b2f-64d7-4e10-8db7-523c0e01a577"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 12/12 [00:00<00:00, 44.72it/s]\n"]}]},{"cell_type":"code","source":["df_unscale_alpha.mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aAXTz9nkt0xu","executionInfo":{"status":"ok","timestamp":1659514254197,"user_tz":420,"elapsed":4,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"e9cfba81-a55d-405b-ca1e-bf2f98991203"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["example                  5.500000\n","similarity normalized    0.774267\n","similarity raw          -0.220792\n","sim w/ ranks             0.973745\n","kendalltau               0.760341\n","rbo                      0.760962\n","Jaccard_sim_95th         0.064841\n","dtype: float64"]},"metadata":{},"execution_count":65}]},{"cell_type":"markdown","source":["Here we are both leaving the attentions unscaled and masking all the non_alphanumeric tokens, so we appropriately set scaled to False and \"alnum_only\" to True."],"metadata":{"id":"2ge0lExJbQag"}},{"cell_type":"code","source":["d_unscale_alpha = {'example': \"attrib_vs_layer12_attn\", 'mean_cosine_sim': df_unscale_alpha.mean()[\"similarity normalized\"].round(6), \"mean_cosine_sim_ranks\":df_unscale_alpha.mean()[\"sim w/ ranks\"].round(6),\n","        \"mean_kendall_tau\": df_unscale_alpha.mean()[\"kendalltau\"].round(6), \"mean_RBO\":df_unscale_alpha.mean()[\"rbo\"].round(6), 'mean_Jaccard_95th':df_unscale_alpha.mean()['Jaccard_sim_95th'].round(6), \"scaled\":False, \"alpha_only\":True}\n","result_dataframe.append(d_unscale_alpha)"],"metadata":{"id":"rJwffxvuQKtc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The same principle for d_unscale is applied here, but for the other example (in our case the upper layers)."],"metadata":{"id":"dhTLMBuNM-uz"}},{"cell_type":"code","source":["df_unscale_alpha2.mean()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KYP_JvItt3eL","executionInfo":{"status":"ok","timestamp":1659514258030,"user_tz":420,"elapsed":194,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"2a4855f7-08f1-46c6-d3c3-5544d59c3ddb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["example                  5.500000\n","similarity normalized    0.776947\n","similarity raw          -0.220300\n","sim w/ ranks             0.974991\n","kendalltau               0.770844\n","rbo                      0.768044\n","Jaccard_sim_95th         0.072057\n","dtype: float64"]},"metadata":{},"execution_count":67}]},{"cell_type":"code","source":["d_unscale_alpha2 = {'example': \"attrib_vs_all_layer_attn\", 'mean_cosine_sim': df_unscale_alpha2.mean()[\"similarity normalized\"].round(6), \n","                    \"mean_cosine_sim_ranks\":df_unscale_alpha2.mean()[\"sim w/ ranks\"].round(6),\n","                    \"mean_kendall_tau\": df_unscale_alpha2.mean()[\"kendalltau\"].round(6), \n","                    \"mean_RBO\":df_unscale_alpha2.mean()[\"rbo\"].round(6), \n","                    'mean_Jaccard_95th':df_unscale_alpha2.mean()['Jaccard_sim_95th'].round(6),\n","                    \"scaled\":False, \"alpha_only\":True}\n","result_dataframe.append(d_unscale_alpha2)"],"metadata":{"id":"2hN3281WPyO8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Saving the End Results"],"metadata":{"id":"SMvfB3aRZtl_"}},{"cell_type":"markdown","source":["Finally, we can combine our results into a single dataframe to store our data and be able to easily access our results. We sort our examples by name and whether or not they are scaled to keep it as organized as possible."],"metadata":{"id":"QPQE-Sf4Qu2C"}},{"cell_type":"code","source":["df_new = pd.DataFrame(result_dataframe)\n","df_new = df_new.sort_values(['example', 'scaled'],\n","              ascending = [True, True])\n","df_new"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":300},"id":"1WCxYo_uJ94H","executionInfo":{"status":"ok","timestamp":1659514260906,"user_tz":420,"elapsed":182,"user":{"displayName":"daniel hou","userId":"13623878136116974888"}},"outputId":"1597acdb-b241-41bd-b1e6-ad4f5813a1e8"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["                    example  mean_cosine_sim  mean_cosine_sim_ranks  \\\n","5  attrib_vs_all_layer_attn         0.769475               0.762385   \n","7  attrib_vs_all_layer_attn         0.776947               0.974991   \n","1  attrib_vs_all_layer_attn         0.770697               0.763060   \n","3  attrib_vs_all_layer_attn         0.777440               0.974820   \n","4    attrib_vs_layer12_attn         0.773959               0.760612   \n","6    attrib_vs_layer12_attn         0.774267               0.973745   \n","0    attrib_vs_layer12_attn         0.775263               0.759044   \n","2    attrib_vs_layer12_attn         0.775128               0.974078   \n","\n","   mean_kendall_tau  mean_RBO  mean_Jaccard_95th  scaled  alpha_only  \n","5          0.033868  0.516537           0.033999   False       False  \n","7          0.770844  0.768044           0.072057   False        True  \n","1          0.035511  0.516411           0.034858    True       False  \n","3          0.768707  0.767221           0.072178    True        True  \n","4          0.028889  0.509123           0.036035   False       False  \n","6          0.760341  0.760962           0.064841   False        True  \n","0          0.024602  0.505569           0.033131    True       False  \n","2          0.762864  0.761922           0.060641    True        True  "],"text/html":["\n","  <div id=\"df-daf3f681-3974-46fe-b458-73f6be2b4b6b\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>example</th>\n","      <th>mean_cosine_sim</th>\n","      <th>mean_cosine_sim_ranks</th>\n","      <th>mean_kendall_tau</th>\n","      <th>mean_RBO</th>\n","      <th>mean_Jaccard_95th</th>\n","      <th>scaled</th>\n","      <th>alpha_only</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>5</th>\n","      <td>attrib_vs_all_layer_attn</td>\n","      <td>0.769475</td>\n","      <td>0.762385</td>\n","      <td>0.033868</td>\n","      <td>0.516537</td>\n","      <td>0.033999</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>attrib_vs_all_layer_attn</td>\n","      <td>0.776947</td>\n","      <td>0.974991</td>\n","      <td>0.770844</td>\n","      <td>0.768044</td>\n","      <td>0.072057</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>attrib_vs_all_layer_attn</td>\n","      <td>0.770697</td>\n","      <td>0.763060</td>\n","      <td>0.035511</td>\n","      <td>0.516411</td>\n","      <td>0.034858</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>attrib_vs_all_layer_attn</td>\n","      <td>0.777440</td>\n","      <td>0.974820</td>\n","      <td>0.768707</td>\n","      <td>0.767221</td>\n","      <td>0.072178</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>attrib_vs_layer12_attn</td>\n","      <td>0.773959</td>\n","      <td>0.760612</td>\n","      <td>0.028889</td>\n","      <td>0.509123</td>\n","      <td>0.036035</td>\n","      <td>False</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>attrib_vs_layer12_attn</td>\n","      <td>0.774267</td>\n","      <td>0.973745</td>\n","      <td>0.760341</td>\n","      <td>0.760962</td>\n","      <td>0.064841</td>\n","      <td>False</td>\n","      <td>True</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>attrib_vs_layer12_attn</td>\n","      <td>0.775263</td>\n","      <td>0.759044</td>\n","      <td>0.024602</td>\n","      <td>0.505569</td>\n","      <td>0.033131</td>\n","      <td>True</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>attrib_vs_layer12_attn</td>\n","      <td>0.775128</td>\n","      <td>0.974078</td>\n","      <td>0.762864</td>\n","      <td>0.761922</td>\n","      <td>0.060641</td>\n","      <td>True</td>\n","      <td>True</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-daf3f681-3974-46fe-b458-73f6be2b4b6b')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-daf3f681-3974-46fe-b458-73f6be2b4b6b button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-daf3f681-3974-46fe-b458-73f6be2b4b6b');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":69}]},{"cell_type":"markdown","source":["If you have a previous version of this dataframe, we can load the csv using pandas and add our new results onto the dataframe by using panda's concatenate function."],"metadata":{"id":"-EU7Lo80SCNh"}},{"cell_type":"code","source":["# df_previous = pd.read_csv(\"/content/drive/MyDrive/cogs402longformer/results/papers/attrib_attn_sim/attrib_attn_sim_means.csv\")"],"metadata":{"id":"DbVFk3Z4Q41Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df_previous"],"metadata":{"id":"z9Tpp6Evcdwy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df_previous.loc[(df_previous[\"alnum_only\"] == True)]"],"metadata":{"id":"-eVM1aZoFLYI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df_previous.loc[(df_previous[\"alnum_only\"] == False)]"],"metadata":{"id":"zI1A0DQLH7e1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["When we concatenate our two dataframes, we want to make sure we don't have any duplicate rows. We consider it a duplicate if two rows have the same example, have the same value for scaled, and the same value for alnum_only. If we find a do find duplicate rows based on the above conditions, the last occuring instance of the row, which is the instance that was obtained earlier in the notebook (and not the instance that was read from file), is kept in the dataframe."],"metadata":{"id":"vvy8NkJYVeta"}},{"cell_type":"code","source":["# df_final = pd.concat([df_previous, df_new], ignore_index=True)"],"metadata":{"id":"tFTC8GOsSdOj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df_final = df_final.drop_duplicates(['example', 'scaled', 'alnum_only'], keep='last').sort_values(['example', 'scaled'],\n","#               ascending = [True, True])"],"metadata":{"id":"GXKKUdO3WID4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df_final"],"metadata":{"id":"LKt9BbUsUD-g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finally, we save this dataframe we made so we can either add on to this dataframe in future explorations, or take these results for other uses."],"metadata":{"id":"QVtUSf1QSMwh"}},{"cell_type":"code","source":["# df_final.to_csv(\"/content/drive/MyDrive/cogs402longformer/results/papers/attrib_attn_sim/attrib_attn_sim_means.csv\", index=False)"],"metadata":{"id":"AeDbwwDMRAnS"},"execution_count":null,"outputs":[]}],"metadata":{"file_extension":".py","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"mimetype":"text/x-python","name":"python","npconvert_exporter":"python","pygments_lexer":"ipython3","version":3,"colab":{"name":"Attn_attr_similarity_means_notes.ipynb","provenance":[{"file_id":"1-FZYN7yBm2jVsZds8Q4lydasdxJRe9zk","timestamp":1658897102714},{"file_id":"14a1tOimrRbLlXd0rbtY-UTcizR5GDd4F","timestamp":1657577217207},{"file_id":"15Zquqi72N2NNusEUXRN53bCKE7qj8KAh","timestamp":1657065530148},{"file_id":"1-zUACrWJtGVU71pkZ-kswoaOj6bn93RG","timestamp":1654732006544}],"collapsed_sections":[],"machine_shape":"hm"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"f81393c0b5f548658cbc98c7376df1f9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8fd1f56d02b54c279e86a26d04df4822","IPY_MODEL_1ab3064b12084945a62ed2de52603d2c","IPY_MODEL_d7caccd3386647dfa9cda51e56584b4a"],"layout":"IPY_MODEL_5727b24cc76841b0a028a80a181b2dbc"}},"8fd1f56d02b54c279e86a26d04df4822":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_26fc9791f8974270b863e1a53085c7de","placeholder":"​","style":"IPY_MODEL_76ec00781ffc443fb49d366e63819bd5","value":"Downloading config.json: 100%"}},"1ab3064b12084945a62ed2de52603d2c":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd360623b92f4f6d86595793e09f8391","max":694,"min":0,"orientation":"horizontal","style":"IPY_MODEL_144692894fa54c67af9847fb84a72e14","value":694}},"d7caccd3386647dfa9cda51e56584b4a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cb468d72122e4f37b371b9cae878fa01","placeholder":"​","style":"IPY_MODEL_20ff3d450578481fb9eb83f0a1d48eb6","value":" 694/694 [00:00&lt;00:00, 26.6kB/s]"}},"5727b24cc76841b0a028a80a181b2dbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"26fc9791f8974270b863e1a53085c7de":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"76ec00781ffc443fb49d366e63819bd5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd360623b92f4f6d86595793e09f8391":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"144692894fa54c67af9847fb84a72e14":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cb468d72122e4f37b371b9cae878fa01":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"20ff3d450578481fb9eb83f0a1d48eb6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d751757f96184a86972067e7c9ea7804":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b5753f8c6f2b4cbaaf0b03ba193609d7","IPY_MODEL_9afd502da29a4bd5a4cfc4af851eacc7","IPY_MODEL_178b39c93bbd48f89a14684318ce7910"],"layout":"IPY_MODEL_602e4579b8c14b949dadd8ca93b30540"}},"b5753f8c6f2b4cbaaf0b03ba193609d7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d66481b8b3884d148b73cfbff0e22bcd","placeholder":"​","style":"IPY_MODEL_ecfd0de10182442db0c1d0e7510247b3","value":"Downloading pytorch_model.bin: 100%"}},"9afd502da29a4bd5a4cfc4af851eacc7":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f57a9e446fc4e3da09d766c8090a2a3","max":597257159,"min":0,"orientation":"horizontal","style":"IPY_MODEL_131950c0fbe64eaa871b961f4e3cd70a","value":597257159}},"178b39c93bbd48f89a14684318ce7910":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_085155aac4fd4b11a61538a5648e18f9","placeholder":"​","style":"IPY_MODEL_447ad5eedaf1417982002270ee5d9630","value":" 570M/570M [00:09&lt;00:00, 60.9MB/s]"}},"602e4579b8c14b949dadd8ca93b30540":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d66481b8b3884d148b73cfbff0e22bcd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ecfd0de10182442db0c1d0e7510247b3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3f57a9e446fc4e3da09d766c8090a2a3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"131950c0fbe64eaa871b961f4e3cd70a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"085155aac4fd4b11a61538a5648e18f9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"447ad5eedaf1417982002270ee5d9630":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"91215adec2ca4693b5e9eddd27de534c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8aa82c7580dc4cfe906f1599706a8828","IPY_MODEL_3ae768799d644c368b7b413557daae28","IPY_MODEL_d53bb5f1945f4648876b2c77531c678c"],"layout":"IPY_MODEL_f85445971ba14e8187bed25bad6addde"}},"8aa82c7580dc4cfe906f1599706a8828":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_05e757971af440ad99526d66396b09bc","placeholder":"​","style":"IPY_MODEL_4616231b58a04cd2a163ad3b0daa8caa","value":"Downloading vocab.json: 100%"}},"3ae768799d644c368b7b413557daae28":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1136f129a484cd687f5d922dca1c4d4","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_72b10f1f9f8540d69244edbb4458bd9f","value":898823}},"d53bb5f1945f4648876b2c77531c678c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef5251e54de6405da874f39d52f55138","placeholder":"​","style":"IPY_MODEL_95effa98de274be698732ab4f65d3b24","value":" 878k/878k [00:00&lt;00:00, 3.68MB/s]"}},"f85445971ba14e8187bed25bad6addde":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"05e757971af440ad99526d66396b09bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4616231b58a04cd2a163ad3b0daa8caa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1136f129a484cd687f5d922dca1c4d4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"72b10f1f9f8540d69244edbb4458bd9f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ef5251e54de6405da874f39d52f55138":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95effa98de274be698732ab4f65d3b24":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cfd04175e20049c1b24b98f5ae230be6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ff5ce5c1b9d14720881c688296ada688","IPY_MODEL_d978db119a2c448c953d2c979b52152b","IPY_MODEL_1ece88ee359442c18aafe5cfb585fdec"],"layout":"IPY_MODEL_97824c5ca330429aace237f626492537"}},"ff5ce5c1b9d14720881c688296ada688":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1717fde6ea1140339abcf1f75a4ac65d","placeholder":"​","style":"IPY_MODEL_22f58760aa5046dd8d0eeefc0ead5bff","value":"Downloading merges.txt: 100%"}},"d978db119a2c448c953d2c979b52152b":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_74354be1fe8e4cb69f63fe52098cb4ba","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_54d4abfb8c494ab1a506eb488a9c8064","value":456318}},"1ece88ee359442c18aafe5cfb585fdec":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7d2329db8c1b4585a59512ee00a5a7e8","placeholder":"​","style":"IPY_MODEL_6c2d159dbed44797b133e45bbf97d2e2","value":" 446k/446k [00:00&lt;00:00, 3.34MB/s]"}},"97824c5ca330429aace237f626492537":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1717fde6ea1140339abcf1f75a4ac65d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22f58760aa5046dd8d0eeefc0ead5bff":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"74354be1fe8e4cb69f63fe52098cb4ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"54d4abfb8c494ab1a506eb488a9c8064":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"7d2329db8c1b4585a59512ee00a5a7e8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6c2d159dbed44797b133e45bbf97d2e2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f15c3e7cfbbb4e88bcde85271986d1fc":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_86fc06064ba74b59930f798334830f98","IPY_MODEL_8eceb2f11f36496dba471c1312fb8a38","IPY_MODEL_8bf8d14e4cf74955a54fe413450ce9bd"],"layout":"IPY_MODEL_91201ff82b7648eea59b6453ae14dbac"}},"86fc06064ba74b59930f798334830f98":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8413354afbd24d218b932aa153d4de3a","placeholder":"​","style":"IPY_MODEL_4e3be7912a384fc3a4a380de8df83114","value":"Downloading: 100%"}},"8eceb2f11f36496dba471c1312fb8a38":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4ad309a5c75f497490639d5dbb4460dc","max":613,"min":0,"orientation":"horizontal","style":"IPY_MODEL_3ec7106896fe4ccaa077c7017205eace","value":613}},"8bf8d14e4cf74955a54fe413450ce9bd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e2c739093a97434eba565cebb2df3c55","placeholder":"​","style":"IPY_MODEL_916f4ba96c344a82a038c0a2c6debeb6","value":" 613/613 [00:00&lt;00:00, 23.2kB/s]"}},"91201ff82b7648eea59b6453ae14dbac":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8413354afbd24d218b932aa153d4de3a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4e3be7912a384fc3a4a380de8df83114":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4ad309a5c75f497490639d5dbb4460dc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3ec7106896fe4ccaa077c7017205eace":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e2c739093a97434eba565cebb2df3c55":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"916f4ba96c344a82a038c0a2c6debeb6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e8c12bf138d447c2a0b2060be17560d7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c69a55d4dbb74a599ea34917f294135d","IPY_MODEL_813d23b62d9f474493cc534141730ee1","IPY_MODEL_6dfca0d9543740ef91ef412902d20a5d"],"layout":"IPY_MODEL_695aece1a68947a7a51d0d82a6fdf0a7"}},"c69a55d4dbb74a599ea34917f294135d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_507c62435bd140588368995d853ae861","placeholder":"​","style":"IPY_MODEL_08aac8d4ecf645bf9a8ae59d3dd0980a","value":"Downloading data files: 100%"}},"813d23b62d9f474493cc534141730ee1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5e3482fcee18454182c49f4bfc7f293e","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ded0eb5deb7b4807b3aedc87e4f65465","value":1}},"6dfca0d9543740ef91ef412902d20a5d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e80ef233f14f40fb9d47381194bbffa0","placeholder":"​","style":"IPY_MODEL_b1944977de324ae59073e8bde89d95be","value":" 1/1 [00:00&lt;00:00,  1.47it/s]"}},"695aece1a68947a7a51d0d82a6fdf0a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"507c62435bd140588368995d853ae861":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08aac8d4ecf645bf9a8ae59d3dd0980a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5e3482fcee18454182c49f4bfc7f293e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ded0eb5deb7b4807b3aedc87e4f65465":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e80ef233f14f40fb9d47381194bbffa0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b1944977de324ae59073e8bde89d95be":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a080e52e02f448e7b1a79cacd3de4212":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_ec91e884a5d240179736fc569bd7ca37","IPY_MODEL_3f45cd198ec8486d9911d0407a3eda31","IPY_MODEL_6a9a232be3664866aad321d2d9582408"],"layout":"IPY_MODEL_67cfa6834bf34413b0fd03903fbc9751"}},"ec91e884a5d240179736fc569bd7ca37":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ba0f9fd74b54f6ea3439b0feab5e56d","placeholder":"​","style":"IPY_MODEL_0c2efdd33d8f438997349f3d1181d0ca","value":"Downloading data: 100%"}},"3f45cd198ec8486d9911d0407a3eda31":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_31d6461ada8c4802adc32b740dc7f053","max":61212,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6fe2f07343544061b2f4d3ecaf8f0b83","value":61212}},"6a9a232be3664866aad321d2d9582408":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_40cbd50f9442444184aaa90e1463cd46","placeholder":"​","style":"IPY_MODEL_8452d41757dc444daf63f3e3b35200e7","value":" 61.2k/61.2k [00:00&lt;00:00, 1.84MB/s]"}},"67cfa6834bf34413b0fd03903fbc9751":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ba0f9fd74b54f6ea3439b0feab5e56d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c2efdd33d8f438997349f3d1181d0ca":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"31d6461ada8c4802adc32b740dc7f053":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6fe2f07343544061b2f4d3ecaf8f0b83":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"40cbd50f9442444184aaa90e1463cd46":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8452d41757dc444daf63f3e3b35200e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0469deaf91044c13b230c5de4381f680":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_aeea89a950504058afb73c8422e91f28","IPY_MODEL_ec2d7d104b9248f080a660e601fc0d35","IPY_MODEL_cb92133c6337469cb26cd4ee3bb291cf"],"layout":"IPY_MODEL_61449827a7294377a979f46f84508447"}},"aeea89a950504058afb73c8422e91f28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd08290fa5834a31890607dfde98322f","placeholder":"​","style":"IPY_MODEL_91253fbdba9843b5aa938eb0fce96282","value":"Extracting data files: 100%"}},"ec2d7d104b9248f080a660e601fc0d35":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_2d782def8a224deb92d321cb61e23a4b","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_39861043e76d420ebd035bb550cef1e0","value":1}},"cb92133c6337469cb26cd4ee3bb291cf":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb36464f6ca7410482ebe4955e7406a1","placeholder":"​","style":"IPY_MODEL_2d645fa0475a479382baa38ca02153c3","value":" 1/1 [00:00&lt;00:00, 26.15it/s]"}},"61449827a7294377a979f46f84508447":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd08290fa5834a31890607dfde98322f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"91253fbdba9843b5aa938eb0fce96282":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"2d782def8a224deb92d321cb61e23a4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39861043e76d420ebd035bb550cef1e0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"eb36464f6ca7410482ebe4955e7406a1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2d645fa0475a479382baa38ca02153c3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e2c845a98994b7cbc835b993c6bfff7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1d9a63c62bf2451db05f43a106593cad","IPY_MODEL_071cb0593ebc492c8c03ecc1257ec207","IPY_MODEL_d2423f00be6049d8871ed3879c99df61"],"layout":"IPY_MODEL_a5127cd43edb4c37b6f8b651723336ba"}},"1d9a63c62bf2451db05f43a106593cad":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f1bb553c3a7f4ac3bdc72310d1df29ce","placeholder":"​","style":"IPY_MODEL_4112ffb75125421c9f3325a3d1d3faef","value":""}},"071cb0593ebc492c8c03ecc1257ec207":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"info","description":"","description_tooltip":null,"layout":"IPY_MODEL_7f64b9eaf1f9431a89ec96f95dd3a13f","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_07c8b12e05e34035b4732bcde64ddad4","value":1}},"d2423f00be6049d8871ed3879c99df61":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cedd0fb40f674e73b4b62dde1a829ece","placeholder":"​","style":"IPY_MODEL_56b5756c48bb483b8c854211a7fdcff2","value":" 0/? [00:00&lt;?, ? tables/s]"}},"a5127cd43edb4c37b6f8b651723336ba":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1bb553c3a7f4ac3bdc72310d1df29ce":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4112ffb75125421c9f3325a3d1d3faef":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7f64b9eaf1f9431a89ec96f95dd3a13f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":"20px"}},"07c8b12e05e34035b4732bcde64ddad4":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cedd0fb40f674e73b4b62dde1a829ece":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"56b5756c48bb483b8c854211a7fdcff2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ac50bb965247424c87f4d3d74e0ecb7f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c7c3f79c46634d77bbacb9f27bbe4660","IPY_MODEL_a208e8d8b9944955a3d0eee6e8890fb5","IPY_MODEL_901a6540128d4204b098bde09843b822"],"layout":"IPY_MODEL_6858411de99340eabdddf3e6709b4e4b"}},"c7c3f79c46634d77bbacb9f27bbe4660":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e53116a2750340969b19215a02bfe748","placeholder":"​","style":"IPY_MODEL_532063b3607f46b2aad4ce8dbce068de","value":"100%"}},"a208e8d8b9944955a3d0eee6e8890fb5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7fb4b698ef2d4e398c8742ab8a75de02","max":1,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cd3c904a0a4e4085bc6d7f0bc4d3ecb6","value":1}},"901a6540128d4204b098bde09843b822":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd5aa0b926f641b7ae310e35d10db58e","placeholder":"​","style":"IPY_MODEL_dea9355b39154005aefcc52caea58dd2","value":" 1/1 [00:00&lt;00:00, 29.43it/s]"}},"6858411de99340eabdddf3e6709b4e4b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e53116a2750340969b19215a02bfe748":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"532063b3607f46b2aad4ce8dbce068de":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7fb4b698ef2d4e398c8742ab8a75de02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd3c904a0a4e4085bc6d7f0bc4d3ecb6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cd5aa0b926f641b7ae310e35d10db58e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dea9355b39154005aefcc52caea58dd2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"03ea1d3264a64819bc3d4527a74fa793":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_26712c36d67840b38f8f0c82a56cb799","IPY_MODEL_26a99ad2d866437792f6b6bea4847b64","IPY_MODEL_69bff586a4634d12be227f1c365ab975"],"layout":"IPY_MODEL_8fffb6dac2fe44d1963a9a5fcd624b51"}},"26712c36d67840b38f8f0c82a56cb799":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3008bae0fc4340149dc036d10a97d9fb","placeholder":"​","style":"IPY_MODEL_5e06e1b5282a4658bef3d10c3c84a6bf","value":"Downloading tokenizer.json: 100%"}},"26a99ad2d866437792f6b6bea4847b64":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fb789ded969945efb39ea360dca78990","max":1355863,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f2d81834a22e487eb165f6499deadadf","value":1355863}},"69bff586a4634d12be227f1c365ab975":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9181632a84894f67b11f7b748e94d2ca","placeholder":"​","style":"IPY_MODEL_e074bfc0fa2d46db948671057606e490","value":" 1.29M/1.29M [00:00&lt;00:00, 7.99MB/s]"}},"8fffb6dac2fe44d1963a9a5fcd624b51":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3008bae0fc4340149dc036d10a97d9fb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5e06e1b5282a4658bef3d10c3c84a6bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fb789ded969945efb39ea360dca78990":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f2d81834a22e487eb165f6499deadadf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"9181632a84894f67b11f7b748e94d2ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e074bfc0fa2d46db948671057606e490":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}