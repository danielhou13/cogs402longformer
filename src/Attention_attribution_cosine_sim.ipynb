{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlqrlFd9SzDV"
   },
   "source": [
    "This notebook explores the relation between the model's attributions and attentions for a given example. Historically, we found that attentions are not a feasible method of explanation whereas attributions are, but attributions are also not part of a model's traditional outputs. Therefore it may be interesting to see if we can find anything with attentions by comparing them to a feasible and plausible method of explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4G07aiA37RC9",
    "outputId": "11e24362-97b2-4ad7-f0c2-b12bd625184b"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HkZ5bD2_z-0C"
   },
   "source": [
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "zMjzQIFJ2P_T"
   },
   "outputs": [],
   "source": [
    "# pip install transformers --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Uno0qwr12UTd"
   },
   "outputs": [],
   "source": [
    "# pip install captum --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "OaOSPMJE3ONc"
   },
   "outputs": [],
   "source": [
    "# pip install datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: rbo in c:\\users\\danie\\miniconda3\\lib\\site-packages (0.1.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.18 in c:\\users\\danie\\miniconda3\\lib\\site-packages (from rbo) (1.22.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install rbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "hRSNYTrRIPkr"
   },
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wrRRZJ6-0_Il"
   },
   "outputs": [],
   "source": [
    "from captum.attr import visualization as viz\n",
    "from captum.attr import IntegratedGradients, LayerConductance, LayerIntegratedGradients\n",
    "from captum.attr import configure_interpretable_embedding_layer, remove_interpretable_embedding_layer\n",
    "\n",
    "import torch\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "b5V31lsc0_Il"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "USHRv2j70Fb4"
   },
   "source": [
    "## Import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "9nzBfB-v0_Im"
   },
   "outputs": [],
   "source": [
    "from transformers import LongformerForSequenceClassification, LongformerTokenizer, LongformerConfig\n",
    "# replace <PATH-TO-SAVED-MODEL> with the real path of the saved model\n",
    "model_path = 'danielhou13/longformer-finetuned_papers_v2'\n",
    "#model_path = 'danielhou13/longformer-finetuned-new-cogs402'\n",
    "\n",
    "# load model\n",
    "model = LongformerForSequenceClassification.from_pretrained(model_path, num_labels = 2)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "model.zero_grad()\n",
    "\n",
    "# load tokenizer\n",
    "tokenizer = LongformerTokenizer.from_pretrained(\"allenai/longformer-base-4096\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bqbvfvXv0VTp"
   },
   "source": [
    "Create functions that give us the input ids and the position ids for the text we want to examine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "y5-heo2y0_Im"
   },
   "outputs": [],
   "source": [
    "def predict(inputs, position_ids=None, attention_mask=None):\n",
    "    output = model(inputs,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask)\n",
    "    return output.logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "vAAjmDRl0_In"
   },
   "outputs": [],
   "source": [
    "ref_token_id = tokenizer.pad_token_id # A token used for generating token reference\n",
    "sep_token_id = tokenizer.sep_token_id # A token used as a separator between question and text and it is also added to the end of the text.\n",
    "cls_token_id = tokenizer.cls_token_id # A token used for prepending to the concatenated question-text word sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "UgBSBpz-0_In"
   },
   "outputs": [],
   "source": [
    "max_length = 2046\n",
    "def construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id):\n",
    "\n",
    "    text_ids = tokenizer.encode(text, truncation = True, add_special_tokens=False, max_length = max_length)\n",
    "    # construct input token ids\n",
    "    input_ids = [cls_token_id] + text_ids + [sep_token_id]\n",
    "    # construct reference token ids \n",
    "    ref_input_ids = [cls_token_id] + [ref_token_id] * len(text_ids) + [sep_token_id]\n",
    "\n",
    "    return torch.tensor([input_ids], device=device), torch.tensor([ref_input_ids], device=device), len(text_ids)\n",
    "\n",
    "def construct_input_ref_pos_id_pair(input_ids):\n",
    "    seq_length = input_ids.size(1)\n",
    "\n",
    "    #taken from the longformer implementation\n",
    "    mask = input_ids.ne(ref_token_id).int()\n",
    "    incremental_indices = torch.cumsum(mask, dim=1).type_as(mask) * mask\n",
    "    position_ids = incremental_indices.long().squeeze() + ref_token_id\n",
    "\n",
    "    # we could potentially also use random permutation with `torch.randperm(seq_length, device=device)`\n",
    "    ref_position_ids = torch.zeros(seq_length, dtype=torch.long, device=device)\n",
    "\n",
    "    position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    ref_position_ids = ref_position_ids.unsqueeze(0).expand_as(input_ids)\n",
    "    return position_ids, ref_position_ids\n",
    "    \n",
    "def construct_attention_mask(input_ids):\n",
    "    return torch.ones_like(input_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdY6GsO00RG_"
   },
   "source": [
    "Import dataset and take a few examples from it for testing purposes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e_gGIPpwkmbH"
   },
   "source": [
    "Here we import the papers dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "0966e7f95cb34762b3ac81e97389b784",
      "de5cbef4d77a435781567241fb0c4e38",
      "e9b750ae6a7f4e1b86c324eb973d0dc7",
      "f817de62cd434a98a25fe83c15b209c8",
      "ab3f6334df3f4e89b7e6e479cd3323ee",
      "9207ad0cc09541bf8e9e5f867e61dfd5",
      "7a7ed35e872043e390c9caa0e85faf2c",
      "8dbbf4b9b7c84b8eb84311c25588cdb6",
      "a27affa8c60e476f97c869a4e88fb687",
      "de96b22b81ff4725b21fff44c6aeebbd",
      "5f77e03e8c38476cb61ccc61969475d2"
     ]
    },
    "id": "w6aQM9nM0_Ip",
    "outputId": "346d6dab-126f-4f70-8f96-d6ecc3fcaefa"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration danielhou13--cogs402dataset-3b57e27666917d08\n",
      "Reusing dataset parquet (C:\\Users\\danie\\.cache\\huggingface\\datasets\\parquet\\danielhou13--cogs402dataset-3b57e27666917d08\\0.0.0\\0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cda9c7e7dea405aa5098d6e763df745",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "cogs402_ds = load_dataset(\"danielhou13/cogs402dataset\")[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bHUUw096r-EL"
   },
   "source": [
    "Here we import the news dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "58zzpZYRWRDj"
   },
   "outputs": [],
   "source": [
    "# cogs402_ds2 = load_dataset('hyperpartisan_news_detection', 'bypublisher')['validation']\n",
    "# val_size = 5000\n",
    "# val_indices = np.random.randint(0, len(cogs402_ds2), val_size)\n",
    "# val_ds = cogs402_ds2.select(val_indices)\n",
    "# labels2 = map(int, val_ds['hyperpartisan'])\n",
    "# labels2 = list(labels2)\n",
    "# val_ds = val_ds.add_column(\"labels\", labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "axwpHq-y0_Io"
   },
   "outputs": [],
   "source": [
    "#set 1 if we are dealing with a positive class, and 0 if dealing with negative class\n",
    "def custom_forward(inputs, position_ids=None, attention_mask=None):\n",
    "    preds = predict(inputs,\n",
    "                   position_ids=position_ids,\n",
    "                   attention_mask=attention_mask\n",
    "                   )\n",
    "    return torch.softmax(preds, dim = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bQaYaSDf0buh"
   },
   "source": [
    "Perform Layer Integrated Gradients using the longformer's embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "lIeE9P7b0_Ir"
   },
   "outputs": [],
   "source": [
    "def summarize_attributions(attributions):\n",
    "    attributions = attributions.sum(dim=-1).squeeze(0)\n",
    "    return attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "4uDuDrip0_Ip"
   },
   "outputs": [],
   "source": [
    "lig = LayerIntegratedGradients(custom_forward, model.longformer.embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n2Xp75N0rtqa"
   },
   "source": [
    "This function will let us get the example and the baseline inputs in order to perform integrated gradients, and add the attributions to our visualization tool. Additionally, we will add the attributions and tokens for each example into an array so we can use them when we want to further example the attributions scores for each example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "wf1SNgF73l1K"
   },
   "outputs": [],
   "source": [
    "all_attributions = {}\n",
    "all_tokens = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "g4nc7qSvUedJ"
   },
   "outputs": [],
   "source": [
    "all_attributions = torch.load('example_attrib_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Cjdh0g6UedZo"
   },
   "outputs": [],
   "source": [
    "example = 891\n",
    "text = cogs402_ds['text'][example]\n",
    "label = cogs402_ds['labels'][example]\n",
    "\n",
    "input_ids, ref_input_ids, sep_id = construct_input_ref_pair(text, ref_token_id, sep_token_id, cls_token_id)\n",
    "position_ids, ref_position_ids = construct_input_ref_pos_id_pair(input_ids)\n",
    "attention_mask = construct_attention_mask(input_ids)\n",
    "\n",
    "indices = input_ids[0].detach().tolist()\n",
    "all_tokens_curr = tokenizer.convert_ids_to_tokens(indices)\n",
    "\n",
    "all_tokens[str(example)] = all_tokens_curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "BSI1Azc2SxvL"
   },
   "outputs": [],
   "source": [
    "# attributions, delta = lig.attribute(inputs=input_ids,\n",
    "#                                   baselines=ref_input_ids,\n",
    "#                                   return_convergence_delta=True,\n",
    "#                                   additional_forward_args=(position_ids, attention_mask),\n",
    "#                                   target=1,\n",
    "#                                   n_steps=1500,\n",
    "#                                   internal_batch_size = 2)\n",
    "\n",
    "# attributions_sum = summarize_attributions(attributions)\n",
    "\n",
    "# all_attributions[str(example)] = attributions_sum.detach().cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Dvsiq5chRqca"
   },
   "outputs": [],
   "source": [
    "# torch.save(all_attributions, '/content/drive/MyDrive/cogs402longformer/results/papers/papers_attributions/example_attrib_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "eJuGcLQpS-Ky"
   },
   "outputs": [],
   "source": [
    "# all_attributions = torch.load('/content/drive/MyDrive/cogs402longformer/results/papers/papers_attributions/example_attrib_dict.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aA--ygMwhjNm",
    "outputId": "c61075fb-7aff-4011-9d85-b2bb30499eea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'605': array([ 0.00000000e+00, -3.40173878e-06, -1.12807776e-06, ...,\n",
      "        4.23677990e-06,  7.29073352e-06,  0.00000000e+00]), '976': array([ 0.00000000e+00, -1.84040401e-04,  1.46363292e-05, ...,\n",
      "       -2.48273481e-06,  1.11380563e-04,  0.00000000e+00]), '148': array([ 0.        ,  0.00225466,  0.00335924, ...,  0.00014387,\n",
      "       -0.000275  ,  0.        ]), '891': array([ 0.00000000e+00, -1.05098504e-03, -2.83239306e-03, ...,\n",
      "        1.52405500e-06,  9.14739375e-05,  0.00000000e+00])}\n"
     ]
    }
   ],
   "source": [
    "print(all_attributions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGDWlUa3GvNS"
   },
   "source": [
    "We then get the attentions and global attentions so we can compare with the attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mx69KUz20KmC",
    "outputId": "3871dc93-5e59-4a8c-a339-59f5cbe159b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_attention.shape torch.Size([12, 1, 12, 2048, 514])\n",
      "gl_output_attention.shape torch.Size([12, 1, 12, 2048, 1])\n"
     ]
    }
   ],
   "source": [
    "output = model(input_ids.cuda(), attention_mask=attention_mask.cuda(), labels=torch.tensor(label).cuda(), output_attentions = True)\n",
    "batch_attn = output[-2]\n",
    "output_attentions = torch.stack(batch_attn).cpu()\n",
    "global_attention = output[-1]\n",
    "output_global_attentions = torch.stack(global_attention).cpu()\n",
    "print(\"output_attention.shape\", output_attentions.shape)\n",
    "print(\"gl_output_attention.shape\", output_global_attentions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dISC7-FaG2Lh"
   },
   "source": [
    "Since the longformer has a unique attention matrix shape, we convert it into the required sequence length x sequence length matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "AFyZpNST1JRr"
   },
   "outputs": [],
   "source": [
    "def create_head_matrix(output_attentions, global_attentions):\n",
    "    new_attention_matrix = torch.zeros((output_attentions.shape[0], \n",
    "                                      output_attentions.shape[0]))\n",
    "    for i in range(output_attentions.shape[0]):\n",
    "        test_non_zeroes = torch.nonzero(output_attentions[i]).squeeze()\n",
    "        test2 = output_attentions[i][test_non_zeroes[1:]]\n",
    "        new_attention_matrix_indices = test_non_zeroes[1:]-257 + i\n",
    "        new_attention_matrix[i][new_attention_matrix_indices] = test2\n",
    "        new_attention_matrix[i][0] = output_attentions[i][0]\n",
    "        new_attention_matrix[0] = global_attentions.squeeze()[:output_attentions.shape[0]]\n",
    "    return new_attention_matrix\n",
    "\n",
    "\n",
    "def attentions_all_heads(output_attentions, global_attentions):\n",
    "    new_matrix = []\n",
    "    for i in range(output_attentions.shape[0]):\n",
    "        matrix = create_head_matrix(output_attentions[i], global_attentions[i])\n",
    "        new_matrix.append(matrix)\n",
    "    return torch.stack(new_matrix)\n",
    "\n",
    "def all_batches(output_attentions, global_attentions):\n",
    "    new_matrix = []\n",
    "    for i in range(output_attentions.shape[0]):\n",
    "        matrix = attentions_all_heads(output_attentions[i], global_attentions[i])\n",
    "        new_matrix.append(matrix)\n",
    "    return torch.stack(new_matrix)\n",
    "\n",
    "def all_layers(output_attentions, global_attentions):\n",
    "    new_matrix = []\n",
    "    for i in range(output_attentions.shape[0]):\n",
    "        matrix = all_batches(output_attentions[i], global_attentions[i])\n",
    "        new_matrix.append(matrix)\n",
    "    return torch.stack(new_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q48gadDy1Kgi",
    "outputId": "f5619662-fdce-495e-be3d-116cb33ea448"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1, 12, 2048, 2048)\n"
     ]
    }
   ],
   "source": [
    "converted_mat = all_layers(output_attentions, output_global_attentions).detach().cpu().numpy()\n",
    "print(converted_mat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WJynBkquG9cv"
   },
   "source": [
    "We scale the attention matrix by head importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "Bt_2TsKb1ML5"
   },
   "outputs": [],
   "source": [
    "head_importance = torch.load(\"T3-vis/papers/head_importance.pt\")\n",
    "# head_importance = torch.load(\"/content/drive/MyDrive/cogs402longformer/t3-visapplication/resources/news/head_importance.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "nDuqZlIZ1NSr"
   },
   "outputs": [],
   "source": [
    "def scale_by_importance(attention_matrix, head_importance):\n",
    "  new_matrix = np.zeros_like(attention_matrix)\n",
    "  for i in range(attention_matrix.shape[0]):\n",
    "    head_importance_layer = head_importance[i]\n",
    "    for j in range(attention_matrix.shape[1]):\n",
    "      new_matrix[i,j] = attention_matrix[i,j] * np.expand_dims(head_importance_layer, axis=(1,2))\n",
    "  return new_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "u5-YW1SN1OYP"
   },
   "outputs": [],
   "source": [
    "converted_mat_importance = scale_by_importance(converted_mat, head_importance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XywJLFZ6HBWO"
   },
   "source": [
    "We get the attentions for each token. The shape of the attention matrix is layer x batch x head x seq_len x seq_len."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gu9biMOM1UWC",
    "outputId": "36a25ee9-9563-48f8-eeb1-60497dd276b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 1, 12, 2048)\n"
     ]
    }
   ],
   "source": [
    "attention_matrix_importance = converted_mat_importance.sum(axis=3)\n",
    "print(attention_matrix_importance.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gB7w8fsMHFT2"
   },
   "source": [
    "Sum the attentions for the last layer and over all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kvEhE5HP1XUQ",
    "outputId": "292649b7-e8a2-497d-dc47-65ddca5e50ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2048,)\n"
     ]
    }
   ],
   "source": [
    "attention_final_layer = attention_matrix_importance[11].squeeze().sum(axis=0)\n",
    "attention_all_layer = attention_matrix_importance.squeeze().sum(axis=1)\n",
    "attention_all_layer = attention_all_layer.sum(axis=0)\n",
    "print(attention_all_layer.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GoxYNBsHHLdc"
   },
   "source": [
    "Grab the attributions we stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "na6SKwOp_htK"
   },
   "outputs": [],
   "source": [
    "exam_attrib = all_attributions[str(example)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jp40sVMVBDYt"
   },
   "source": [
    "Since we have the attributions and the attentions, we want to see how the largest attributions (in terms of magnitude) compares to the highest attentions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05cKorNqG-2T"
   },
   "source": [
    "Cosine similarity using the raw attributions and attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_kWnacBd9qk5",
    "outputId": "34135aa0-47dd-4bab-b342-8aece2e07d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 12 Cosine Similarity raw attrib:\n",
      " 0.02248637021832213\n",
      "Layer 12 Cosine Similarity raw attrib:\n",
      " -0.012734362617167585\n"
     ]
    }
   ],
   "source": [
    "from numpy.linalg import norm\n",
    "cosine_raw = np.dot(exam_attrib, attention_final_layer) / (norm(exam_attrib)*norm(attention_final_layer))\n",
    "print(\"Layer 12 Cosine Similarity raw attrib:\\n\", cosine_raw)\n",
    "cosine_all_raw = np.dot(exam_attrib, attention_all_layer) / (norm(exam_attrib)*norm(attention_all_layer))\n",
    "print(\"Layer 12 Cosine Similarity raw attrib:\\n\", cosine_all_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6cwcV4jXNWe-"
   },
   "source": [
    "The attributions and the attentions have different ranges. The attributions could range from -1 to 1 whereas the attentions range from 0 to 1. However, negative attributions would not necessarily mean that they have the lowest attention, rather they might have really high attention as they are more likely to help the model predict the negative class, and might be something the attentions picked up as a feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "ukncMseTj1-K"
   },
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    return (data - np.min(data)) / (np.max(data) - np.min(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "b0fjYbf5k5qv"
   },
   "outputs": [],
   "source": [
    "attention_final_layer2 = normalize(attention_final_layer)\n",
    "attention_all_layer2 = normalize(attention_all_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "gQIFZTQzlAEI"
   },
   "outputs": [],
   "source": [
    "exam_attrib2 = np.abs(exam_attrib)\n",
    "exam_attrib2 = normalize(exam_attrib2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtCEZaQFOgwh",
    "outputId": "78681fe0-8d58-467a-a1d2-265e9186bc07"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 4.56952825e-03 1.23148281e-02 ... 6.62636680e-06\n",
      " 3.97715217e-04 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "print(exam_attrib2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wuflawaXGUAK"
   },
   "source": [
    "Calculate cosine simularity using normalized attentions and attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oV1y_ZDk_gFu",
    "outputId": "8b8b27bc-1ede-4cfa-b73e-64893bd65874"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 12 Cosine Similarity:\n",
      " 0.17583135298631875\n",
      "All layer Cosine Similarity:\n",
      " 0.10448268465151779\n"
     ]
    }
   ],
   "source": [
    "cosine = np.dot(exam_attrib2, attention_final_layer2) / (norm(exam_attrib2)*norm(attention_final_layer2))\n",
    "print(\"Layer 12 Cosine Similarity:\\n\", cosine)\n",
    "cosine2 = np.dot(exam_attrib2, attention_all_layer2) / (norm(exam_attrib2)*norm(attention_all_layer2))\n",
    "print(\"All layer Cosine Similarity:\\n\", cosine2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0ivl51-oHFPu"
   },
   "source": [
    "Cosine similarity while setting all the attention and attribution values below the median to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "X5q_9i4iOoFs"
   },
   "outputs": [],
   "source": [
    "exam_attrib3 = np.abs(exam_attrib)\n",
    "exam_attrib3 = normalize(exam_attrib3)\n",
    "median_exam = np.percentile(exam_attrib3, 50)\n",
    "exam_attrib3[exam_attrib3 < median_exam] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "9zTGe6IEGeFv"
   },
   "outputs": [],
   "source": [
    "attention_final_layer3 = np.copy(attention_final_layer)\n",
    "attention_final_layer3 = normalize(attention_final_layer3)\n",
    "median_12 = np.percentile(attention_final_layer3, 50)\n",
    "attention_final_layer3[attention_final_layer3 < median_12] = 0\n",
    "\n",
    "attention_all_layer3 = np.copy(attention_all_layer) \n",
    "attention_all_layer3 = normalize(attention_all_layer3)\n",
    "median_all = np.percentile(attention_all_layer3, 50)\n",
    "attention_all_layer3[attention_all_layer3 < median_all] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i8cKjhfAHrWi",
    "outputId": "c1aee7c2-1fc6-4885-c4ec-93d78e93d0e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 12 Cosine Similarity med:\n",
      " 0.17376752101559828\n",
      "All layer Cosine Similarity med:\n",
      " 0.10084104034199037\n"
     ]
    }
   ],
   "source": [
    "cosine_med = np.dot(exam_attrib3, attention_final_layer3) / (norm(exam_attrib3)*norm(attention_final_layer3))\n",
    "print(\"Layer 12 Cosine Similarity med:\\n\", cosine_med)\n",
    "cosine_med2 = np.dot(exam_attrib3, attention_all_layer3) / (norm(exam_attrib3)*norm(attention_all_layer3))\n",
    "print(\"All layer Cosine Similarity med:\\n\", cosine_med2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7srdG4VeT4wt"
   },
   "source": [
    "Cosine similarity while setting all the attention and attribution values below the mean to 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "NHw9NX_cWwTT"
   },
   "outputs": [],
   "source": [
    "exam_attrib4 = np.abs(exam_attrib)\n",
    "exam_attrib4 = normalize(exam_attrib4)\n",
    "mean_exam = np.mean(exam_attrib4)\n",
    "exam_attrib4[exam_attrib4 < mean_exam] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "EHj9zUwrUee-"
   },
   "outputs": [],
   "source": [
    "attention_final_layer4 = np.copy(attention_final_layer)\n",
    "attention_final_layer4 = normalize(attention_final_layer4)\n",
    "mean_12 = np.mean(attention_final_layer4)\n",
    "attention_final_layer4[attention_final_layer4 < mean_12] = 0\n",
    "\n",
    "attention_all_layer4 = np.copy(attention_all_layer) \n",
    "attention_all_layer4 = normalize(attention_all_layer4)\n",
    "mean_all = np.mean(attention_all_layer4)\n",
    "attention_all_layer4[attention_all_layer4 < mean_all] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NWlVxlzHUlAj",
    "outputId": "035b82c3-2f69-41a1-b0cf-55c815feabdf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 12 Cosine Similarity mean:\n",
      " 0.15724215783372744\n",
      "All layer Cosine Similarity mean:\n",
      " 0.08985276883412399\n"
     ]
    }
   ],
   "source": [
    "cosine_mean = np.dot(exam_attrib4, attention_final_layer4) / (norm(exam_attrib4)*norm(attention_final_layer4))\n",
    "print(\"Layer 12 Cosine Similarity mean:\\n\", cosine_mean)\n",
    "cosine_mean2 = np.dot(exam_attrib4, attention_all_layer4) / (norm(exam_attrib4)*norm(attention_all_layer4))\n",
    "print(\"All layer Cosine Similarity mean:\\n\", cosine_mean2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHUIfDlnjlFU"
   },
   "source": [
    "Cosine Similarity using the ranks of each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F8t6CWK6jkG8",
    "outputId": "9d8490ed-4e06-414f-98e0-f72f11649fa7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0 2047 2037 ...  695   32  720]\n"
     ]
    }
   ],
   "source": [
    "exam_attrib_rank = np.abs(exam_attrib)\n",
    "order_attrib = exam_attrib_rank.argsort()\n",
    "print(order_attrib)\n",
    "ranks_attrib = order_attrib.argsort()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "Abb5yDNljwdI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1662 1922 2014 ... 1864 1823 1803]\n",
      "[1528  239  308 ...  360  187 1114]\n"
     ]
    }
   ],
   "source": [
    "attention_final_layer_rank = np.copy(attention_final_layer)\n",
    "order = attention_final_layer_rank.argsort()\n",
    "print(order)\n",
    "ranks = order.argsort()+1\n",
    "print(ranks)\n",
    "\n",
    "attention_all_layer_rank = np.copy(attention_all_layer)\n",
    "order2 = attention_all_layer_rank.argsort()\n",
    "ranks2 = order2.argsort()+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "H7Ucz3QdtBxN",
    "outputId": "126dddf6-2567-4108-cd73-e3a31f33ccc5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 12 Cosine Similarity rank:\n",
      " 0.8430761265725671\n",
      "All layer Cosine Similarity rank:\n",
      " 0.7993863440837687\n"
     ]
    }
   ],
   "source": [
    "cosine_rank = np.dot(ranks_attrib, ranks) / (norm(ranks_attrib)*norm(ranks))\n",
    "print(\"Layer 12 Cosine Similarity rank:\\n\", cosine_rank)\n",
    "cosine_rank2 = np.dot(ranks_attrib, ranks2) / (norm(ranks_attrib)*norm(ranks2))\n",
    "print(\"All layer Cosine Similarity rank:\\n\", cosine_rank2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kendall Tau metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tau statistic layer 12: 0.25441099016853935 p value 1.0218679901904437e-66\n",
      "Tau statistic: all layers 0.13212456491206645 p value 3.2100350471997357e-19\n"
     ]
    }
   ],
   "source": [
    "import scipy.stats as stats\n",
    "tau, p_value = stats.kendalltau(ranks_attrib, ranks)\n",
    "print(\"Tau statistic layer 12:\", tau, \"p value\", p_value)\n",
    "tau, p_value = stats.kendalltau(ranks_attrib, ranks2)\n",
    "print(\"Tau statistic: all layers\", tau, \"p value\", p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbo layer 12 0.6031866800272659\n",
      "rbo all 0.5311173049407113\n"
     ]
    }
   ],
   "source": [
    "import rbo\n",
    "print(\"rbo layer 12\", rbo.RankingSimilarity(order_attrib, order).rbo())\n",
    "print(\"rbo all\", rbo.RankingSimilarity(order_attrib, order2).rbo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PGIoaQ3QOaOf"
   },
   "source": [
    "The cosine similarity using only the last layer of attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "-da4EW2rBkIX",
    "outputId": "6d4da7be-23d6-4c21-e85a-ea0c47106bb0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example</th>\n",
       "      <th>similarity normalized</th>\n",
       "      <th>similarity raw</th>\n",
       "      <th>sim_norm w/ median threshold</th>\n",
       "      <th>sim_norm w/ mean threshold</th>\n",
       "      <th>sim w/ ranks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>891</td>\n",
       "      <td>0.175831</td>\n",
       "      <td>0.022486</td>\n",
       "      <td>0.173768</td>\n",
       "      <td>0.157242</td>\n",
       "      <td>0.843076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example  similarity normalized  similarity raw  \\\n",
       "0      891               0.175831        0.022486   \n",
       "\n",
       "   sim_norm w/ median threshold  sim_norm w/ mean threshold  sim w/ ranks  \n",
       "0                      0.173768                    0.157242      0.843076  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = {'example': [example], 'similarity normalized': [cosine], 'similarity raw': [cosine_raw], 'sim_norm w/ median threshold':[cosine_med], 'sim_norm w/ mean threshold':[cosine_mean], \"sim w/ ranks\":cosine_rank}\n",
    "df = pd.DataFrame(data=d)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7w4ofGq2OfLC"
   },
   "source": [
    "The cosine similarity using all layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 81
    },
    "id": "ep6wbADaFgME",
    "outputId": "7c6f66b8-17b5-4275-d535-d7f85e3fd95c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example</th>\n",
       "      <th>similarity normalized</th>\n",
       "      <th>similarity raw</th>\n",
       "      <th>sim_norm w/ median threshold</th>\n",
       "      <th>sim_norm w/ mean threshold</th>\n",
       "      <th>sim w/ ranks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>891</td>\n",
       "      <td>0.104483</td>\n",
       "      <td>-0.012734</td>\n",
       "      <td>0.100841</td>\n",
       "      <td>0.089853</td>\n",
       "      <td>0.799386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example  similarity normalized  similarity raw  \\\n",
       "0      891               0.104483       -0.012734   \n",
       "\n",
       "   sim_norm w/ median threshold  sim_norm w/ mean threshold  sim w/ ranks  \n",
       "0                      0.100841                    0.089853      0.799386  "
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d2 = {'example': [example], 'similarity normalized': [cosine2], 'similarity raw': [cosine_all_raw], 'sim_norm w/ median threshold':[cosine_med2], 'sim_norm w/ mean threshold':[cosine_mean2], \"sim w/ ranks\":cosine_rank2}\n",
    "df2 = pd.DataFrame(data=d2)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "0wdSRDsTCN-J"
   },
   "outputs": [],
   "source": [
    "df_layer12 = pd.read_csv(\"cos_sim_layer12.csv\")\n",
    "df_all = pd.read_csv(\"cos_sim_all.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "UlT8_ax5CwQT",
    "outputId": "3cea2e31-c32b-4962-b576-9189ad35772c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example</th>\n",
       "      <th>similarity normalized</th>\n",
       "      <th>similarity raw</th>\n",
       "      <th>sim_norm w/ median threshold</th>\n",
       "      <th>sim_norm w/ mean threshold</th>\n",
       "      <th>sim w/ ranks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>0.147101</td>\n",
       "      <td>-0.020260</td>\n",
       "      <td>0.142670</td>\n",
       "      <td>0.123295</td>\n",
       "      <td>0.826202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605</td>\n",
       "      <td>0.299375</td>\n",
       "      <td>-0.165507</td>\n",
       "      <td>0.288221</td>\n",
       "      <td>0.271128</td>\n",
       "      <td>0.802063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>891</td>\n",
       "      <td>0.175846</td>\n",
       "      <td>0.022566</td>\n",
       "      <td>0.173801</td>\n",
       "      <td>0.157258</td>\n",
       "      <td>0.842796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>976</td>\n",
       "      <td>0.141923</td>\n",
       "      <td>0.082403</td>\n",
       "      <td>0.137433</td>\n",
       "      <td>0.119868</td>\n",
       "      <td>0.823112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example  similarity normalized  similarity raw  \\\n",
       "0      148               0.147101       -0.020260   \n",
       "1      605               0.299375       -0.165507   \n",
       "2      891               0.175846        0.022566   \n",
       "3      976               0.141923        0.082403   \n",
       "\n",
       "   sim_norm w/ median threshold  sim_norm w/ mean threshold  sim w/ ranks  \n",
       "0                      0.142670                    0.123295      0.826202  \n",
       "1                      0.288221                    0.271128      0.802063  \n",
       "2                      0.173801                    0.157258      0.842796  \n",
       "3                      0.137433                    0.119868      0.823112  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_layer12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "o0P3tm47FP2N",
    "outputId": "91beae48-6473-4278-ee58-191005a21b48"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example</th>\n",
       "      <th>similarity normalized</th>\n",
       "      <th>similarity raw</th>\n",
       "      <th>sim_norm w/ median threshold</th>\n",
       "      <th>sim_norm w/ mean threshold</th>\n",
       "      <th>sim w/ ranks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>0.140745</td>\n",
       "      <td>0.016048</td>\n",
       "      <td>0.134218</td>\n",
       "      <td>0.123968</td>\n",
       "      <td>0.805108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605</td>\n",
       "      <td>0.249735</td>\n",
       "      <td>-0.219944</td>\n",
       "      <td>0.233159</td>\n",
       "      <td>0.219811</td>\n",
       "      <td>0.784505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>891</td>\n",
       "      <td>0.104379</td>\n",
       "      <td>-0.012725</td>\n",
       "      <td>0.100741</td>\n",
       "      <td>0.089762</td>\n",
       "      <td>0.799092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>976</td>\n",
       "      <td>0.115270</td>\n",
       "      <td>0.096755</td>\n",
       "      <td>0.109309</td>\n",
       "      <td>0.097665</td>\n",
       "      <td>0.819613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example  similarity normalized  similarity raw  \\\n",
       "0      148               0.140745        0.016048   \n",
       "1      605               0.249735       -0.219944   \n",
       "2      891               0.104379       -0.012725   \n",
       "3      976               0.115270        0.096755   \n",
       "\n",
       "   sim_norm w/ median threshold  sim_norm w/ mean threshold  sim w/ ranks  \n",
       "0                      0.134218                    0.123968      0.805108  \n",
       "1                      0.233159                    0.219811      0.784505  \n",
       "2                      0.100741                    0.089762      0.799092  \n",
       "3                      0.109309                    0.097665      0.819613  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RSbHulrIOjG7"
   },
   "source": [
    "Append the new row into the dataframe.\n",
    "\n",
    "Comment out if revisiting a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "-xxzlJwpCeX0"
   },
   "outputs": [],
   "source": [
    "# df_layer12 = pd.concat([df, df_layer12], axis=0)\n",
    "# df_all = pd.concat([df2, df_all], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "pW8I8FIXJWrN",
    "outputId": "bab08b44-999b-404d-fd32-e5f31c381f28"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example</th>\n",
       "      <th>similarity normalized</th>\n",
       "      <th>similarity raw</th>\n",
       "      <th>sim_norm w/ median threshold</th>\n",
       "      <th>sim_norm w/ mean threshold</th>\n",
       "      <th>sim w/ ranks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>0.147101</td>\n",
       "      <td>-0.020260</td>\n",
       "      <td>0.142670</td>\n",
       "      <td>0.123295</td>\n",
       "      <td>0.826202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605</td>\n",
       "      <td>0.299375</td>\n",
       "      <td>-0.165507</td>\n",
       "      <td>0.288221</td>\n",
       "      <td>0.271128</td>\n",
       "      <td>0.802063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>891</td>\n",
       "      <td>0.175846</td>\n",
       "      <td>0.022566</td>\n",
       "      <td>0.173801</td>\n",
       "      <td>0.157258</td>\n",
       "      <td>0.842796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>976</td>\n",
       "      <td>0.141923</td>\n",
       "      <td>0.082403</td>\n",
       "      <td>0.137433</td>\n",
       "      <td>0.119868</td>\n",
       "      <td>0.823112</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example  similarity normalized  similarity raw  \\\n",
       "0      148               0.147101       -0.020260   \n",
       "1      605               0.299375       -0.165507   \n",
       "2      891               0.175846        0.022566   \n",
       "3      976               0.141923        0.082403   \n",
       "\n",
       "   sim_norm w/ median threshold  sim_norm w/ mean threshold  sim w/ ranks  \n",
       "0                      0.142670                    0.123295      0.826202  \n",
       "1                      0.288221                    0.271128      0.802063  \n",
       "2                      0.173801                    0.157258      0.842796  \n",
       "3                      0.137433                    0.119868      0.823112  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_layer12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "id": "GGbVSSH4JUUG",
    "outputId": "ea223bda-3faf-42de-fcb6-1ff6683ff994"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>example</th>\n",
       "      <th>similarity normalized</th>\n",
       "      <th>similarity raw</th>\n",
       "      <th>sim_norm w/ median threshold</th>\n",
       "      <th>sim_norm w/ mean threshold</th>\n",
       "      <th>sim w/ ranks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>148</td>\n",
       "      <td>0.140745</td>\n",
       "      <td>0.016048</td>\n",
       "      <td>0.134218</td>\n",
       "      <td>0.123968</td>\n",
       "      <td>0.805108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605</td>\n",
       "      <td>0.249735</td>\n",
       "      <td>-0.219944</td>\n",
       "      <td>0.233159</td>\n",
       "      <td>0.219811</td>\n",
       "      <td>0.784505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>891</td>\n",
       "      <td>0.104379</td>\n",
       "      <td>-0.012725</td>\n",
       "      <td>0.100741</td>\n",
       "      <td>0.089762</td>\n",
       "      <td>0.799092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>976</td>\n",
       "      <td>0.115270</td>\n",
       "      <td>0.096755</td>\n",
       "      <td>0.109309</td>\n",
       "      <td>0.097665</td>\n",
       "      <td>0.819613</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   example  similarity normalized  similarity raw  \\\n",
       "0      148               0.140745        0.016048   \n",
       "1      605               0.249735       -0.219944   \n",
       "2      891               0.104379       -0.012725   \n",
       "3      976               0.115270        0.096755   \n",
       "\n",
       "   sim_norm w/ median threshold  sim_norm w/ mean threshold  sim w/ ranks  \n",
       "0                      0.134218                    0.123968      0.805108  \n",
       "1                      0.233159                    0.219811      0.784505  \n",
       "2                      0.100741                    0.089762      0.799092  \n",
       "3                      0.109309                    0.097665      0.819613  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "aFUDQ5Q8ojhx"
   },
   "outputs": [],
   "source": [
    "df_layer12 = df_layer12.sort_values(by=['example'])\n",
    "df_all = df_all.sort_values(by=['example'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "crIEOZx6OnpZ"
   },
   "source": [
    "Save the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "qlQRunAqC3fa"
   },
   "outputs": [],
   "source": [
    "# df_layer12.to_csv(\"/content/drive/MyDrive/cogs402longformer/results/papers/papers_attributions/cos_sim_layer12.csv\", index=False)\n",
    "# df_all.to_csv(\"/content/drive/MyDrive/cogs402longformer/results/papers/papers_attributions/cos_sim_all.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7kNTsdDTtHO"
   },
   "source": [
    "We know from the cosine similarities that it does not seem like the attribtions and the attentions are very similar; however, we can find out if there are similarities in the tokens in the highest percentiles.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R3ftA3vQBCu5",
    "outputId": "1a535589-abc7-430b-a211-a9d4e0519546"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00000000e+00 4.56952825e-03 1.23148281e-02 ... 6.62636680e-06\n",
      " 3.97715217e-04 0.00000000e+00]\n"
     ]
    }
   ],
   "source": [
    "attention_final_layer5 = np.copy(attention_final_layer)\n",
    "attention_final_layer5 = normalize(attention_final_layer5)\n",
    "\n",
    "attention_all_layer5 = np.copy(attention_all_layer) \n",
    "attention_all_layer5 = normalize(attention_all_layer5)\n",
    "\n",
    "exam_attrib5 = np.abs(exam_attrib)\n",
    "exam_attrib5 = normalize(exam_attrib5)\n",
    "print(exam_attrib5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hJAxo97YOCey",
    "outputId": "33748380-fa16-422b-bc51-74e15f414a0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.048841727922329804\n"
     ]
    }
   ],
   "source": [
    "top_final = np.percentile(attention_final_layer5, 95)\n",
    "top_all = np.percentile(attention_final_layer5, 95)\n",
    "top_attrib = np.percentile(exam_attrib5, 95)\n",
    "print(top_attrib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "Cejei_JPPQRq"
   },
   "outputs": [],
   "source": [
    "attention_final_layer5[attention_final_layer5<top_final] = 0\n",
    "attention_all_layer5[attention_all_layer5<top_all] = 0\n",
    "exam_attrib5[exam_attrib5<top_attrib] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CLwaUUcUruOl",
    "outputId": "7eba2c24-b3cf-495e-869b-697b5030f7d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. ... 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(exam_attrib5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "avMBmrohlBV1",
    "outputId": "07e56d86-f1d2-4762-e880-7178611d183e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 12 Cosine Similarity 95th:\n",
      " 0.13128457143072728\n",
      "All layer Cosine Similarity 95th:\n",
      " 0.02065351602442174\n"
     ]
    }
   ],
   "source": [
    "cosine_thresh = np.dot(exam_attrib5, attention_final_layer5) / (norm(exam_attrib5)*norm(attention_final_layer5))\n",
    "print(\"Layer 12 Cosine Similarity 95th:\\n\", cosine_thresh)\n",
    "cosine_thresh2 = np.dot(exam_attrib5, attention_all_layer5) / (norm(exam_attrib5)*norm(attention_all_layer5))\n",
    "print(\"All layer Cosine Similarity 95th:\\n\", cosine_thresh2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uaECjgO-vG0O",
    "outputId": "de672e56-a134-4165-857e-a4ac46ecb17a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "102.0\n",
      "[ 1  0  0 ... 18  0  2]\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "num = 2048 - np.ceil(2048 * 0.95)\n",
    "print(num)\n",
    "\n",
    "exam_attrib_rank2 = np.copy(ranks_attrib)\n",
    "exam_attrib_rank2[exam_attrib_rank2 > num] = 0\n",
    "print(exam_attrib_rank2)\n",
    "\n",
    "attention_final_layer_rank2 = np.copy(ranks)\n",
    "attention_final_layer_rank2[attention_final_layer_rank2 > num] = 0\n",
    "print(attention_final_layer_rank2)\n",
    "\n",
    "attention_all_layer_rank2 = np.copy(ranks2)\n",
    "attention_all_layer_rank2[attention_all_layer_rank2 > num] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "axsWfzNSxSc-",
    "outputId": "4d09168b-2cd3-4780-996f-5f8c3a6b45da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 12 Cosine Similarity 95th ranks:\n",
      " 0.10306584390801075\n",
      "All layer Cosine Similarity 95th ranks:\n",
      " 0.021016561964591663\n"
     ]
    }
   ],
   "source": [
    "cosine_rank_top = np.dot(exam_attrib_rank2, attention_final_layer_rank2) / (norm(exam_attrib_rank2)*norm(attention_final_layer_rank2))\n",
    "print(\"Layer 12 Cosine Similarity 95th ranks:\\n\", cosine_rank_top)\n",
    "cosine_rank_top2 = np.dot(exam_attrib_rank2, attention_all_layer_rank2) / (norm(exam_attrib_rank2)*norm(attention_all_layer_rank2))\n",
    "print(\"All layer Cosine Similarity 95th ranks:\\n\", cosine_rank_top2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0 2047 2037 1621 1683  364 1972 1881 1819  491 1254 1934 1604 1612\n",
      " 1813 1780 1952 2045 1922 1866 1987 1687 1983 1960 1591 1849 1933  935\n",
      " 2026 1893 1153 1706 1835 2022 1867 1262 1921 1349 1841 1899 1929 1947\n",
      " 2042 1949 1195 1677 1885 1280 1887 1831 1644 1930 1828 1984 1938 1185\n",
      "  913 1889 2009 1918 2023 1134 1969 1903 1985 1964 1965 1978 1940 1839\n",
      " 1998 2003 1994 1948 2020 1966 2014 1645 1371 1606 1907 1757 1726 1331\n",
      " 2033 1927 1848 1812 2007 1882 1924 1698 1454 2010 1412 1665 1720 1981\n",
      " 1992 1467  930 1678]\n",
      "[1662 1922 2014 1858 1927 1764  592 1615 1960 1796 1528  977 1868   12\n",
      "  118 1586 1986 1999 1014 1751 1089 1601 1985   50 1598 1641 1862 1987\n",
      " 2022 1509 1669   61 1446 2033 1596   57 1663 1774 1141  609 1982  121\n",
      "  362   52 1088 2008 1494 2031 1066 1854  942    5 1594 2036 1436    8\n",
      " 1715 2017 1605 1116 1856 1608 1950  165 2035 1595   11  883 2037 1938\n",
      " 1936 1980 1770   22 1496 2006 1454 1119    6 1988 1016 1602 1819 1590\n",
      " 2010 1024 1944   35 1506 1756 1903 1931  831  745 1616 1391 1815 1582\n",
      " 1549 1642 2042  375]\n",
      "[2023   41 1026 2009 2022  936  925 1120  363   59   97  651 2020  441\n",
      " 1517 1534 1006 1479   61  833 1124 1987 2027 1121  842 1085  926 1563\n",
      " 2010  560  396 1090 2036 1543 2041  372  146 1516 1442 1640  220 2035\n",
      "  767 1327 1241  312  653 1402  724  294  894 1443  915  819 2040 1765\n",
      "   40 1112 1110 1261 1533  561 1089 1564  362  832 1169 1641  830 1554\n",
      "  421   54 1008 1502  828 1501  369  339  449  318 1518  506  448   53\n",
      "  336 1428  289  924  466  319 1410 2008  983 1153  376 1337 1403  465\n",
      "   60  779  212  368]\n"
     ]
    }
   ],
   "source": [
    "exam_attrib_order2 = np.copy(order_attrib)\n",
    "print(exam_attrib_order2[:int(num)])\n",
    "\n",
    "attention_final_layer_order2 = np.copy(order)\n",
    "print(attention_final_layer_order2[:int(num)])\n",
    "\n",
    "attention_all_layer_order2 = np.copy(order2)\n",
    "print(attention_all_layer_order2[:int(num)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rbo layer 12 95th 0.07937527536789793\n",
      "rbo all 95th 0.04187165721843579\n"
     ]
    }
   ],
   "source": [
    "print(\"rbo layer 12 95th\", rbo.RankingSimilarity(exam_attrib_order2[:int(num)], attention_final_layer_order2[:int(num)]).rbo())\n",
    "print(\"rbo all 95th\", rbo.RankingSimilarity(exam_attrib_order2[:int(num)], attention_all_layer_order2[:int(num)]).rbo())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mV9EKBe8QIdA",
    "outputId": "6623cea1-49b4-4b8a-d589-a5299b2d9b7f"
   },
   "outputs": [],
   "source": [
    "attention_final_layer_top = np.flatnonzero(attention_final_layer5)\n",
    "attention_final_layer_top = set(attention_final_layer_top)\n",
    "\n",
    "attention_all_layer_top = np.flatnonzero(attention_all_layer5)\n",
    "attention_all_layer_top = set(attention_all_layer_top)\n",
    "\n",
    "exam_attrib_top = np.flatnonzero(exam_attrib5)\n",
    "exam_attrib_top = set(exam_attrib_top)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM-P3g6fSzsN"
   },
   "source": [
    "Grab the tokens stored in the all tokens dictionary so we can know which tokens we are working with as we currently only have the indices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "hm3JJFcWQfWj"
   },
   "outputs": [],
   "source": [
    "exam_tokens = all_tokens[str(example)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XJPiXABt6klk"
   },
   "source": [
    "Find out which tokens have the highest attentions but not the highest attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "02gaQP2cUfZv",
    "outputId": "e815b12b-4dd6-445e-9e49-1018ce39f2b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>position</th>\n",
       "      <th>attention_norm</th>\n",
       "      <th>attribution_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ġimperative</td>\n",
       "      <td>228</td>\n",
       "      <td>0.069075</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ġmulti</td>\n",
       "      <td>266</td>\n",
       "      <td>0.034261</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>aint</td>\n",
       "      <td>313</td>\n",
       "      <td>0.040501</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>.</td>\n",
       "      <td>413</td>\n",
       "      <td>0.536035</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ġresource</td>\n",
       "      <td>429</td>\n",
       "      <td>0.034572</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>.</td>\n",
       "      <td>1803</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>.</td>\n",
       "      <td>1823</td>\n",
       "      <td>0.976264</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>.</td>\n",
       "      <td>1864</td>\n",
       "      <td>0.928553</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>.</td>\n",
       "      <td>1910</td>\n",
       "      <td>0.861682</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>.</td>\n",
       "      <td>1961</td>\n",
       "      <td>0.712712</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          token  position  attention_norm  attribution_norm\n",
       "0   Ġimperative       228        0.069075               0.0\n",
       "1        Ġmulti       266        0.034261               0.0\n",
       "2          aint       313        0.040501               0.0\n",
       "3             .       413        0.536035               0.0\n",
       "4     Ġresource       429        0.034572               0.0\n",
       "..          ...       ...             ...               ...\n",
       "68            .      1803        1.000000               0.0\n",
       "69            .      1823        0.976264               0.0\n",
       "70            .      1864        0.928553               0.0\n",
       "71            .      1910        0.861682               0.0\n",
       "72            .      1961        0.712712               0.0\n",
       "\n",
       "[73 rows x 4 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff = sorted(list(attention_final_layer_top - exam_attrib_top))\n",
    "print(len(diff))\n",
    "diff_tokens = [exam_tokens[idx] for idx in diff]\n",
    "d_diff = {\"token\": diff_tokens, \"position\":diff, \"attention_norm\":attention_final_layer5[diff], \"attribution_norm\":exam_attrib5[diff]}\n",
    "df_diff = pd.DataFrame(d_diff)\n",
    "df_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtN0uGi7LlTT",
    "outputId": "daed3918-c09e-41d4-89bf-fbf279638c6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                 21\n",
      "Ġimperative        2\n",
      "Ġdistributed       2\n",
      "Ġmulti             2\n",
      "OP                 2\n",
      "Ġcoordination      2\n",
      "Ġresource          2\n",
      "Ġpassing           1\n",
      "Ġmessage           1\n",
      "Ġutilities         1\n",
      "ĠLogic             1\n",
      "Ġimplicit          1\n",
      "Ġfunctions         1\n",
      "Ġoptimized         1\n",
      "Ġsol               1\n",
      "Ġpropag            1\n",
      "Ġconsistency       1\n",
      "Ġmodel             1\n",
      "Ġsolve             1\n",
      "Ġlanguage          1\n",
      "Ġof                1\n",
      "agent              1\n",
      "Ġallocation        1\n",
      "lf                 1\n",
      "Ġprotocols         1\n",
      "Ġinput             1\n",
      "Ġinformation       1\n",
      "Ġexpressive        1\n",
      "pling              1\n",
      "aint               1\n",
      "Ġscheduling        1\n",
      "Ġmeetings          1\n",
      "Ġa                 1\n",
      "Ġnetwork           1\n",
      "Ġevacuation        1\n",
      "Ġpower             1\n",
      "Ġdistribution      1\n",
      "Ġcoalition         1\n",
      "Ġlogistics         1\n",
      "Ġoperations        1\n",
      "-                  1\n",
      "Ļ                  1\n",
      "T                  1\n",
      "Ġresolution        1\n",
      "Ġdecentralized     1\n",
      "ference            1\n",
      "ĠProgramming       1\n",
      "Name: token, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_diff['token'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1Mw4yzv62iA"
   },
   "source": [
    "Find out which tokens have the highest attributions but not the highest attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 441
    },
    "id": "SUJBtsFvoITx",
    "outputId": "24a832d3-7772-416d-d4cb-ca799d003871"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>position</th>\n",
       "      <th>attention_norm</th>\n",
       "      <th>attribution_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ġ2017</td>\n",
       "      <td>20</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.064682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ĠUnder</td>\n",
       "      <td>22</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.063502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ġpublication</td>\n",
       "      <td>25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.121091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ĠTheory</td>\n",
       "      <td>27</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.095636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ĠPractice</td>\n",
       "      <td>29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.198533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>Ġdecl</td>\n",
       "      <td>1025</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>ar</td>\n",
       "      <td>1026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.062032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>Ġalgorithm</td>\n",
       "      <td>1099</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.134293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>ization</td>\n",
       "      <td>1537</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.050738</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>ĠProblems</td>\n",
       "      <td>1538</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.048910</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>73 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           token  position  attention_norm  attribution_norm\n",
       "0          Ġ2017        20             0.0          0.064682\n",
       "1         ĠUnder        22             0.0          0.063502\n",
       "2   Ġpublication        25             0.0          0.121091\n",
       "3        ĠTheory        27             0.0          0.095636\n",
       "4      ĠPractice        29             0.0          0.198533\n",
       "..           ...       ...             ...               ...\n",
       "68         Ġdecl      1025             0.0          0.066455\n",
       "69            ar      1026             0.0          0.062032\n",
       "70    Ġalgorithm      1099             0.0          0.134293\n",
       "71       ization      1537             0.0          0.050738\n",
       "72     ĠProblems      1538             0.0          0.048910\n",
       "\n",
       "[73 rows x 4 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "diff2 = sorted(list(exam_attrib_top - attention_final_layer_top))\n",
    "print(len(diff))\n",
    "diff_tokens2 = [exam_tokens[idx] for idx in diff2]\n",
    "d_diff2 = {\"token\": diff_tokens2, \"position\":diff2, \"attention_norm\": attention_final_layer5[diff2], \"attribution_norm\":exam_attrib5[diff2]}\n",
    "df_diff2 = pd.DataFrame(d_diff2)\n",
    "df_diff2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "q9EQU04lMKxc",
    "outputId": "e86e4c40-1dd2-405d-d1c6-ba59c535ac9d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ġalgorithms       6\n",
      "ĠLogic            4\n",
      "Ġof               3\n",
      "ĠProgramming      3\n",
      "ĠProblems         3\n",
      "Ġalgorithm        3\n",
      "Ġin               2\n",
      "Ġ                 2\n",
      "ĠOptim            2\n",
      "Ġ2017             1\n",
      "Ġproofs           1\n",
      "Ġresults          1\n",
      "Ġexperimental     1\n",
      "Ġnetworks         1\n",
      "Ġresearchers      1\n",
      "Ġscenarios        1\n",
      "ĠResearchers      1\n",
      "Ġagents           1\n",
      "Ġfield            1\n",
      "ĠDC               1\n",
      "Ġcontinue         1\n",
      "Ġdevelop          1\n",
      "Ġmajority         1\n",
      "-                 1\n",
      "based             1\n",
      "Ġthe              1\n",
      "Ġcommands         1\n",
      "Ġdecl             1\n",
      "ar                1\n",
      "T                 1\n",
      "Ġlimitations      1\n",
      "Ġconsideration    1\n",
      "ĠAnswer           1\n",
      "Ġpublication      1\n",
      "ĠTheory           1\n",
      "ĠPractice         1\n",
      "Ġ1                1\n",
      "ĠTie              1\n",
      "ĠComputer         1\n",
      "ĠDepartment       1\n",
      "edu               1\n",
      "Ġsubmitted        1\n",
      "ĠSet              1\n",
      "1                 1\n",
      "ributed           1\n",
      "Ġnovel            1\n",
      "Ġcontributions    1\n",
      "Ġprograms         1\n",
      ";                 1\n",
      "Ġcounterpart      1\n",
      ")                 1\n",
      "ĠUnder            1\n",
      "s                 1\n",
      "ization           1\n",
      "Name: token, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_diff2['token'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7KUR67I68i9"
   },
   "source": [
    "Find out which tokens are part of the highest attentions and highest attributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 976
    },
    "id": "k4VAamjiUvr0",
    "outputId": "6cdf7940-5b0d-4a7a-c271-a4107e4e462c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>position</th>\n",
       "      <th>attention_norm</th>\n",
       "      <th>attribution_norm</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>.</td>\n",
       "      <td>110</td>\n",
       "      <td>0.551079</td>\n",
       "      <td>0.185865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>).</td>\n",
       "      <td>155</td>\n",
       "      <td>0.627575</td>\n",
       "      <td>0.100035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ġlogic</td>\n",
       "      <td>177</td>\n",
       "      <td>0.042015</td>\n",
       "      <td>0.064856</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ġlogic</td>\n",
       "      <td>199</td>\n",
       "      <td>0.039247</td>\n",
       "      <td>0.086852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ġprogramming</td>\n",
       "      <td>200</td>\n",
       "      <td>0.057668</td>\n",
       "      <td>0.331142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Ġprogramming</td>\n",
       "      <td>229</td>\n",
       "      <td>0.060675</td>\n",
       "      <td>0.388581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Ġmemory</td>\n",
       "      <td>247</td>\n",
       "      <td>0.061827</td>\n",
       "      <td>0.076359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>agent</td>\n",
       "      <td>268</td>\n",
       "      <td>0.053241</td>\n",
       "      <td>0.059310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>.</td>\n",
       "      <td>276</td>\n",
       "      <td>0.663305</td>\n",
       "      <td>0.231863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>ĠProgramming</td>\n",
       "      <td>286</td>\n",
       "      <td>0.052310</td>\n",
       "      <td>0.401087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>ĠProgramming</td>\n",
       "      <td>303</td>\n",
       "      <td>0.050176</td>\n",
       "      <td>0.170107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>.</td>\n",
       "      <td>444</td>\n",
       "      <td>0.528481</td>\n",
       "      <td>0.152787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ġtargets</td>\n",
       "      <td>483</td>\n",
       "      <td>0.040669</td>\n",
       "      <td>0.100456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Ġsensors</td>\n",
       "      <td>485</td>\n",
       "      <td>0.036759</td>\n",
       "      <td>0.075240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Ġdisaster</td>\n",
       "      <td>502</td>\n",
       "      <td>0.044454</td>\n",
       "      <td>0.134998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>ĠASP</td>\n",
       "      <td>602</td>\n",
       "      <td>0.036186</td>\n",
       "      <td>0.064768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Ġalgorithm</td>\n",
       "      <td>606</td>\n",
       "      <td>0.051179</td>\n",
       "      <td>0.252586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>.</td>\n",
       "      <td>635</td>\n",
       "      <td>0.616578</td>\n",
       "      <td>0.233722</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ġsophisticated</td>\n",
       "      <td>693</td>\n",
       "      <td>0.057034</td>\n",
       "      <td>0.084256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Ġsolving</td>\n",
       "      <td>694</td>\n",
       "      <td>0.138591</td>\n",
       "      <td>0.389254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ġalgorithms</td>\n",
       "      <td>695</td>\n",
       "      <td>0.083422</td>\n",
       "      <td>0.758530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>.</td>\n",
       "      <td>696</td>\n",
       "      <td>0.631526</td>\n",
       "      <td>0.168138</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>ĠSearch</td>\n",
       "      <td>717</td>\n",
       "      <td>0.087359</td>\n",
       "      <td>0.406729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Ġvalue</td>\n",
       "      <td>794</td>\n",
       "      <td>0.048634</td>\n",
       "      <td>0.077137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Ġprogramming</td>\n",
       "      <td>889</td>\n",
       "      <td>0.081631</td>\n",
       "      <td>0.048977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Ġsearch</td>\n",
       "      <td>948</td>\n",
       "      <td>0.185193</td>\n",
       "      <td>0.427080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Ġimperative</td>\n",
       "      <td>966</td>\n",
       "      <td>0.041678</td>\n",
       "      <td>0.050654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Ġprogramming</td>\n",
       "      <td>967</td>\n",
       "      <td>0.057228</td>\n",
       "      <td>0.155693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Ġprogramming</td>\n",
       "      <td>1028</td>\n",
       "      <td>0.055262</td>\n",
       "      <td>0.077218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>ĠProgramming</td>\n",
       "      <td>1104</td>\n",
       "      <td>0.068209</td>\n",
       "      <td>0.054061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             token  position  attention_norm  attribution_norm\n",
       "0                .       110        0.551079          0.185865\n",
       "1               ).       155        0.627575          0.100035\n",
       "2           Ġlogic       177        0.042015          0.064856\n",
       "3           Ġlogic       199        0.039247          0.086852\n",
       "4     Ġprogramming       200        0.057668          0.331142\n",
       "5     Ġprogramming       229        0.060675          0.388581\n",
       "6          Ġmemory       247        0.061827          0.076359\n",
       "7            agent       268        0.053241          0.059310\n",
       "8                .       276        0.663305          0.231863\n",
       "9     ĠProgramming       286        0.052310          0.401087\n",
       "10    ĠProgramming       303        0.050176          0.170107\n",
       "11               .       444        0.528481          0.152787\n",
       "12        Ġtargets       483        0.040669          0.100456\n",
       "13        Ġsensors       485        0.036759          0.075240\n",
       "14       Ġdisaster       502        0.044454          0.134998\n",
       "15            ĠASP       602        0.036186          0.064768\n",
       "16      Ġalgorithm       606        0.051179          0.252586\n",
       "17               .       635        0.616578          0.233722\n",
       "18  Ġsophisticated       693        0.057034          0.084256\n",
       "19        Ġsolving       694        0.138591          0.389254\n",
       "20     Ġalgorithms       695        0.083422          0.758530\n",
       "21               .       696        0.631526          0.168138\n",
       "22         ĠSearch       717        0.087359          0.406729\n",
       "23          Ġvalue       794        0.048634          0.077137\n",
       "24    Ġprogramming       889        0.081631          0.048977\n",
       "25         Ġsearch       948        0.185193          0.427080\n",
       "26     Ġimperative       966        0.041678          0.050654\n",
       "27    Ġprogramming       967        0.057228          0.155693\n",
       "28    Ġprogramming      1028        0.055262          0.077218\n",
       "29    ĠProgramming      1104        0.068209          0.054061"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "same = sorted(list(attention_final_layer_top & exam_attrib_top))\n",
    "print(len(same))\n",
    "same_tokens = [exam_tokens[idx] for idx in same]\n",
    "d_same = {\"token\": same_tokens, \"position\":same, \"attention_norm\": attention_final_layer5[same], \"attribution_norm\":exam_attrib5[same]}\n",
    "df_same = pd.DataFrame(d_same)\n",
    "df_same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VXCHsbU-MWlO",
    "outputId": "972609e7-209b-46b7-e090-1bfff436eecd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".                 5\n",
      "Ġprogramming      5\n",
      "ĠProgramming      3\n",
      "Ġlogic            2\n",
      "Ġalgorithm        1\n",
      "Ġsearch           1\n",
      "Ġvalue            1\n",
      "ĠSearch           1\n",
      "Ġalgorithms       1\n",
      "Ġsolving          1\n",
      "Ġsophisticated    1\n",
      "Ġdisaster         1\n",
      "ĠASP              1\n",
      ").                1\n",
      "Ġsensors          1\n",
      "Ġtargets          1\n",
      "agent             1\n",
      "Ġmemory           1\n",
      "Ġimperative       1\n",
      "Name: token, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df_same['token'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "id": "JT1F0CMNR5_T"
   },
   "outputs": [],
   "source": [
    "def jaccard_similarity(set1, set2):\n",
    "    intersection = len(list(set1.intersection(set2)))\n",
    "    print(intersection)\n",
    "    union = (len(set1) + len(set2)) - intersection\n",
    "    print(union)\n",
    "    return float(intersection) / union"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "MlxxoI6PR_iE"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "176\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.17045454545454544"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jaccard_similarity(attention_final_layer_top, exam_attrib_top)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "Attention_attribution_cosine_sim.ipynb",
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "cogs402",
   "language": "python",
   "name": "cogs402"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3,
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0966e7f95cb34762b3ac81e97389b784": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_de5cbef4d77a435781567241fb0c4e38",
       "IPY_MODEL_e9b750ae6a7f4e1b86c324eb973d0dc7",
       "IPY_MODEL_f817de62cd434a98a25fe83c15b209c8"
      ],
      "layout": "IPY_MODEL_ab3f6334df3f4e89b7e6e479cd3323ee"
     }
    },
    "5f77e03e8c38476cb61ccc61969475d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7a7ed35e872043e390c9caa0e85faf2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "8dbbf4b9b7c84b8eb84311c25588cdb6": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9207ad0cc09541bf8e9e5f867e61dfd5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a27affa8c60e476f97c869a4e88fb687": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ab3f6334df3f4e89b7e6e479cd3323ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "de5cbef4d77a435781567241fb0c4e38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9207ad0cc09541bf8e9e5f867e61dfd5",
      "placeholder": "​",
      "style": "IPY_MODEL_7a7ed35e872043e390c9caa0e85faf2c",
      "value": "100%"
     }
    },
    "de96b22b81ff4725b21fff44c6aeebbd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e9b750ae6a7f4e1b86c324eb973d0dc7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8dbbf4b9b7c84b8eb84311c25588cdb6",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_a27affa8c60e476f97c869a4e88fb687",
      "value": 2
     }
    },
    "f817de62cd434a98a25fe83c15b209c8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_de96b22b81ff4725b21fff44c6aeebbd",
      "placeholder": "​",
      "style": "IPY_MODEL_5f77e03e8c38476cb61ccc61969475d2",
      "value": " 2/2 [00:00&lt;00:00,  7.88it/s]"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
