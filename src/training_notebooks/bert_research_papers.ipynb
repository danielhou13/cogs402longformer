{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08bba816-6364-4b40-ba29-a22a8bb9e964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\Documents\\COGS402\\cogs402longformer\\src\\training_notebooks\n",
      "C:\\Users\\danie\\Documents\\COGS402\\cogs402longformer\\src\n",
      "C:\\Users\\danie\\Documents\\COGS402\\cogs402longformer\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())\n",
    "path_parent = os.path.dirname(os.getcwd())\n",
    "os.chdir(path_parent)\n",
    "print(os.getcwd())\n",
    "\n",
    "path_parent = os.path.dirname(os.getcwd())\n",
    "os.chdir(path_parent)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bd42335-c93f-4627-9d9f-500e811119c7",
   "metadata": {},
   "source": [
    "This notebook contains the training pipeline for the research papers custom dataset. Specifically, this notebook serves to train the [BERT](https://huggingface.co/bert-base-cased) model as a baseline to compare against the longformer model we train. To train on our custom dataset, we import the bert-base-cased tokenizer and the base model from the [Huggingface Hub](https://huggingface.co/bert-base-cased) in order to fine-tune the model for our needs. The dataset used is a .csv file stored in the cogs402longformer/data folder, and this notebook includes steps for importing the model into the huggingface hub for easy use in other notebooks/projects. After fine-tuning the model, we will evaluate the model's performance and push the model into huggingface for use in other notebooks/projects."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4c14be-f567-49a0-94de-02fea10d568a",
   "metadata": {},
   "source": [
    "## Import model, tokenizer, and datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9fc435f-6101-43d7-9b9e-42f4e3d4720c",
   "metadata": {},
   "source": [
    "This block of code imports the base model and tokenizer that we will use to fine-tune our model. We also include code demonstrating how to import a model from your own hub (in this case danielhou13) and how to import from a checkpoint stored in your local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2d269558-f433-4c28-8ac2-6ede43cb338a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#import huggingface models\n",
    "from transformers import BertForSequenceClassification, BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c21f34-2019-4fd6-b0b4-1c6eb8af0886",
   "metadata": {},
   "source": [
    "We can import a dataset like so, but if you are creating your own dataset and have not pushed it to huggingface, ignore this next block of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ab1418b-1793-4244-af4c-1d847080d2ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration danielhou13--cogs402dataset-3b57e27666917d08\n",
      "Reusing dataset parquet (C:\\Users\\danie\\.cache\\huggingface\\datasets\\parquet\\danielhou13--cogs402dataset-3b57e27666917d08\\0.0.0\\0b6d5799bb726b24ad7fc7be720c170d8e497f575d02d47537de9a5bac074901)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6196b1eec5764ce9bf4b70061325b6a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 4280\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text', 'labels'],\n",
      "        num_rows: 1070\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# ignore next 3 blocks if importing from hub\n",
    "from datasets import load_dataset\n",
    "cogs402_ds = load_dataset(\"danielhou13/cogs402dataset\")\n",
    "print(cogs402_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c374e282-651e-4547-8266-e3f206b554b6",
   "metadata": {},
   "source": [
    "To convert a .csv file into a huggingface dataset, you can use pandas to first load in your csv file. Then using a huggingface dataset function, we can easily convert the .csv file into a dataset using the `from_pandas` function. Furthermore, we can partition the data using the `train_test_split` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ce08ccf-9c30-4c5c-8f75-eac961a8d739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# ds = pd.read_csv(\"data/longdoc.csv\")\n",
    "\n",
    "# #convert to huggingface dataset and split into validation\n",
    "# import datasets\n",
    "# dataset = datasets.Dataset.from_pandas(ds)\n",
    "# cogs402_ds = dataset.train_test_split(test_size=0.20)\n",
    "# print(cogs402_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeef4a5b-6581-4cb3-82de-f983cf7d652b",
   "metadata": {},
   "source": [
    "The following two blocks of code connects to your personal huggingface account and saves your newly created dataset onto a repository. From here, we can easily access this dataset by importing the dataset (example above) rather than having to convert from a .csv file to a huggingface dataset. \n",
    "\n",
    "More information about authentication tokens can be found on [here](https://huggingface.co/course/chapter4/3?fw=pt#using-the-pushtohub-api) on the huggingface website, which gives a brief summary of how to properly connect to your huggingface account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1172eee5-5449-4f78-a553-b114da37b459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # upload this dataset to huggingface\n",
    "# from huggingface_hub import notebook_login\n",
    "\n",
    "# notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74b23c8d-9230-4ad2-9bbf-b4f434721752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cogs402_ds.push_to_hub(\"danielhou13/cogs402dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14ece2b-9aab-427c-84f2-764102ba147e",
   "metadata": {},
   "source": [
    "Lastly, we make sure to split the data into their respective sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae258077-e0df-4704-b3ca-89bac8f9280a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'labels'],\n",
      "    num_rows: 1070\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#seperate the two datasets\n",
    "train_ds = cogs402_ds[\"train\"]\n",
    "eval_ds = cogs402_ds[\"test\"]\n",
    "print(eval_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5596f2d9-41f9-4e1a-baae-198c4e29cade",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a49a575-e947-455b-be60-04ce88ecc249",
   "metadata": {},
   "source": [
    "We want to make sure that the number of classes are roughly the same for the training set, otherwise we would have a class imbalance. We first want to convert the dataset into pandas, which will allow us to easily perform our exploratory data analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f2276e34-395e-496c-8cf8-5f3cc0147363",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "cogs402_ds.set_format(type=\"pandas\")\n",
    "df = cogs402_ds[\"train\"][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c55b80c-ac50-498e-ba50-9984b52008af",
   "metadata": {},
   "source": [
    "We can see that the distribution of classes is not a perfect 50/50, as we see the positive class (AI papers) have slightly more examples than the negative (PL papers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a376c790-66b4-4f9e-b9ba-0aa0ab04c1cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df[\"labels\"].value_counts(ascending=True).plot.barh()\n",
    "plt.title(\"Frequency of Classes\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92103b3c-6741-48b7-8c41-dfe974d74df7",
   "metadata": {},
   "source": [
    "Yet, the distribution is still about equal so there isn't a big class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ebd97e97-2f74-4ad4-92a1-6f970317a018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2103\n",
       "1    2177\n",
       "Name: labels, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"labels\"].value_counts(ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "17f9f148-b9cb-45a7-9f92-247d1ce7cbfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e88ecb9-8218-4a04-abd4-98701ca77eee",
   "metadata": {},
   "source": [
    "We also see that the number of words per example in the dataset can be upwards of over 25,000. That is way too many words for the Longformer (max 4098 tokens); therefore, we know that absolutely have to truncate our dataset when training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bd818846-93c7-4dfe-9316-442d90fabac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.boxplot(\"Words Per Tweet\", by=\"labels\", grid=False,\n",
    "          showfliers=False, color=\"black\")\n",
    "plt.suptitle(\"\")\n",
    "plt.xlabel(\"\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db88668d-7857-4ef1-8aeb-b86301023379",
   "metadata": {},
   "source": [
    "Now that we are done with our exploratory data analysis, we want to convert the dataset from pandas back to the huggingface dataset format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0b96502-41cc-4f67-a4c3-d10ceec8758e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cogs402_ds.reset_format()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d5ce22-50a4-494d-a5ab-a90de0402090",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a818aa8e-5332-44c2-be50-a8fc8e576486",
   "metadata": {},
   "source": [
    "Since the target column is already labeled \"labels\", we just have to tokenize the text in the dataset. We set truncation to true as we know that many, if not all, examples in our dataset will be over the max amount of tokens our longformer can handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e0ba762-6c42-48de-bb43-fcbed56449a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trainer API auto uses dynamic padding... supposedly\n",
    "def tokenize(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d9b78e6c-12f5-4c58-bfd6-94a820e6ce6b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a50566f1a8449a4b06f2ec7a0754744",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f43513523a954e7abc67e3c965338713",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0ex [00:00, ?ex/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#tokenize the data\n",
    "train_dataset = train_ds.map(tokenize, load_from_cache_file=False)\n",
    "val_dataset = eval_ds.map(tokenize, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44cac47e-d42d-4732-a5ff-c0e512c623b2",
   "metadata": {},
   "source": [
    "Finally we convert the columns into a pytorch compatible format and remove the text column since the model operates using input ids rather than raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "96ede4fd-0500-455a-8a70-28500c80216d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert required columns and remove unrequired ones\n",
    "train_dataset.set_format(\"torch\", columns=['input_ids', 'attention_mask', \"labels\"])\n",
    "val_dataset.set_format(\"torch\", columns=['input_ids', 'attention_mask', \"labels\"])\n",
    "\n",
    "train_dataset = train_dataset.remove_columns(['text'])\n",
    "val_dataset = val_dataset.remove_columns(['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32c110ce-27fe-41aa-adad-78cb3dda5502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['labels', 'input_ids', 'token_type_ids', 'attention_mask'],\n",
      "    num_rows: 1070\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f7f555-032e-4d38-aa38-2af98701c4af",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806b4626-d47f-4664-9251-7ebfae9cab89",
   "metadata": {},
   "source": [
    "We train 2 epochs and use gradient accumulation, gradient checkpointing and mixed precision in order to lower the memory requirements of the training. These methods, simply put, help your model reduce the amount of memory required to train the model. Given that each example may have up to 4098 tokens, the training uses a lot of memory, so we try to minimize the amount needed. This may have an impact on our training speed, so adjust the parameters based on your GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5dc200b8-0157-4095-99ad-1d0b2298c478",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a local models folder for checkpoint storage\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.mkdir(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de57360a-f827-4221-a2a6-bf642caa2247",
   "metadata": {},
   "source": [
    "Here we set the parameters for our training. Most of these are default parameters, shown for demonstration purposes. We train for 2 epochs and after every epoch, we do a quick checkup on the performance. We set mixed precision (fp16), gradient accumulation and gradient checkpointing to true for memory saving purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6a749d43-e2d1-4de8-bbc9-50a5a687c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "batch_size = 1\n",
    "gradient_acc = 4\n",
    "logging_steps = (len(train_dataset) // batch_size) //gradient_acc\n",
    "model_name = f\"bert-finetuned_papers\"\n",
    "training_args = TrainingArguments(output_dir=f\"models/{model_name}\",\n",
    "                                  num_train_epochs = 2,\n",
    "                                  learning_rate=2e-5,\n",
    "                                  per_device_train_batch_size=batch_size,\n",
    "                                  per_device_eval_batch_size=batch_size,\n",
    "                                  weight_decay=0.01,\n",
    "                                  evaluation_strategy=\"epoch\",\n",
    "                                  disable_tqdm=False,\n",
    "                                  logging_steps=logging_steps,\n",
    "                                  log_level=\"error\",\n",
    "                                  fp16=True,\n",
    "                                  gradient_accumulation_steps=gradient_acc,\n",
    "                                  gradient_checkpointing=True,\n",
    "                                  save_strategy = \"epoch\",\n",
    "                                  report_to=\"none\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a864d47-085a-4e5f-97c0-81c53d64d15f",
   "metadata": {},
   "source": [
    "When passing in more than one example, you need to make sure both examples are the same length, so we pad to the longest example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f6544d7-bffa-4feb-9899-cbecc48f1a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "788b7972-34cc-4745-86ea-0b664d616c06",
   "metadata": {},
   "source": [
    "Make sure you're using connecting to your GPU if you have one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e65bc1f8-8b0f-4a47-b2bc-a2f0d8b54eab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "model.to(device)\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945bf95f-1043-4c2d-b187-80a93bcfdf84",
   "metadata": {},
   "source": [
    "We use f1-score and accuracy as fairly general metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb9f6cb1-220b-407f-b9fa-43672b117209",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    labels = pred.label_ids\n",
    "    preds = pred.predictions.argmax(-1)\n",
    "    f1 = f1_score(labels, preds, average=\"weighted\")\n",
    "    acc = accuracy_score(labels, preds)\n",
    "    return {\"accuracy\": acc, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b32de8-71f6-4db0-a4c9-dfa0ccb06536",
   "metadata": {},
   "source": [
    "The training arguments are to define the hyperparameters of your fine-tuning but here we are passing in the parameters we want to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "154504ce-d89d-4c03-9978-403b3d7a8a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator = data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9bad5d88-a2cb-44c7-a9b1-f662e96fe49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ac61cd-d06e-46e2-aad3-742c2a071643",
   "metadata": {},
   "source": [
    "Here we do the actual training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ebed74ca-a33a-43f3-b429-764d17260c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\miniconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use thePyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='2140' max='2140' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2140/2140 10:12, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.277200</td>\n",
       "      <td>0.165630</td>\n",
       "      <td>0.962617</td>\n",
       "      <td>0.962617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.121300</td>\n",
       "      <td>0.146768</td>\n",
       "      <td>0.973832</td>\n",
       "      <td>0.973825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=2140, training_loss=0.19925993446991822, metrics={'train_runtime': 614.2241, 'train_samples_per_second': 13.936, 'train_steps_per_second': 3.484, 'total_flos': 2252230633881600.0, 'train_loss': 0.19925993446991822, 'epoch': 2.0})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "57e2e2f2-646c-4166-ba49-5158a96811a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1070' max='1070' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1070/1070 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds_output = trainer.predict(val_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fcce3d-2b32-4321-a3c3-290f41fa34e0",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb2fa57d-2038-4f12-9407-5880e4fc0042",
   "metadata": {},
   "source": [
    "Overall the model does very well at predicting whether an example is a Programming Language (P.L.) related paper or Artificial Intelligence (A.I.) related paper. Our accuracy and f1 score is 0.98, which is very high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "19b548e1-058b-47f9-ac6b-bbb0b39ce827",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.1467684507369995,\n",
       " 'test_accuracy': 0.9738317757009346,\n",
       " 'test_f1': 0.9738249134043527,\n",
       " 'test_runtime': 18.1536,\n",
       " 'test_samples_per_second': 58.942,\n",
       " 'test_steps_per_second': 58.942}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_output.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4f6b0148-92e1-48ec-938d-26c269ee1301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_preds = np.argmax(preds_output.predictions, axis=1)\n",
    "y_true = np.array(val_dataset[\"labels\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576d2a5-7130-4143-a2e2-43095f018fff",
   "metadata": {},
   "source": [
    "The classification report tells us how the model does in terms of recall, precision, f1-score and accuracy for both classes. We can see that precision is higher for PL papers while recall is higher for AI papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "33b770a6-2a13-4878-b3bf-33b4eaf71159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          PL       0.98      0.96      0.97       525\n",
      "          AI       0.97      0.98      0.97       545\n",
      "\n",
      "    accuracy                           0.97      1070\n",
      "   macro avg       0.97      0.97      0.97      1070\n",
      "weighted avg       0.97      0.97      0.97      1070\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(\n",
    "    classification_report(\n",
    "        y_true, y_preds, target_names=[\"PL\", \"AI\"]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eeb66c-a7b8-4c55-97f1-2e30793335fc",
   "metadata": {},
   "source": [
    "By looking at the confusion matrix, we can see how many examples were properly predicted, and how many were false positives (predicted AI instead of PL) or false negatives (predicted PL instead of AI). We use sklearn's confusion matrix function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ceb13bd-a16c-4c95-a28a-c957b1129478",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1070,)\n",
      "(1070,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(y_preds, y_true, labels):\n",
    "    cm = confusion_matrix(y_true, y_preds)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"PL\", \"AI\"])\n",
    "    disp.plot(cmap=\"Blues\", values_format=\".2f\", ax=ax, colorbar=False)\n",
    "    plt.title(\"Normalized confusion matrix\")\n",
    "    plt.show()\n",
    "\n",
    "plot_confusion_matrix(y_preds, y_true, labels=[0,1])\n",
    "print(y_true.shape)\n",
    "print(y_preds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4149220f-0dd4-4705-83b7-8b557fd710b3",
   "metadata": {},
   "source": [
    "There are more instance of wrongly predicting AI compared to wrongly predicting PL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e28f799-2376-4dd1-9a85-c5a0dcf82562",
   "metadata": {},
   "source": [
    "## Publish to Hub"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b89d825-1837-45d9-8d49-736a01e6ed58",
   "metadata": {},
   "source": [
    "Now that we are done fine-tuning the model and are satisfied with the chosen hyperparameters, we can upload this model to huggingface so we can easily access it in other notebooks/projects. Using the `push_to_hub` function, we pass in our username/name-we-want to create a new repository for your model. If you are wanting to save over a previous model, you can pass in that original repository's name like so: username/original-project. \n",
    "\n",
    "More information about authentication tokens can be found on [here](https://huggingface.co/course/chapter4/3?fw=pt#using-the-pushtohub-api) on the huggingface website, which gives a brief summary of how to properly connect to your huggingface account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e428563-d097-4dab-aa4c-0a3f38d46e01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cae5e15a322439faebfbd4fa557fd22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center>\\n<img src=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# upload model to huggingface for storage\n",
    "# replace danielhou13 with personal user\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0514e25c-c509-42d7-b645-99b048f6e789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danie\\miniconda3\\lib\\site-packages\\huggingface_hub\\hf_api.py:1001: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n",
      "  warnings.warn(\n",
      "Cloning https://huggingface.co/danielhou13/bert-finetuned_papers into local empty directory.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c820a2b4fd0e4c6ca09fc6dc4ead604e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload file pytorch_model.bin:   0%|          | 32.0k/413M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/danielhou13/bert-finetuned_papers\n",
      "   40c2374..a6b85c9  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/danielhou13/bert-finetuned_papers/commit/a6b85c9c5fe16f042d2abcc194e2974ff5f0a747'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub('danielhou13/bert-finetuned_papers')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0653c2-cc5e-4994-bb10-7dd66bd20f3d",
   "metadata": {},
   "source": [
    "Now you can easily access both the model and the dataset used to fine-tune the model for any other tasks you wish using the import functions described in the import section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cogs402",
   "language": "python",
   "name": "cogs402"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
