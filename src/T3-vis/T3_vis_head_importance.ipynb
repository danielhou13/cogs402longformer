{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "T3_vis_head_importance.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "mount_file_id": "1O4QCi8ewBp7asegKqySRflTQZ9HeH8mQ",
      "authorship_tag": "ABX9TyMnoQZ1cpERYvBNqFDyNlYA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "11085d4b24d8456baeb24a602a5fb758": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_02a4d50a840a4871b3b90b77cce87bdc",
              "IPY_MODEL_c682f066fd0c49399c0e91f1856722e4",
              "IPY_MODEL_fc9af1c547f3420caaef1eab0063d20d"
            ],
            "layout": "IPY_MODEL_2882316e5ef249809728fa0b5e228e8c"
          }
        },
        "02a4d50a840a4871b3b90b77cce87bdc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40b1f4fcc40549f2b54dadb0c81c0d87",
            "placeholder": "​",
            "style": "IPY_MODEL_0342eae4c9af4b3594dca26ee9ed0b83",
            "value": "100%"
          }
        },
        "c682f066fd0c49399c0e91f1856722e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a9c2c6fec3dc48539090c026014c29bb",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_13937ff71e4241faab4e337da50319fd",
            "value": 2
          }
        },
        "fc9af1c547f3420caaef1eab0063d20d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d51efcb834947349349e14844861991",
            "placeholder": "​",
            "style": "IPY_MODEL_1819e39554e349b2a8cd30405786f4a0",
            "value": " 2/2 [00:00&lt;00:00, 61.62it/s]"
          }
        },
        "2882316e5ef249809728fa0b5e228e8c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40b1f4fcc40549f2b54dadb0c81c0d87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0342eae4c9af4b3594dca26ee9ed0b83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a9c2c6fec3dc48539090c026014c29bb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13937ff71e4241faab4e337da50319fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d51efcb834947349349e14844861991": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1819e39554e349b2a8cd30405786f4a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danielhou13/cogs402longformer/blob/main/src/T3-vis/T3_vis_head_importance.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook is a part of the [T3-vis](https://arxiv.org/abs/2108.13587) implmentation for visualizing Transformer Neural Networks. Here, using the dataset, model and the functions, we calculate the importance of each head in each layer, allowing us to scale the attention output of a transformer model (in this case longformer) by their head and layer. By doing so, there is more attention on the tokens for the head and layer with higher importance. It operates on the validation set and we export the results for use in other notebooks."
      ],
      "metadata": {
        "id": "YWubtpDOkPAQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Packages"
      ],
      "metadata": {
        "id": "0HS0gmAUlQKv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "o1F-s-tiVphK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 328
        },
        "outputId": "a632640b-55d2-42c4-a229-1357ea5d444f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-d5df0069828e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms)\u001b[0m\n\u001b[1;32m    103\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m       \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout_ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m       ephemeral=True)\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral)\u001b[0m\n\u001b[1;32m    118\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m     _message.blocking_request(\n\u001b[0;32m--> 120\u001b[0;31m         'request_auth', request={'authType': 'dfs_ephemeral'}, timeout_sec=None)\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m   \u001b[0mmountpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_os\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpanduser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    169\u001b[0m   request_id = send_request(\n\u001b[1;32m    170\u001b[0m       request_type, request, parent=parent, expect_reply=True)\n\u001b[0;32m--> 171\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    100\u001b[0m         reply.get('colab_msg_id') == message_id):\n\u001b[1;32m    101\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 102\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    103\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import pdb"
      ],
      "metadata": {
        "id": "XKgcvhx9O0TV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install datasets --quiet"
      ],
      "metadata": {
        "id": "0SmTN_U8Qxtn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eef69021-8dd7-48a0-b3b4-591249469407"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 365 kB 15.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 58.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 140 kB 77.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 101 kB 12.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 212 kB 94.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 66.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 127 kB 85.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 94 kB 3.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 144 kB 93.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 271 kB 97.6 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers --quiet"
      ],
      "metadata": {
        "id": "p5FqxDogQyE_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51b71b38-656e-4c60-ea2c-2090d381c34d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 4.4 MB 12.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 67.2 MB/s \n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import dataset"
      ],
      "metadata": {
        "id": "AiNrfP4AlUNG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are importing the model and the dataset we want to assess. The import is replicating the manner used by the T3-vis implementation, with the removal of a few items such as \"idx\" and \"visualize columns\" as they are unnecessary. "
      ],
      "metadata": {
        "id": "Q7cHZxRPlbim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
        "\n",
        "def longformer_finetuned_papers():\n",
        "    model = AutoModelForSequenceClassification.from_pretrained('danielhou13/longformer-finetuned_papers_v2', num_labels = 2)\n",
        "    return model\n",
        "\n",
        "def preprocess_function(tokenizer, example, max_length):\n",
        "    example.update(tokenizer(example['text'], padding='max_length', max_length=max_length, truncation=True))\n",
        "    return example\n",
        "\n",
        "def get_papers_dataset(dataset_type):\n",
        "    max_length = 2048\n",
        "    dataset = load_dataset(\"danielhou13/cogs402dataset\")[dataset_type]\n",
        "    new_col = list(np.arange(0, len(dataset)))\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained('allenai/longformer-base-4096')\n",
        "    # tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
        "    dataset = dataset.map(lambda x: preprocess_function(tokenizer, x, max_length), batched=True)\n",
        "    setattr(dataset, 'input_columns', ['input_ids', 'attention_mask'])\n",
        "    setattr(dataset, 'target_columns', ['labels'])\n",
        "    setattr(dataset, 'max_length', max_length)\n",
        "    setattr(dataset, 'tokenizer', tokenizer)\n",
        "    return dataset\n",
        "\n",
        "def papers_train_set():\n",
        "    return get_papers_dataset('train')\n",
        "\n",
        "def papers_test_set():\n",
        "    return get_papers_dataset('test')"
      ],
      "metadata": {
        "id": "YiPx4114QuzZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### T3-vis functions"
      ],
      "metadata": {
        "id": "8X77rWD-lZQ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "These functions are a copy of the T3-vis functions but substituting bert for longformer. \n",
        "\n",
        "We iterate over the entire dataset and compute the importance of each head for every layer.\n",
        "\n",
        "**Output:\n",
        "Array of shape: (Layer, Head)**\n",
        "\n",
        "Each item in the array will be (after normalization) a value from 0.0 - 1.0 indicating how important that particular head is (1.0 being most important and 0.0 being least).\n"
      ],
      "metadata": {
        "id": "bnebCvPOl8yN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cdNtnToCOqUq"
      },
      "outputs": [],
      "source": [
        "def normalize(matrix, axis=None):\n",
        "    normalized = (matrix - matrix.min(axis=axis)) /\\\n",
        "                 (matrix.max(axis=axis) - matrix.min(axis=axis))\n",
        "    return normalized\n",
        "\n",
        "def find_pruneable_heads_and_indices(heads, n_heads, head_size, already_pruned_heads):\n",
        "    \"\"\"\n",
        "    List, int, int, set -> Tuple[set, \"torch.LongTensor\"]\n",
        "    \"\"\"\n",
        "\n",
        "    mask = torch.ones(n_heads, head_size)\n",
        "    heads = set(heads) - already_pruned_heads  # Convert to set and remove already pruned heads\n",
        "    for head in heads:\n",
        "        # Compute how many pruned heads are before the head and move the index accordingly\n",
        "        head = head - sum(1 if h < head else 0 for h in already_pruned_heads)\n",
        "        mask[head] = 0\n",
        "\n",
        "    mask = mask.view(-1).contiguous().eq(1)\n",
        "    index: torch.LongTensor = torch.arange(len(mask))[~mask].long()\n",
        "    return heads, index\n",
        "\n",
        "def get_taylor_importance(model):\n",
        "    n_layers = model.config.num_hidden_layers\n",
        "    n_heads = model.config.num_attention_heads\n",
        "    head_size = int(model.config.hidden_size / n_heads)\n",
        "    importance_scores = np.zeros((n_layers, n_heads))\n",
        "\n",
        "    for i in range(n_layers):\n",
        "        attention = model.longformer.encoder.layer[i].attention\n",
        "        num_attention_heads = attention.self.num_heads\n",
        "\n",
        "        pruned_heads = attention.pruned_heads\n",
        "        leftover_heads = set(list(range(n_heads))) - pruned_heads\n",
        "\n",
        "        for head_idx in leftover_heads:\n",
        "            heads, index = find_pruneable_heads_and_indices([head_idx], num_attention_heads, head_size, pruned_heads)\n",
        "            index = index.to(model.device)\n",
        "\n",
        "            query_b_grad = (attention.self.query.bias.grad[index] *\\\n",
        "                            attention.self.query.bias[index]) ** 2\n",
        "            query_W_grad = (attention.self.query.weight.grad.index_select(0, index) *\\\n",
        "                            attention.self.query.weight.index_select(0, index)) ** 2\n",
        "\n",
        "            key_b_grad = (attention.self.key.bias.grad[index] *\\\n",
        "                          attention.self.key.bias[index]) ** 2\n",
        "            key_W_grad = (attention.self.key.weight.grad.index_select(0, index) *\\\n",
        "                          attention.self.key.weight.index_select(0, index)) ** 2\n",
        "\n",
        "            value_b_grad = (attention.self.value.bias.grad[index] *\\\n",
        "                            attention.self.value.bias[index]) ** 2\n",
        "            value_W_grad = (attention.self.value.weight.grad.index_select(0, index) *\\\n",
        "                            attention.self.value.weight.index_select(0, index)) ** 2\n",
        "\n",
        "            output_W_grad = (attention.output.dense.weight.grad.index_select(1, index) *\n",
        "                             attention.output.dense.weight.index_select(1, index)) ** 2\n",
        "            abs_grad_magnitude = query_b_grad.sum() + query_W_grad.sum() + key_b_grad.sum() + \\\n",
        "                key_W_grad.sum() + value_b_grad.sum() + value_W_grad.sum() + output_W_grad.sum()\n",
        "\n",
        "                \n",
        "            score = abs_grad_magnitude.item()\n",
        "            importance_scores[i, head_idx] += score\n",
        "    return importance_scores\n",
        "\n",
        "\n",
        "def compute_importance(model, dataloader, measure='taylor'):\n",
        "\n",
        "    assert measure in ['taylor', 'oracle', 'sensitivity']\n",
        "\n",
        "    max_input_len = model.config.max_position_embeddings\n",
        "    n_layers = model.config.num_hidden_layers\n",
        "    n_heads = model.config.num_attention_heads\n",
        "    head_size = int(model.config.hidden_size / n_heads)\n",
        "\n",
        "    importance_scores = np.zeros((n_layers, n_heads))\n",
        "\n",
        "    device = model.device\n",
        "    total_loss = 0.\n",
        "\n",
        "    if measure == 'sensitivity':\n",
        "        head_mask = torch.ones(n_layers, n_heads).to(device)\n",
        "        head_mask.requires_grad_(requires_grad=True)\n",
        "    else:\n",
        "        head_mask = None\n",
        "\n",
        "    for step, inputs in enumerate(tqdm(dataloader)):\n",
        "        batch_size_ = inputs['input_ids'].__len__()\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            for k, v in inputs.items():\n",
        "                if isinstance(v, torch.Tensor):\n",
        "                    inputs[k] = v.cuda()\n",
        "\n",
        "\n",
        "        output = model(**inputs)\n",
        "        loss = output['loss']\n",
        "        loss.backward()\n",
        "\n",
        "        if measure == 'sensitivity':\n",
        "            importance_scores += head_mask.grad.abs().detach().cpu().numpy()\n",
        "        elif measure == 'taylor':\n",
        "            importance_scores = get_taylor_importance(model)\n",
        "\n",
        "    return importance_scores"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we are calling the functions to import the dataset, model and making sure that the dataset is in a pytorch compatible manner."
      ],
      "metadata": {
        "id": "ck4ZlF1AmFZA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cogs402_test = papers_train_set()\n",
        "model = longformer_finetuned_papers()\n",
        "columns = cogs402_test.input_columns + cogs402_test.target_columns\n",
        "print(columns)\n",
        "cogs402_test.set_format(type='torch', columns=columns)\n",
        "cogs402_test=cogs402_test.remove_columns(['text'])\n",
        "print(cogs402_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 188,
          "referenced_widgets": [
            "11085d4b24d8456baeb24a602a5fb758",
            "02a4d50a840a4871b3b90b77cce87bdc",
            "c682f066fd0c49399c0e91f1856722e4",
            "fc9af1c547f3420caaef1eab0063d20d",
            "2882316e5ef249809728fa0b5e228e8c",
            "40b1f4fcc40549f2b54dadb0c81c0d87",
            "0342eae4c9af4b3594dca26ee9ed0b83",
            "a9c2c6fec3dc48539090c026014c29bb",
            "13937ff71e4241faab4e337da50319fd",
            "0d51efcb834947349349e14844861991",
            "1819e39554e349b2a8cd30405786f4a0"
          ]
        },
        "id": "D5vWjYPkP1-n",
        "outputId": "d7279010-29c8-47f4-b891-6d53acdfd6cd"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using custom data configuration danielhou13--cogs402dataset-144b958ac1a53abb\n",
            "Reusing dataset parquet (/root/.cache/huggingface/datasets/danielhou13___parquet/danielhou13--cogs402dataset-144b958ac1a53abb/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "11085d4b24d8456baeb24a602a5fb758"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/danielhou13___parquet/danielhou13--cogs402dataset-144b958ac1a53abb/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec/cache-6a18812811048ba8.arrow\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['input_ids', 'attention_mask', 'labels']\n",
            "Dataset({\n",
            "    features: ['labels', 'input_ids', 'attention_mask'],\n",
            "    num_rows: 4280\n",
            "})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Don't forget to send the model to GPU if you have one."
      ],
      "metadata": {
        "id": "R3PGKqYnmXZV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    model = model.cuda()\n",
        "\n",
        "print(model.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7AKZwOvJT4Z2",
        "outputId": "08a997ed-c677-4fce-cd6c-6b9c9f0afcf4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The functions operate using the dataloader so we convert our validation set (as per the T3-vis implementation) to a dataloader. We keep batch size to 1 to minimize the memory required as longformer models are memory intensive."
      ],
      "metadata": {
        "id": "4YHGXm_PmjtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "val_dataloader = torch.utils.data.DataLoader(cogs402_test, batch_size=1)"
      ],
      "metadata": {
        "id": "lMtpK5LnULrS"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, we run the function, normalize the resulting matrix, and save the results for future use."
      ],
      "metadata": {
        "id": "m8gc_2cLmzUp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "importance = compute_importance(model, val_dataloader)\n",
        "importance = normalize(importance)"
      ],
      "metadata": {
        "id": "WyksiXkZPrbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "b565c99f-5487-4173-d7d7-c9b1b137ecc2"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/4280 [00:03<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-e2006e7a9f4c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mimportance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mimportance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimportance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-689d355d107d>\u001b[0m in \u001b[0;36mcompute_importance\u001b[0;34m(model, dataloader, measure)\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0mimportance_scores\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mmeasure\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'taylor'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m             \u001b[0mimportance_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_taylor_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mimportance_scores\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-689d355d107d>\u001b[0m in \u001b[0;36mget_taylor_importance\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mget_taylor_importance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mn_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mn_heads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_attention_heads\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0mhead_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn_heads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1206\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1207\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m-> 1208\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m   1209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1210\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Module'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'LongformerForSequenceClassification' object has no attribute 'num_hidden_layers'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lastly, we save our importance matrix. Remember to change the path to whatever suits your project's needs. The commented-out line of code saves the importance matrix in the current working directory.\n"
      ],
      "metadata": {
        "id": "0WoH_cfbHFja"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(importance, \"/content/drive/MyDrive/cogs402longformer/t3-visapplication/resources/papers/pretrained/head_importance.pt\")\n",
        "# torch.save(importance, \"head_importance.pt\")"
      ],
      "metadata": {
        "id": "nc5zd7B2HEgj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}